{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, confusion_matrix, mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "list_stop_words = stopwords.words('english')\n",
    "list_stop_words.append(\"im\")\n",
    "stopwords1 = set(STOPWORDS)\n",
    "stopwords1.update([\"br\", \"href\",\"https\",\"t\",\"co\",\"c\",\"b'RT\",\"b'\",\"'\",\"neg\",\"b\",\"neg'\", \"I m\", \"I d\", \"i tt\", \"ift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25d06189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'twitter_training.csv')\n",
    "df.columns = ['id', 'topic', 'polarity', 'tweet']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bcdc44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        topic  polarity  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                               tweet  \n",
       "0  I am coming to the borders and I will kill you...  \n",
       "1  im getting on borderlands and i will kill you ...  \n",
       "2  im coming on borderlands and i will murder you...  \n",
       "3  im getting on borderlands 2 and i will murder ...  \n",
       "4  im getting into borderlands and i can murder y...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49c76139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73995, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd612c",
   "metadata": {},
   "source": [
    "<body>selecting the data based on the topic as the data is too large</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f24e05f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18443, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df['topic'].isin(['Amazon', 'Facebook', 'Xbox(Xseries)','Nvidia','Google','Microsoft', 'FIFA', 'HomeDepot'])]#.groupby('topic').count()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bcf504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the data\n",
    "def process_tweet(tweet):\n",
    "    char = ''\n",
    "    arr = []\n",
    "    empty_words = ['',' ', '  ']\n",
    "    \n",
    "    for word in str(tweet).replace(\",\", \" \").split():\n",
    "        #print(word)\n",
    "        if(word.lower().strip() in ['amazon','facebook','nvidia','google','microsoft']):\n",
    "            continue\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        word = re.sub(r'^RT[\\s]','', word)\n",
    "        word = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', word)\n",
    "        word = re.sub(r'\\s+','', word)\n",
    "        if(re.search('^@[\\s]?[a-zA-Z0-9]',word)):\n",
    "            continue\n",
    "        if(re.search('^@[\\s]+[a-zA-Z0-9]',word)):\n",
    "            #print(word)\n",
    "            continue \n",
    "        if(re.search('\\W',word)):\n",
    "            word = re.sub(r'\\W','',word)\n",
    "        if(re.search('pi.*om',word)):\n",
    "            continue\n",
    "        if(re.search(r'\\d',word)):\n",
    "            continue\n",
    "        if (re.search(r'\\ ',word)):\n",
    "            continue\n",
    "        if word.strip().lower() not in list_stop_words and word not in string.punctuation and word not in empty_words and word not in stopwords1:\n",
    "            arr.append(word.strip())\n",
    "    return \" \".join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6631b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "new_col = []\n",
    "for t in df['tweet']:\n",
    "    new_col.append(process_tweet(t))\n",
    "    \n",
    "df['unique_words'] = new_col\n",
    "df = df[df['polarity'].isin(['Positive','Negative'])]\n",
    "\n",
    "temp = [] \n",
    "for i in df['polarity']:\n",
    "    if i == 'Positive':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "\n",
    "df['Sentiment'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98fcb644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@amazon wtf .</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@ amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@ amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@amazon wtf?</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>7 @amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   topic  polarity           tweet unique_words  Sentiment\n",
       "4661   1  Amazon  Negative  @amazon wtf .           wtf          0\n",
       "4662   1  Amazon  Negative   @ amazon wtf.          wtf          0\n",
       "4663   1  Amazon  Negative   @ amazon wtf.          wtf          0\n",
       "4664   1  Amazon  Negative    @amazon wtf?          wtf          0\n",
       "4665   1  Amazon  Negative  7 @amazon wtf.          wtf          0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccdf1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the rare words from the tweets and getting the words that are occured only once \n",
    "rare_words = pd.Series(\" \".join(df[\"unique_words\"]).split()).value_counts()\n",
    "rare_words = rare_words[rare_words <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e678d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the words that occured only once\n",
    "df[\"unique_words\"] = df[\"unique_words\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\n",
    "\n",
    "#getting the features and labels\n",
    "X = df['unique_words']\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4222287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9894,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896170f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2020)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting and converting the data into numerical fearures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "vectorizer = TfidfVectorizer(min_df = 7)\n",
    "X_vector_train = vectorizer.fit_transform(X_train)\n",
    "X_vector_valid = vectorizer.transform(X_valid)\n",
    "X_vector_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c162f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2021)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the data into array and adding bias\n",
    "x_train_df = X_vector_train.toarray()\n",
    "x_valid_df = X_vector_valid.toarray()\n",
    "\n",
    "x_train_df_new = np.insert(x_train_df, 0, 1, axis = 1)\n",
    "x_valid_df_new = np.insert(x_valid_df, 0, 1, axis = 1)\n",
    "\n",
    "x_train_df = x_train_df_new\n",
    "x_valid_df = x_valid_df_new\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the labels into arrays\n",
    "y_train_df = pd.Series(y_train).array\n",
    "y_valid_df = pd.Series(y_valid).array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a0465",
   "metadata": {},
   "source": [
    "<h3> Perceptron </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c575e7",
   "metadata": {},
   "source": [
    "<body> function to train the perceptron </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "457e9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, w, eta = 0.1, n = 1):\n",
    "    \n",
    "    k = 0\n",
    "    for it in range(n):\n",
    "        for ind, ex in enumerate(x):\n",
    "            prod = np.dot(ex, w)\n",
    "\n",
    "            if prod > 0:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "            true_label = y[ind]\n",
    "            if true_label != pred:\n",
    "                k+=1\n",
    "                for i in range(len(w)):\n",
    "\n",
    "                    w[i] += eta * (true_label - pred )*ex[i]\n",
    "                \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20833c51",
   "metadata": {},
   "source": [
    "<body> training the perceptron on train data and checking the accuracy on validation data</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1410449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score whene eta = 0.01 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.01 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.05 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.05 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.1 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.1 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.5 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.5 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.9 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.9 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.01 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.01 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.05 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.05 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.1 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.1 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.5 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.5 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.9 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.9 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.01 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.01 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.05 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.05 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.1 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.1 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.5 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.5 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.9 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.9 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.01 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.01 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.05 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.05 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.1 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.1 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.5 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.5 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.9 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.9 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.01 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.01 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.05 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.05 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.1 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.1 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.5 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.5 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.9 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.9 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.01 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.01 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.05 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.05 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.1 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.1 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.5 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.5 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.9 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.9 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.01 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.01 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.05 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.05 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.1 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.1 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.5 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.5 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.9 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.9 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.01 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.01 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.05 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.05 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.1 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.1 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.5 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.5 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.9 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.9 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n"
     ]
    }
   ],
   "source": [
    "n = [1, 3, 5, 7, 10, 20,30, 50]\n",
    "eta = [0.01, 0.05, 0.1, 0.5, 0.9]\n",
    "precision_score_mat = {}\n",
    "confusion_matrix_mat = {}\n",
    "for i_n in n:\n",
    "    for j in eta:\n",
    "        W = np.zeros((x_train_df.shape[1], 1))\n",
    "        W = perceptron(x_train_df, y_train_df, W, j, i_n)\n",
    "        \n",
    "        pred_valid_y = []\n",
    "        for i in range(len(x_valid_df)):\n",
    "            val = np.dot(x_valid_df[i], W)\n",
    "\n",
    "            if val > 0:\n",
    "                pred_valid_y.append(1)\n",
    "            else:\n",
    "                pred_valid_y.append(0)\n",
    "        p_s = accuracy_score(y_valid_df,pred_valid_y)\n",
    "        C_m = confusion_matrix(y_valid_df,pred_valid_y)\n",
    "        print(f\"the accuracy score whene eta = {j} and n = {i_n} is {p_s}\")\n",
    "        precision_score_mat[str(i_n), str(j)] = p_s\n",
    "        print(f\"the confusion matrix whene eta = {j} and n = {i_n} is {C_m}\")\n",
    "        confusion_matrix_mat[str(i_n), str(j)] = C_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725d6a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2021)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1133c264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '0.1') 0.8514401212733704\n",
      "('3', '0.1') 0.8807478524507327\n",
      "('5', '0.1') 0.8807478524507327\n",
      "('7', '0.1') 0.8817584638706417\n",
      "('10', '0.1') 0.8913592723597776\n",
      "('20', '0.1') 0.8888327438100051\n",
      "('30', '0.1') 0.8888327438100051\n",
      "('50', '0.1') 0.8832743810005053\n"
     ]
    }
   ],
   "source": [
    "iter_num = []\n",
    "prec = []\n",
    "for i in precision_score_mat.keys():\n",
    "    if i[1] == '0.1':\n",
    "        iter_num.append(i[0])\n",
    "        prec.append(precision_score_mat[i])\n",
    "        print(i, precision_score_mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be0d72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy score for different number of iteration for learning rate = 0.1 for perceptron')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iter_num, prec)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(\"Accuracy score for different number of iteration for learning rate = 0.1 for perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e4be",
   "metadata": {},
   "source": [
    "<h3> Logistic Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9402fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def calc_h(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8a6bd",
   "metadata": {},
   "source": [
    "<body> function to create a logistic regression model </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3240a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_inp, y, num_iter = 100, alpha = 0.1, lambda_value = 0.0000001):\n",
    "    cost_epochs = []\n",
    "    theta = np.zeros(X_inp.shape[1])\n",
    "    m = y.size\n",
    "    batchsize = 100\n",
    "    batches = floor(y.size/batchsize)\n",
    "    #lambda_value = 0.0000001\n",
    "    for epoch in range(num_iter):\n",
    "        cost_list = []\n",
    "        for i in range(batches):\n",
    "            s = i*batchsize\n",
    "            k= s+ batchsize\n",
    "            Y = y[s:k]\n",
    "            x = X_inp[s:k,:]\n",
    "            h = calc_h(x, theta)\n",
    "            lasso_reg_term = (lambda_value / 2 * batchsize) * sum(abs(theta))\n",
    "            cost = (-Y * np.log(h) - (1 - Y) * np.log(1 - h)).mean() + lasso_reg_term\n",
    "            cost_list.append(cost)\n",
    "\n",
    "            gradient = (np.dot(x.T, (h - Y)))/ batchsize  +  (lambda_value * abs(theta))\n",
    "            theta -= alpha * gradient\n",
    "\n",
    "        cost_epochs.append(np.average(cost_list))\n",
    "        if((epoch + 1 )%10 ==0):    \n",
    "            print(f\"After {epoch+1} epochs, Loss = {cost}\")\n",
    "            \n",
    "\n",
    "    #print('Adjusted coefficient: {}'.format(theta))\n",
    "    print(h.shape)\n",
    "    return theta, cost_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36912456",
   "metadata": {},
   "source": [
    "<body> training the logistic regression on various number of iterations </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_accuracy = []\n",
    "ite = [20, 200, 2000, 4000, 10000]\n",
    "lambda_val = [0.0000001, 0.00001, 0.001, 1, 3, 5, 5000]\n",
    "for i in ite:\n",
    "    theta, cost_epochs = logistic_regression(x_train_df, y_train_df, i, alpha = 0.1)\n",
    "    preds_prob = calc_h(x_valid_df, theta)\n",
    "    y_pred = preds_prob.round()\n",
    "    valid_accuracy.append(accuracy_score(y_valid_df,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c6908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy score for logistic regression for learning rate = 0.1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ite, valid_accuracy)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(\"Accuracy score for logistic regression for learning rate = 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae0cb0",
   "metadata": {},
   "source": [
    "<h3> Neural Networks </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdaaf94",
   "metadata": {},
   "source": [
    "<body> class that creates and trains a neural network model </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11bdde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    " \n",
    "# helpers\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    " \n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    " \n",
    "class Neural_Network:\n",
    "    # sizes is a list of the number of nodes in each layer\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.accuracy_mat = []\n",
    "        \n",
    "        #creating weights and bias\n",
    "        self.bias = []\n",
    "        self.weights = []\n",
    "        for layer in range(len(sizes)):\n",
    "            if layer != 0:\n",
    "                self.bias.append(np.random.randn(sizes[layer], 1))\n",
    "                \n",
    "                inp_layer = sizes[layer-1]\n",
    "                self.weights.append(np.random.randn(sizes[layer], inp_layer))\n",
    "    \n",
    "    def forwardPropagation(self, a):\n",
    "        for b, w in zip(self.bias, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "   \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        samples = len(training_data)\n",
    "       \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "       \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                            for k in range(0, samples, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                #print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "                accuracy = self.evaluate(test_data)\n",
    "                self.accuracy_mat.append(accuracy)\n",
    "                print(f\"Epoch {j}: {accuracy}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "   \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return(output_activations - y)\n",
    "   \n",
    "    def backpropagation(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # forwardPropagation\n",
    "        activation = x\n",
    "        activations = [x] # stores activations layer by layer\n",
    "        zs = [] # stores z vectors layer by layer\n",
    "        for b, w in zip(self.bias, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "       \n",
    "       \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_derivative(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "       \n",
    "        for _layer in range(2, self.num_layers):\n",
    "            z = zs[-_layer]\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-_layer+1].transpose(), delta) * sp\n",
    "            nabla_b[-_layer] = delta\n",
    "            nabla_w[-_layer] = np.dot(delta, activations[-_layer-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "   \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.bias = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.bias, nabla_b)]\n",
    "       \n",
    "    def evaluate(self, test_data):\n",
    "        pred = []\n",
    "        true_label = []\n",
    "        for (x, y) in test_data:\n",
    "            #print(x, y)\n",
    "            if y[0] == 1:\n",
    "                true_label.append(0)\n",
    "            else:\n",
    "                true_label.append(1)\n",
    "                \n",
    "            pred.append(np.argmax(self.forwardPropagation(x)))\n",
    "        return accuracy_score(true_label, pred)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        prediction = [np.argmax(self.forwardPropagation(x)) for (x, y) in data]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f197c",
   "metadata": {},
   "source": [
    "<body> function for doing one hot encoding. This is typically used on labels as the perceptron has two output neurons.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    encoded = np.zeros((2, 1))\n",
    "    encoded[y] = 1.0\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ba674",
   "metadata": {},
   "source": [
    "<body> doing the one hot encoding on the labels. </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb23fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_df = X_vector_train.toarray()\n",
    "x_valid_df = X_vector_valid.toarray()\n",
    "y_train_df = np.array(y_train)\n",
    "y_valid_df = np.array(y_valid)\n",
    "\n",
    "x_train_df = [np.reshape(x, (X_vector_train.shape[1], 1)) for x in x_train_df]\n",
    "y_train_df = [one_hot_encode(y) for y in y_train_df]\n",
    "\n",
    "x_valid_df = [np.reshape(x, (X_vector_train.shape[1], 1)) for x in x_valid_df]\n",
    "y_valid_df = [one_hot_encode(y) for y in y_valid_df]\n",
    "\n",
    "training_data = zip(x_train_df,y_train_df)\n",
    "valid_data = zip(x_valid_df,y_valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578ded8",
   "metadata": {},
   "source": [
    "<body> creating and training the networkk. </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb8cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Neural_Network([X_vector_train.shape[1], 100, 25, 2])\n",
    "net.SGD(training_data, 500, 200, 0.5, test_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82919fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = net.accuracy_mat\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca30c31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs number of epochs on validation data for Neural Networks with 2 layers')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy)\n",
    "plt.xlabel('# of epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accuracy vs number of epochs on validation data for Neural Networks with 2 layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17fc47",
   "metadata": {},
   "source": [
    "<h3> prediction on test data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef979af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(r'C:\\Users\\Bhavesh Kilaru\\Desktop\\ML\\project2\\code and files\\data files\\twitter_validation.csv')\n",
    "df_test.columns = ['id', 'topic', 'polarity', 'tweet']\n",
    "df_test = df_test.loc[df_test['polarity'].isin(['Positive','Negative'])]\n",
    "df_test = df_test.dropna()\n",
    "df_test = df_test.loc[df_test['topic'].isin(['Amazon', 'Facebook', 'Xbox(Xseries)','Nvidia','Google','Microsoft', 'FIFA', 'HomeDepot'])]#.groupby('topic').count()\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad47b9",
   "metadata": {},
   "source": [
    "<body> label encoding for test data labels.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e592501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>pay WORD functions poorly Chromebook</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "      <td>Hi Ive Madeleine McCann cellar past years litt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9135</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Congrats to the NVIDIA NeMo team for the 1.0.0...</td>\n",
       "      <td>Congrats NeMo team release candidate Really ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8056</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>What does that say about Microsoft hardware &amp; ...</td>\n",
       "      <td>say hardware software security Man gets hacked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8857</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Watching NVIDIA position itself as not just a ...</td>\n",
       "      <td>Watching position leading hardware manufacture...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>8318</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Why didn’t anyone think of this acronym for BL...</td>\n",
       "      <td>didnt anyone think acronym BLM Batteries Lives...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>397</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Positive</td>\n",
       "      <td>#Amazon Best Seller!\\n\\nScreen Cleaner Kit - B...</td>\n",
       "      <td>Amazon Best Seller Screen Cleaner Kit Best LED...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>5708</td>\n",
       "      <td>HomeDepot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Thank you to Matching funds Home Depot RW paym...</td>\n",
       "      <td>Thank Matching funds Home Depot RW payment gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>314</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Please explain how this is possible! How can t...</td>\n",
       "      <td>Please explain possible let companies overchar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "      <td>Bought fraction today Small wins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      topic  polarity  \\\n",
       "1    8312  Microsoft  Negative   \n",
       "4    6273       FIFA  Negative   \n",
       "14   9135     Nvidia  Positive   \n",
       "18   8056  Microsoft  Negative   \n",
       "29   8857     Nvidia  Positive   \n",
       "..    ...        ...       ...   \n",
       "970  8318  Microsoft  Negative   \n",
       "973   397     Amazon  Positive   \n",
       "988  5708  HomeDepot  Positive   \n",
       "992   314     Amazon  Negative   \n",
       "997  8069  Microsoft  Positive   \n",
       "\n",
       "                                                 tweet  \\\n",
       "1    @Microsoft Why do I pay for WORD when it funct...   \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...   \n",
       "14   Congrats to the NVIDIA NeMo team for the 1.0.0...   \n",
       "18   What does that say about Microsoft hardware & ...   \n",
       "29   Watching NVIDIA position itself as not just a ...   \n",
       "..                                                 ...   \n",
       "970  Why didn’t anyone think of this acronym for BL...   \n",
       "973  #Amazon Best Seller!\\n\\nScreen Cleaner Kit - B...   \n",
       "988  Thank you to Matching funds Home Depot RW paym...   \n",
       "992  Please explain how this is possible! How can t...   \n",
       "997  Bought a fraction of Microsoft today. Small wins.   \n",
       "\n",
       "                                          unique_words  Sentiment  \n",
       "1                 pay WORD functions poorly Chromebook          0  \n",
       "4    Hi Ive Madeleine McCann cellar past years litt...          0  \n",
       "14   Congrats NeMo team release candidate Really ex...          1  \n",
       "18      say hardware software security Man gets hacked          0  \n",
       "29   Watching position leading hardware manufacture...          1  \n",
       "..                                                 ...        ...  \n",
       "970  didnt anyone think acronym BLM Batteries Lives...          0  \n",
       "973  Amazon Best Seller Screen Cleaner Kit Best LED...          1  \n",
       "988  Thank Matching funds Home Depot RW payment gen...          1  \n",
       "992  Please explain possible let companies overchar...          0  \n",
       "997                   Bought fraction today Small wins          1  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col = []\n",
    "for t in df_test['tweet']:\n",
    "    new_col.append(process_tweet(t))\n",
    "    \n",
    "df_test['unique_words'] = new_col\n",
    "\n",
    "temp = [] \n",
    "for i in df_test['polarity']:\n",
    "    if i == 'Positive':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "\n",
    "        \n",
    "df_test['Sentiment'] = temp\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfefe7",
   "metadata": {},
   "source": [
    "<body> removing the words that occured less than once in test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36756bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"unique_words\"] = df_test[\"unique_words\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\n",
    "X_test_df = df_test['unique_words']\n",
    "y_test_df = df_test['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b146e83",
   "metadata": {},
   "source": [
    "<body>converting the entire train data and validation data into numerical features and transforming the test data into numerical features.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9417d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_test = TfidfVectorizer(min_df = 7)\n",
    "Final_Data = vectorizer_test.fit_transform(X)\n",
    "X_vector_test= vectorizer_test.transform(X_test_df)\n",
    "x_train = Final_Data.toarray()\n",
    "x_test = X_vector_test.toarray()\n",
    "\n",
    "x_train_df_new = np.insert(x_train, 0, 1, axis = 1)\n",
    "x_test_df_new = np.insert(x_test, 0, 1, axis = 1)\n",
    "\n",
    "x_train = x_train_df_new\n",
    "x_test  = x_test_df_new\n",
    "\n",
    "y_train = pd.Series(y).array\n",
    "y_test = pd.Series(y_test_df).array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212acfbd",
   "metadata": {},
   "source": [
    "<body> training the perceptron on entire train and validation data, then checking its accuracy on test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b51d07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score for eta = 0.1 and n = 30 is 0.9166666666666666\n",
      "the confusion matrix for eta = 0.1 and n = 30 is [[65  0]\n",
      " [10 45]]\n"
     ]
    }
   ],
   "source": [
    "final_perceptron  = np.zeros((x_train.shape[1], 1))\n",
    "final_perceptron = perceptron(x_train, y_train, final_perceptron, 0.1,30)\n",
    "\n",
    "pred_test_y = []\n",
    "for i in range(len(x_test)):\n",
    "    val = np.dot(x_test[i], final_perceptron)\n",
    "\n",
    "    if val > 0:\n",
    "        pred_test_y.append(1)\n",
    "    else:\n",
    "        pred_test_y.append(0)\n",
    "        \n",
    "p_s = accuracy_score(y_test, pred_test_y)\n",
    "C_m = confusion_matrix(y_test,pred_test_y)\n",
    "print(f\"the accuracy score for eta = 0.1 and n = 30 is {p_s}\")\n",
    "print(f\"the confusion matrix for eta = 0.1 and n = 30 is {C_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31735f",
   "metadata": {},
   "source": [
    "<body> training the logistic regression model on entire train and validation data, then checking its accuracy on test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46e3aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 epochs, Loss = 0.7123941009823659\n",
      "After 20 epochs, Loss = 0.7017669381789405\n",
      "After 30 epochs, Loss = 0.6934044239551864\n",
      "After 40 epochs, Loss = 0.6863446656848504\n",
      "After 50 epochs, Loss = 0.6800517192355431\n",
      "After 60 epochs, Loss = 0.6742306598713823\n",
      "After 70 epochs, Loss = 0.6687180759861779\n",
      "After 80 epochs, Loss = 0.6634221230645051\n",
      "After 90 epochs, Loss = 0.6582904798616883\n",
      "After 100 epochs, Loss = 0.6532930603637929\n",
      "After 110 epochs, Loss = 0.6484122535472623\n",
      "After 120 epochs, Loss = 0.6436377651132192\n",
      "After 130 epochs, Loss = 0.6389633404366314\n",
      "After 140 epochs, Loss = 0.634385119385512\n",
      "After 150 epochs, Loss = 0.6299007691262519\n",
      "After 160 epochs, Loss = 0.6255085086114144\n",
      "After 170 epochs, Loss = 0.6212067890276658\n",
      "After 180 epochs, Loss = 0.6169943648587661\n",
      "After 190 epochs, Loss = 0.6128700905646912\n",
      "After 200 epochs, Loss = 0.6088326420135717\n",
      "After 210 epochs, Loss = 0.6048806322186883\n",
      "After 220 epochs, Loss = 0.6010126659468128\n",
      "After 230 epochs, Loss = 0.5972273136465402\n",
      "After 240 epochs, Loss = 0.5935229849949796\n",
      "After 250 epochs, Loss = 0.589898210593352\n",
      "After 260 epochs, Loss = 0.586351331070001\n",
      "After 270 epochs, Loss = 0.5828806438774939\n",
      "After 280 epochs, Loss = 0.5794844529942021\n",
      "After 290 epochs, Loss = 0.5761610183747873\n",
      "After 300 epochs, Loss = 0.5729086903100296\n",
      "After 310 epochs, Loss = 0.5697257222174655\n",
      "After 320 epochs, Loss = 0.5666104225588322\n",
      "After 330 epochs, Loss = 0.5635610982202477\n",
      "After 340 epochs, Loss = 0.560576038917922\n",
      "After 350 epochs, Loss = 0.5576536647434515\n",
      "After 360 epochs, Loss = 0.5547923052485783\n",
      "After 370 epochs, Loss = 0.5519903115192568\n",
      "After 380 epochs, Loss = 0.5492461201772739\n",
      "After 390 epochs, Loss = 0.5465582088049925\n",
      "After 400 epochs, Loss = 0.543925154650761\n",
      "After 410 epochs, Loss = 0.5413454799557484\n",
      "After 420 epochs, Loss = 0.5388176493983102\n",
      "After 430 epochs, Loss = 0.5363402761835877\n",
      "After 440 epochs, Loss = 0.5339119898446906\n",
      "After 450 epochs, Loss = 0.5315314513318763\n",
      "After 460 epochs, Loss = 0.5291973985981762\n",
      "After 470 epochs, Loss = 0.5269086197805046\n",
      "After 480 epochs, Loss = 0.5246638378293659\n",
      "After 490 epochs, Loss = 0.5224618487521099\n",
      "After 500 epochs, Loss = 0.5203014953540177\n",
      "After 510 epochs, Loss = 0.518181696562996\n",
      "After 520 epochs, Loss = 0.5161013762605393\n",
      "After 530 epochs, Loss = 0.5140594504227144\n",
      "After 540 epochs, Loss = 0.5120548578297953\n",
      "After 550 epochs, Loss = 0.5100866390921699\n",
      "After 560 epochs, Loss = 0.5081539220877948\n",
      "After 570 epochs, Loss = 0.5062557357400894\n",
      "After 580 epochs, Loss = 0.5043911135864618\n",
      "After 590 epochs, Loss = 0.5025591289713439\n",
      "After 600 epochs, Loss = 0.5007589257805185\n",
      "After 610 epochs, Loss = 0.4989896737056964\n",
      "After 620 epochs, Loss = 0.49725058128473815\n",
      "After 630 epochs, Loss = 0.4955408622279812\n",
      "After 640 epochs, Loss = 0.4938597970512367\n",
      "After 650 epochs, Loss = 0.4922066079813101\n",
      "After 660 epochs, Loss = 0.49058061753683574\n",
      "After 670 epochs, Loss = 0.4889811986657529\n",
      "After 680 epochs, Loss = 0.48740760946814987\n",
      "After 690 epochs, Loss = 0.48585918843266174\n",
      "After 700 epochs, Loss = 0.48433529721403257\n",
      "After 710 epochs, Loss = 0.4828353372625978\n",
      "After 720 epochs, Loss = 0.48135869816749105\n",
      "After 730 epochs, Loss = 0.47990480579058914\n",
      "After 740 epochs, Loss = 0.4784731206018944\n",
      "After 750 epochs, Loss = 0.4770631077896515\n",
      "After 760 epochs, Loss = 0.4756742430329883\n",
      "After 770 epochs, Loss = 0.4743059858094196\n",
      "After 780 epochs, Loss = 0.47295785648892114\n",
      "After 790 epochs, Loss = 0.4716293767898249\n",
      "After 800 epochs, Loss = 0.47032008635016287\n",
      "After 810 epochs, Loss = 0.46902957761134856\n",
      "After 820 epochs, Loss = 0.46775736750631053\n",
      "After 830 epochs, Loss = 0.4665030251323616\n",
      "After 840 epochs, Loss = 0.46526613647550036\n",
      "After 850 epochs, Loss = 0.46404630414602765\n",
      "After 860 epochs, Loss = 0.4628431201225089\n",
      "After 870 epochs, Loss = 0.4616562285059549\n",
      "After 880 epochs, Loss = 0.4604852378483602\n",
      "After 890 epochs, Loss = 0.4593297908608827\n",
      "After 900 epochs, Loss = 0.4581895518191501\n",
      "After 910 epochs, Loss = 0.45706419861372877\n",
      "After 920 epochs, Loss = 0.45595338127102136\n",
      "After 930 epochs, Loss = 0.454856788301068\n",
      "After 940 epochs, Loss = 0.45377413275318795\n",
      "After 950 epochs, Loss = 0.4527050941900972\n",
      "After 960 epochs, Loss = 0.4516493975803043\n",
      "After 970 epochs, Loss = 0.4506067408363067\n",
      "After 980 epochs, Loss = 0.4495768486425922\n",
      "After 990 epochs, Loss = 0.44855944558560223\n",
      "After 1000 epochs, Loss = 0.4475542671428544\n",
      "After 1010 epochs, Loss = 0.4465610879059234\n",
      "After 1020 epochs, Loss = 0.44557964548211837\n",
      "After 1030 epochs, Loss = 0.44460968713062726\n",
      "After 1040 epochs, Loss = 0.4436510388846274\n",
      "After 1050 epochs, Loss = 0.44270346753889056\n",
      "After 1060 epochs, Loss = 0.441766706337661\n",
      "After 1070 epochs, Loss = 0.4408405352534301\n",
      "After 1080 epochs, Loss = 0.43992474469160825\n",
      "After 1090 epochs, Loss = 0.4390191274294281\n",
      "After 1100 epochs, Loss = 0.4381234849165616\n",
      "After 1110 epochs, Loss = 0.4372376410285593\n",
      "After 1120 epochs, Loss = 0.436361403243301\n",
      "After 1130 epochs, Loss = 0.43549458142216996\n",
      "After 1140 epochs, Loss = 0.43463699096901276\n",
      "After 1150 epochs, Loss = 0.4337884579103352\n",
      "After 1160 epochs, Loss = 0.4329488127489274\n",
      "After 1170 epochs, Loss = 0.43211789032831854\n",
      "After 1180 epochs, Loss = 0.431295529701681\n",
      "After 1190 epochs, Loss = 0.43048157400419557\n",
      "After 1200 epochs, Loss = 0.4296758787857381\n",
      "After 1210 epochs, Loss = 0.4288782886046254\n",
      "After 1220 epochs, Loss = 0.4280886561023309\n",
      "After 1230 epochs, Loss = 0.4273068395616625\n",
      "After 1240 epochs, Loss = 0.42653270077360256\n",
      "After 1250 epochs, Loss = 0.4257661049351784\n",
      "After 1260 epochs, Loss = 0.4250069205483845\n",
      "After 1270 epochs, Loss = 0.4242550577798791\n",
      "After 1280 epochs, Loss = 0.4235103847709407\n",
      "After 1290 epochs, Loss = 0.42277274747722776\n",
      "After 1300 epochs, Loss = 0.4220420325015453\n",
      "After 1310 epochs, Loss = 0.4213181183085193\n",
      "After 1320 epochs, Loss = 0.4206008906661101\n",
      "After 1330 epochs, Loss = 0.4198902386589592\n",
      "After 1340 epochs, Loss = 0.4191860539817822\n",
      "After 1350 epochs, Loss = 0.4184882376298646\n",
      "After 1360 epochs, Loss = 0.4177966856236631\n",
      "After 1370 epochs, Loss = 0.4171112955437226\n",
      "After 1380 epochs, Loss = 0.4164319745051467\n",
      "After 1390 epochs, Loss = 0.4157586188180595\n",
      "After 1400 epochs, Loss = 0.41509113918805496\n",
      "After 1410 epochs, Loss = 0.4144294610740334\n",
      "After 1420 epochs, Loss = 0.41377348099483646\n",
      "After 1430 epochs, Loss = 0.4131230992592948\n",
      "After 1440 epochs, Loss = 0.4124782506917781\n",
      "After 1450 epochs, Loss = 0.41183884937757964\n",
      "After 1460 epochs, Loss = 0.4112047941916917\n",
      "After 1470 epochs, Loss = 0.4105760060847771\n",
      "After 1480 epochs, Loss = 0.40995244152001836\n",
      "After 1490 epochs, Loss = 0.4093339966795706\n",
      "After 1500 epochs, Loss = 0.4087205982002776\n",
      "After 1510 epochs, Loss = 0.40811217383602505\n",
      "After 1520 epochs, Loss = 0.40750865497276495\n",
      "After 1530 epochs, Loss = 0.40690997116120187\n",
      "After 1540 epochs, Loss = 0.4063160386442311\n",
      "After 1550 epochs, Loss = 0.4057267883422833\n",
      "After 1560 epochs, Loss = 0.4051421649843739\n",
      "After 1570 epochs, Loss = 0.4045620930472335\n",
      "After 1580 epochs, Loss = 0.40398651308422157\n",
      "After 1590 epochs, Loss = 0.4034153691504603\n",
      "After 1600 epochs, Loss = 0.40284859138739404\n",
      "After 1610 epochs, Loss = 0.40228611915611096\n",
      "After 1620 epochs, Loss = 0.4017279178004703\n",
      "After 1630 epochs, Loss = 0.40117390788759705\n",
      "After 1640 epochs, Loss = 0.4006240348841119\n",
      "After 1650 epochs, Loss = 0.4000782397124415\n",
      "After 1660 epochs, Loss = 0.3995364684301619\n",
      "After 1670 epochs, Loss = 0.39899867297798164\n",
      "After 1680 epochs, Loss = 0.39846480178403654\n",
      "After 1690 epochs, Loss = 0.3979347989486648\n",
      "After 1700 epochs, Loss = 0.3974086158441801\n",
      "After 1710 epochs, Loss = 0.3968862038859955\n",
      "After 1720 epochs, Loss = 0.3963675310436007\n",
      "After 1730 epochs, Loss = 0.3958525359860588\n",
      "After 1740 epochs, Loss = 0.39534117953867803\n",
      "After 1750 epochs, Loss = 0.394833413517823\n",
      "After 1760 epochs, Loss = 0.3943291861545347\n",
      "After 1770 epochs, Loss = 0.3938284538029821\n",
      "After 1780 epochs, Loss = 0.3933311803022498\n",
      "After 1790 epochs, Loss = 0.3928373197497936\n",
      "After 1800 epochs, Loss = 0.39234683574455625\n",
      "After 1810 epochs, Loss = 0.3918596831849739\n",
      "After 1820 epochs, Loss = 0.3913758191123981\n",
      "After 1830 epochs, Loss = 0.3908952044267937\n",
      "After 1840 epochs, Loss = 0.3904178007249106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1850 epochs, Loss = 0.3899435702829643\n",
      "After 1860 epochs, Loss = 0.3894724780734383\n",
      "After 1870 epochs, Loss = 0.38900448605381366\n",
      "After 1880 epochs, Loss = 0.38853955800407625\n",
      "After 1890 epochs, Loss = 0.38807766006001965\n",
      "After 1900 epochs, Loss = 0.38761876921989796\n",
      "After 1910 epochs, Loss = 0.38716283865585277\n",
      "After 1920 epochs, Loss = 0.38670983499323364\n",
      "After 1930 epochs, Loss = 0.38625972542260156\n",
      "After 1940 epochs, Loss = 0.38581248780967053\n",
      "After 1950 epochs, Loss = 0.38536813470726383\n",
      "After 1960 epochs, Loss = 0.3849266017144179\n",
      "After 1970 epochs, Loss = 0.38448783686707383\n",
      "After 1980 epochs, Loss = 0.38405180999628125\n",
      "After 1990 epochs, Loss = 0.383618491427959\n",
      "After 2000 epochs, Loss = 0.3831878519714093\n",
      "After 2010 epochs, Loss = 0.382759871200485\n",
      "After 2020 epochs, Loss = 0.38233452995962625\n",
      "After 2030 epochs, Loss = 0.38191180317471307\n",
      "After 2040 epochs, Loss = 0.3814916608270477\n",
      "After 2050 epochs, Loss = 0.38107406439321095\n",
      "After 2060 epochs, Loss = 0.3806589932585456\n",
      "After 2070 epochs, Loss = 0.3802464455151805\n",
      "After 2080 epochs, Loss = 0.37983637247574087\n",
      "After 2090 epochs, Loss = 0.379428778369596\n",
      "After 2100 epochs, Loss = 0.3790236079251106\n",
      "After 2110 epochs, Loss = 0.37862082452147994\n",
      "After 2120 epochs, Loss = 0.37822040402299467\n",
      "After 2130 epochs, Loss = 0.37782232265993365\n",
      "After 2140 epochs, Loss = 0.37742655863827956\n",
      "After 2150 epochs, Loss = 0.37703309338499935\n",
      "After 2160 epochs, Loss = 0.3766418995262686\n",
      "After 2170 epochs, Loss = 0.3762529532590164\n",
      "After 2180 epochs, Loss = 0.3758662325348182\n",
      "After 2190 epochs, Loss = 0.37548171562902904\n",
      "After 2200 epochs, Loss = 0.37509938451441593\n",
      "After 2210 epochs, Loss = 0.3747192153647837\n",
      "After 2220 epochs, Loss = 0.37434121551863486\n",
      "After 2230 epochs, Loss = 0.37396534555730027\n",
      "After 2240 epochs, Loss = 0.3735915754213591\n",
      "After 2250 epochs, Loss = 0.37321988519859983\n",
      "After 2260 epochs, Loss = 0.37285025525884685\n",
      "After 2270 epochs, Loss = 0.37248266624854387\n",
      "After 2280 epochs, Loss = 0.37211709908546226\n",
      "After 2290 epochs, Loss = 0.37175353495354274\n",
      "After 2300 epochs, Loss = 0.3713919552978512\n",
      "After 2310 epochs, Loss = 0.3710323418196574\n",
      "After 2320 epochs, Loss = 0.370674676471629\n",
      "After 2330 epochs, Loss = 0.3703189414531358\n",
      "After 2340 epochs, Loss = 0.36996511920566394\n",
      "After 2350 epochs, Loss = 0.3696131988626067\n",
      "After 2360 epochs, Loss = 0.36926316697758577\n",
      "After 2370 epochs, Loss = 0.3689149965978114\n",
      "After 2380 epochs, Loss = 0.36856867308605995\n",
      "After 2390 epochs, Loss = 0.36822420546160267\n",
      "After 2400 epochs, Loss = 0.36788155005406176\n",
      "After 2410 epochs, Loss = 0.3675406908798523\n",
      "After 2420 epochs, Loss = 0.3672016121644463\n",
      "After 2430 epochs, Loss = 0.36686429973235435\n",
      "After 2440 epochs, Loss = 0.3665287409720476\n",
      "After 2450 epochs, Loss = 0.36619491859855796\n",
      "After 2460 epochs, Loss = 0.36586282842031326\n",
      "After 2470 epochs, Loss = 0.3655324428399797\n",
      "After 2480 epochs, Loss = 0.3652037472614504\n",
      "After 2490 epochs, Loss = 0.36487672849115904\n",
      "After 2500 epochs, Loss = 0.3645513856166954\n",
      "After 2510 epochs, Loss = 0.36422769006059225\n",
      "After 2520 epochs, Loss = 0.3639056279464792\n",
      "After 2530 epochs, Loss = 0.3635851855702943\n",
      "After 2540 epochs, Loss = 0.36326634939734564\n",
      "After 2550 epochs, Loss = 0.362949106059455\n",
      "After 2560 epochs, Loss = 0.36263344235216577\n",
      "After 2570 epochs, Loss = 0.36231934523200704\n",
      "After 2580 epochs, Loss = 0.3620068018138176\n",
      "After 2590 epochs, Loss = 0.3616958109331033\n",
      "After 2600 epochs, Loss = 0.3613863520572629\n",
      "After 2610 epochs, Loss = 0.36107841591336465\n",
      "After 2620 epochs, Loss = 0.3607719921010659\n",
      "After 2630 epochs, Loss = 0.36046706149103896\n",
      "After 2640 epochs, Loss = 0.36016361049920287\n",
      "After 2650 epochs, Loss = 0.3598616272777019\n",
      "After 2660 epochs, Loss = 0.35956110011763204\n",
      "After 2670 epochs, Loss = 0.35926201773579325\n",
      "After 2680 epochs, Loss = 0.3589643757945415\n",
      "After 2690 epochs, Loss = 0.35866815665980156\n",
      "After 2700 epochs, Loss = 0.35837334810718074\n",
      "After 2710 epochs, Loss = 0.35807994713091035\n",
      "After 2720 epochs, Loss = 0.3577879364688127\n",
      "After 2730 epochs, Loss = 0.35749730362273735\n",
      "After 2740 epochs, Loss = 0.3572080454655141\n",
      "After 2750 epochs, Loss = 0.3569201481979692\n",
      "After 2760 epochs, Loss = 0.3566335971193666\n",
      "After 2770 epochs, Loss = 0.35634838191045093\n",
      "After 2780 epochs, Loss = 0.35606449236692994\n",
      "After 2790 epochs, Loss = 0.35578191839773154\n",
      "After 2800 epochs, Loss = 0.3555006500232999\n",
      "After 2810 epochs, Loss = 0.3552206823758928\n",
      "After 2820 epochs, Loss = 0.3549420191948734\n",
      "After 2830 epochs, Loss = 0.3546646323135144\n",
      "After 2840 epochs, Loss = 0.35438851218147405\n",
      "After 2850 epochs, Loss = 0.3541136493518792\n",
      "After 2860 epochs, Loss = 0.35384003447979934\n",
      "After 2870 epochs, Loss = 0.3535676583207561\n",
      "After 2880 epochs, Loss = 0.35329652042952175\n",
      "After 2890 epochs, Loss = 0.3530266035086962\n",
      "After 2900 epochs, Loss = 0.35275791085696884\n",
      "After 2910 epochs, Loss = 0.3524904231608268\n",
      "After 2920 epochs, Loss = 0.35222412940738856\n",
      "After 2930 epochs, Loss = 0.3519590236345829\n",
      "After 2940 epochs, Loss = 0.3516950951782381\n",
      "After 2950 epochs, Loss = 0.35143233494791837\n",
      "After 2960 epochs, Loss = 0.3511707377176212\n",
      "After 2970 epochs, Loss = 0.3509102998667175\n",
      "After 2980 epochs, Loss = 0.35065101335694315\n",
      "After 2990 epochs, Loss = 0.3503928654422138\n",
      "After 3000 epochs, Loss = 0.3501358617687485\n",
      "After 3010 epochs, Loss = 0.3498799866724228\n",
      "After 3020 epochs, Loss = 0.3496252227430643\n",
      "After 3030 epochs, Loss = 0.34937156216145127\n",
      "After 3040 epochs, Loss = 0.34911899718725\n",
      "After 3050 epochs, Loss = 0.34886752015794487\n",
      "After 3060 epochs, Loss = 0.3486171325113718\n",
      "After 3070 epochs, Loss = 0.34836782342616474\n",
      "After 3080 epochs, Loss = 0.34811957977522967\n",
      "After 3090 epochs, Loss = 0.3478723941968815\n",
      "After 3100 epochs, Loss = 0.34762625941458497\n",
      "After 3110 epochs, Loss = 0.3473811817783727\n",
      "After 3120 epochs, Loss = 0.3471371453092654\n",
      "After 3130 epochs, Loss = 0.3468941381856028\n",
      "After 3140 epochs, Loss = 0.34665215340010436\n",
      "After 3150 epochs, Loss = 0.3464111911541604\n",
      "After 3160 epochs, Loss = 0.346171238211716\n",
      "After 3170 epochs, Loss = 0.34593228699718975\n",
      "After 3180 epochs, Loss = 0.34569433770383534\n",
      "After 3190 epochs, Loss = 0.3454573941401127\n",
      "After 3200 epochs, Loss = 0.3452214339770427\n",
      "After 3210 epochs, Loss = 0.34498644897880065\n",
      "After 3220 epochs, Loss = 0.34475243265742944\n",
      "After 3230 epochs, Loss = 0.3445193832074573\n",
      "After 3240 epochs, Loss = 0.3442873023200852\n",
      "After 3250 epochs, Loss = 0.3440561797265433\n",
      "After 3260 epochs, Loss = 0.3438260029853714\n",
      "After 3270 epochs, Loss = 0.3435967633973495\n",
      "After 3280 epochs, Loss = 0.3433684548310677\n",
      "After 3290 epochs, Loss = 0.34314107121203047\n",
      "After 3300 epochs, Loss = 0.3429146065219521\n",
      "After 3310 epochs, Loss = 0.34268905505393765\n",
      "After 3320 epochs, Loss = 0.3424644129312342\n",
      "After 3330 epochs, Loss = 0.34224067202350134\n",
      "After 3340 epochs, Loss = 0.3420178265304023\n",
      "After 3350 epochs, Loss = 0.34179587070445655\n",
      "After 3360 epochs, Loss = 0.3415747988504043\n",
      "After 3370 epochs, Loss = 0.34135460532456513\n",
      "After 3380 epochs, Loss = 0.3411352858950205\n",
      "After 3390 epochs, Loss = 0.34091683439587644\n",
      "After 3400 epochs, Loss = 0.34069924460131273\n",
      "After 3410 epochs, Loss = 0.3404825110678589\n",
      "After 3420 epochs, Loss = 0.3402666284006164\n",
      "After 3430 epochs, Loss = 0.34005159125268003\n",
      "After 3440 epochs, Loss = 0.33983739432457555\n",
      "After 3450 epochs, Loss = 0.33962403578138806\n",
      "After 3460 epochs, Loss = 0.3394115114185554\n",
      "After 3470 epochs, Loss = 0.3391998116623236\n",
      "After 3480 epochs, Loss = 0.3389889313974597\n",
      "After 3490 epochs, Loss = 0.3387788655534545\n",
      "After 3500 epochs, Loss = 0.33856960910400585\n",
      "After 3510 epochs, Loss = 0.3383611571420192\n",
      "After 3520 epochs, Loss = 0.33815350748446354\n",
      "After 3530 epochs, Loss = 0.33794665241375943\n",
      "After 3540 epochs, Loss = 0.3377405947528899\n",
      "After 3550 epochs, Loss = 0.3375353296976119\n",
      "After 3560 epochs, Loss = 0.3373308447725729\n",
      "After 3570 epochs, Loss = 0.33712713524701576\n",
      "After 3580 epochs, Loss = 0.3369241964305245\n",
      "After 3590 epochs, Loss = 0.3367220236725643\n",
      "After 3600 epochs, Loss = 0.3365206123620348\n",
      "After 3610 epochs, Loss = 0.33631997335543473\n",
      "After 3620 epochs, Loss = 0.3361200980774513\n",
      "After 3630 epochs, Loss = 0.33592097054261866\n",
      "After 3640 epochs, Loss = 0.33572258629757196\n",
      "After 3650 epochs, Loss = 0.33552494548084544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 3660 epochs, Loss = 0.33532804634594143\n",
      "After 3670 epochs, Loss = 0.33513187735456434\n",
      "After 3680 epochs, Loss = 0.3349364341987561\n",
      "After 3690 epochs, Loss = 0.334741712606038\n",
      "After 3700 epochs, Loss = 0.33454770833903025\n",
      "After 3710 epochs, Loss = 0.3343544171950821\n",
      "After 3720 epochs, Loss = 0.33416183500590696\n",
      "After 3730 epochs, Loss = 0.3339699576372207\n",
      "After 3740 epochs, Loss = 0.33377878098838604\n",
      "After 3750 epochs, Loss = 0.33358830099206194\n",
      "After 3760 epochs, Loss = 0.33339851361385353\n",
      "After 3770 epochs, Loss = 0.33320941499207063\n",
      "After 3780 epochs, Loss = 0.33302100834890797\n",
      "After 3790 epochs, Loss = 0.332833282405771\n",
      "After 3800 epochs, Loss = 0.3326462332563125\n",
      "After 3810 epochs, Loss = 0.33245985702572745\n",
      "After 3820 epochs, Loss = 0.3322741498701727\n",
      "After 3830 epochs, Loss = 0.3320891079764453\n",
      "After 3840 epochs, Loss = 0.3319047429278997\n",
      "After 3850 epochs, Loss = 0.3317210459421127\n",
      "After 3860 epochs, Loss = 0.33153800793165766\n",
      "After 3870 epochs, Loss = 0.33135562727379114\n",
      "After 3880 epochs, Loss = 0.3311738942347847\n",
      "After 3890 epochs, Loss = 0.3309928042048778\n",
      "After 3900 epochs, Loss = 0.3308123535765303\n",
      "After 3910 epochs, Loss = 0.33063253877043464\n",
      "After 3920 epochs, Loss = 0.3304533562352326\n",
      "After 3930 epochs, Loss = 0.3302748024472281\n",
      "After 3940 epochs, Loss = 0.33009687391720555\n",
      "After 3950 epochs, Loss = 0.329919567184574\n",
      "After 3960 epochs, Loss = 0.3297428788068128\n",
      "After 3970 epochs, Loss = 0.3295668053679727\n",
      "After 3980 epochs, Loss = 0.3293913434784124\n",
      "After 3990 epochs, Loss = 0.3292164897745382\n",
      "After 4000 epochs, Loss = 0.3290422409184829\n",
      "After 4010 epochs, Loss = 0.32886859956191866\n",
      "After 4020 epochs, Loss = 0.32869555727601146\n",
      "After 4030 epochs, Loss = 0.328523109951526\n",
      "After 4040 epochs, Loss = 0.3283512543515974\n",
      "After 4050 epochs, Loss = 0.32817998726390635\n",
      "After 4060 epochs, Loss = 0.32800930902956515\n",
      "After 4070 epochs, Loss = 0.32783921889693346\n",
      "After 4080 epochs, Loss = 0.32766970778926613\n",
      "After 4090 epochs, Loss = 0.327500772589922\n",
      "After 4100 epochs, Loss = 0.32733241020561155\n",
      "After 4110 epochs, Loss = 0.3271646175661768\n",
      "After 4120 epochs, Loss = 0.32699739162436503\n",
      "After 4130 epochs, Loss = 0.3268307293556105\n",
      "After 4140 epochs, Loss = 0.32666462775781613\n",
      "After 4150 epochs, Loss = 0.32649908385113874\n",
      "After 4160 epochs, Loss = 0.3263340959947731\n",
      "After 4170 epochs, Loss = 0.32616966435423955\n",
      "After 4180 epochs, Loss = 0.3260057815986042\n",
      "After 4190 epochs, Loss = 0.3258424448348791\n",
      "After 4200 epochs, Loss = 0.3256796511912952\n",
      "After 4210 epochs, Loss = 0.32551739781709904\n",
      "After 4220 epochs, Loss = 0.32535568188235503\n",
      "After 4230 epochs, Loss = 0.3251945005776594\n",
      "After 4240 epochs, Loss = 0.32503386364094344\n",
      "After 4250 epochs, Loss = 0.32487376092429454\n",
      "After 4260 epochs, Loss = 0.3247141845140863\n",
      "After 4270 epochs, Loss = 0.32455513168100325\n",
      "After 4280 epochs, Loss = 0.3243966026564059\n",
      "After 4290 epochs, Loss = 0.324238592012448\n",
      "After 4300 epochs, Loss = 0.3240810991489735\n",
      "After 4310 epochs, Loss = 0.3239241216403175\n",
      "After 4320 epochs, Loss = 0.3237676543410409\n",
      "After 4330 epochs, Loss = 0.3236116946376542\n",
      "After 4340 epochs, Loss = 0.3234562399353017\n",
      "After 4350 epochs, Loss = 0.32330128795933527\n",
      "After 4360 epochs, Loss = 0.3231468367274742\n",
      "After 4370 epochs, Loss = 0.3229928828219895\n",
      "After 4380 epochs, Loss = 0.32283942372080987\n",
      "After 4390 epochs, Loss = 0.32268645952120345\n",
      "After 4400 epochs, Loss = 0.322533987646947\n",
      "After 4410 epochs, Loss = 0.3223820053021008\n",
      "After 4420 epochs, Loss = 0.3222305078450373\n",
      "After 4430 epochs, Loss = 0.32207949284098736\n",
      "After 4440 epochs, Loss = 0.3219289578721743\n",
      "After 4450 epochs, Loss = 0.3217789005376585\n",
      "After 4460 epochs, Loss = 0.32162933193875953\n",
      "After 4470 epochs, Loss = 0.3214802371491451\n",
      "After 4480 epochs, Loss = 0.32133161288231893\n",
      "After 4490 epochs, Loss = 0.32118345680323557\n",
      "After 4500 epochs, Loss = 0.3210357665929437\n",
      "After 4510 epochs, Loss = 0.32088853994844047\n",
      "After 4520 epochs, Loss = 0.320741774582535\n",
      "After 4530 epochs, Loss = 0.3205954682237042\n",
      "After 4540 epochs, Loss = 0.32044961861595683\n",
      "After 4550 epochs, Loss = 0.32030423621338544\n",
      "After 4560 epochs, Loss = 0.3201593177283001\n",
      "After 4570 epochs, Loss = 0.32001484926682455\n",
      "After 4580 epochs, Loss = 0.31987083195149885\n",
      "After 4590 epochs, Loss = 0.31972726120680345\n",
      "After 4600 epochs, Loss = 0.3195841339459645\n",
      "After 4610 epochs, Loss = 0.3194414528377396\n",
      "After 4620 epochs, Loss = 0.31929921145096957\n",
      "After 4630 epochs, Loss = 0.319157407135618\n",
      "After 4640 epochs, Loss = 0.31901603778419135\n",
      "After 4650 epochs, Loss = 0.3188751013032882\n",
      "After 4660 epochs, Loss = 0.3187345956134768\n",
      "After 4670 epochs, Loss = 0.3185945186491768\n",
      "After 4680 epochs, Loss = 0.3184548683585395\n",
      "After 4690 epochs, Loss = 0.3183156427033314\n",
      "After 4700 epochs, Loss = 0.3181768396588187\n",
      "After 4710 epochs, Loss = 0.31803845721365187\n",
      "After 4720 epochs, Loss = 0.3179004933697502\n",
      "After 4730 epochs, Loss = 0.31776294614219386\n",
      "After 4740 epochs, Loss = 0.31762581355910857\n",
      "After 4750 epochs, Loss = 0.3174890936615582\n",
      "After 4760 epochs, Loss = 0.3173527845034331\n",
      "After 4770 epochs, Loss = 0.3172168857292851\n",
      "After 4780 epochs, Loss = 0.3170814028886254\n",
      "After 4790 epochs, Loss = 0.316946327564245\n",
      "After 4800 epochs, Loss = 0.31681166035209063\n",
      "After 4810 epochs, Loss = 0.3166773943280601\n",
      "After 4820 epochs, Loss = 0.31654352762074095\n",
      "After 4830 epochs, Loss = 0.3164100583708225\n",
      "After 4840 epochs, Loss = 0.31627699245493784\n",
      "After 4850 epochs, Loss = 0.31614432400710846\n",
      "After 4860 epochs, Loss = 0.3160120475320684\n",
      "After 4870 epochs, Loss = 0.3158801664376655\n",
      "After 4880 epochs, Loss = 0.31574868278701423\n",
      "After 4890 epochs, Loss = 0.3156175871200051\n",
      "After 4900 epochs, Loss = 0.3154868852294371\n",
      "After 4910 epochs, Loss = 0.31535656635427656\n",
      "After 4920 epochs, Loss = 0.31522662873959384\n",
      "After 4930 epochs, Loss = 0.3150970706416013\n",
      "After 4940 epochs, Loss = 0.31496789032755007\n",
      "After 4950 epochs, Loss = 0.31483908607564415\n",
      "After 4960 epochs, Loss = 0.3147106561749493\n",
      "After 4970 epochs, Loss = 0.3145825989253054\n",
      "After 4980 epochs, Loss = 0.314454912637241\n",
      "After 4990 epochs, Loss = 0.31432760134389337\n",
      "After 5000 epochs, Loss = 0.31420066008545333\n",
      "After 5010 epochs, Loss = 0.31407408478592874\n",
      "After 5020 epochs, Loss = 0.31394787379777145\n",
      "After 5030 epochs, Loss = 0.31382202548369764\n",
      "After 5040 epochs, Loss = 0.31369653821660654\n",
      "After 5050 epochs, Loss = 0.31357141037949965\n",
      "After 5060 epochs, Loss = 0.31344664036539965\n",
      "After 5070 epochs, Loss = 0.3133222345479623\n",
      "After 5080 epochs, Loss = 0.31319818485920503\n",
      "After 5090 epochs, Loss = 0.31307450093739403\n",
      "After 5100 epochs, Loss = 0.312951170397298\n",
      "After 5110 epochs, Loss = 0.3128281897967112\n",
      "After 5120 epochs, Loss = 0.31270555758699875\n",
      "After 5130 epochs, Loss = 0.31258327222900983\n",
      "After 5140 epochs, Loss = 0.31246133219300193\n",
      "After 5150 epochs, Loss = 0.312339735958568\n",
      "After 5160 epochs, Loss = 0.31221848201456315\n",
      "After 5170 epochs, Loss = 0.3120975745157399\n",
      "After 5180 epochs, Loss = 0.3119770162516974\n",
      "After 5190 epochs, Loss = 0.31185679585654624\n",
      "After 5200 epochs, Loss = 0.3117369118520806\n",
      "After 5210 epochs, Loss = 0.31161736276921054\n",
      "After 5220 epochs, Loss = 0.31149814997223146\n",
      "After 5230 epochs, Loss = 0.31137927678113075\n",
      "After 5240 epochs, Loss = 0.3112607341438398\n",
      "After 5250 epochs, Loss = 0.31114252062627\n",
      "After 5260 epochs, Loss = 0.31102463480389797\n",
      "After 5270 epochs, Loss = 0.3109070774908605\n",
      "After 5280 epochs, Loss = 0.3107898494593042\n",
      "After 5290 epochs, Loss = 0.3106729448996487\n",
      "After 5300 epochs, Loss = 0.31055636242147927\n",
      "After 5310 epochs, Loss = 0.31044010135478095\n",
      "After 5320 epochs, Loss = 0.3103241669901797\n",
      "After 5330 epochs, Loss = 0.3102085505883005\n",
      "After 5340 epochs, Loss = 0.3100932507916513\n",
      "After 5350 epochs, Loss = 0.30997826625082425\n",
      "After 5360 epochs, Loss = 0.30986359562442495\n",
      "After 5370 epochs, Loss = 0.30974923757900824\n",
      "After 5380 epochs, Loss = 0.30963519078901436\n",
      "After 5390 epochs, Loss = 0.30952145393670527\n",
      "After 5400 epochs, Loss = 0.3094080257121031\n",
      "After 5410 epochs, Loss = 0.30929490481292843\n",
      "After 5420 epochs, Loss = 0.30918208994453894\n",
      "After 5430 epochs, Loss = 0.30906957981987\n",
      "After 5440 epochs, Loss = 0.3089573731593751\n",
      "After 5450 epochs, Loss = 0.3088454686909679\n",
      "After 5460 epochs, Loss = 0.30873386514996237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5470 epochs, Loss = 0.3086225612790181\n",
      "After 5480 epochs, Loss = 0.3085115597892816\n",
      "After 5490 epochs, Loss = 0.30840085894156044\n",
      "After 5500 epochs, Loss = 0.30829045403315697\n",
      "After 5510 epochs, Loss = 0.3081803438355347\n",
      "After 5520 epochs, Loss = 0.30807052712722677\n",
      "After 5530 epochs, Loss = 0.3079610026937796\n",
      "After 5540 epochs, Loss = 0.3078517693277006\n",
      "After 5550 epochs, Loss = 0.3077428258284053\n",
      "After 5560 epochs, Loss = 0.3076341710021663\n",
      "After 5570 epochs, Loss = 0.3075258036620615\n",
      "After 5580 epochs, Loss = 0.30741772262792333\n",
      "After 5590 epochs, Loss = 0.3073099267262891\n",
      "After 5600 epochs, Loss = 0.3072024147903499\n",
      "After 5610 epochs, Loss = 0.3070951856599022\n",
      "After 5620 epochs, Loss = 0.30698823818129933\n",
      "After 5630 epochs, Loss = 0.3068815712074036\n",
      "After 5640 epochs, Loss = 0.3067751835975363\n",
      "After 5650 epochs, Loss = 0.30666907421743317\n",
      "After 5660 epochs, Loss = 0.30656324193919476\n",
      "After 5670 epochs, Loss = 0.3064576856412428\n",
      "After 5680 epochs, Loss = 0.3063524042082722\n",
      "After 5690 epochs, Loss = 0.30624739653120514\n",
      "After 5700 epochs, Loss = 0.30614266150714897\n",
      "After 5710 epochs, Loss = 0.306038198039347\n",
      "After 5720 epochs, Loss = 0.3059340050371384\n",
      "After 5730 epochs, Loss = 0.30583008141591106\n",
      "After 5740 epochs, Loss = 0.3057264260970609\n",
      "After 5750 epochs, Loss = 0.3056230398927743\n",
      "After 5760 epochs, Loss = 0.3055199260399035\n",
      "After 5770 epochs, Loss = 0.3054170772774517\n",
      "After 5780 epochs, Loss = 0.30531449255058096\n",
      "After 5790 epochs, Loss = 0.3052121708102542\n",
      "After 5800 epochs, Loss = 0.3051101110131803\n",
      "After 5810 epochs, Loss = 0.3050083121217765\n",
      "After 5820 epochs, Loss = 0.30490677310412745\n",
      "After 5830 epochs, Loss = 0.3048054929339447\n",
      "After 5840 epochs, Loss = 0.30470447059052863\n",
      "After 5850 epochs, Loss = 0.30460370822345534\n",
      "After 5860 epochs, Loss = 0.3045032119383361\n",
      "After 5870 epochs, Loss = 0.3044029704409028\n",
      "After 5880 epochs, Loss = 0.30430298273246026\n",
      "After 5890 epochs, Loss = 0.30420324781971075\n",
      "After 5900 epochs, Loss = 0.3041037647147119\n",
      "After 5910 epochs, Loss = 0.30400453243483877\n",
      "After 5920 epochs, Loss = 0.3039055500027478\n",
      "After 5930 epochs, Loss = 0.30380681644634216\n",
      "After 5940 epochs, Loss = 0.30370833079873105\n",
      "After 5950 epochs, Loss = 0.30361009209819917\n",
      "After 5960 epochs, Loss = 0.3035120993881673\n",
      "After 5970 epochs, Loss = 0.3034143519950359\n",
      "After 5980 epochs, Loss = 0.3033168538333637\n",
      "After 5990 epochs, Loss = 0.3032195988230568\n",
      "After 6000 epochs, Loss = 0.3031225860277063\n",
      "After 6010 epochs, Loss = 0.3030258145158786\n",
      "After 6020 epochs, Loss = 0.3029292833610737\n",
      "After 6030 epochs, Loss = 0.30283299164168737\n",
      "After 6040 epochs, Loss = 0.30273693844098015\n",
      "After 6050 epochs, Loss = 0.3026411228470447\n",
      "After 6060 epochs, Loss = 0.3025455439527715\n",
      "After 6070 epochs, Loss = 0.3024502008558173\n",
      "After 6080 epochs, Loss = 0.30235509265857363\n",
      "After 6090 epochs, Loss = 0.302260218468134\n",
      "After 6100 epochs, Loss = 0.30216557739626143\n",
      "After 6110 epochs, Loss = 0.30207116855936034\n",
      "After 6120 epochs, Loss = 0.30197699107844095\n",
      "After 6130 epochs, Loss = 0.30188304407909217\n",
      "After 6140 epochs, Loss = 0.3017893266914482\n",
      "After 6150 epochs, Loss = 0.30169583805016015\n",
      "After 6160 epochs, Loss = 0.30160257729436635\n",
      "After 6170 epochs, Loss = 0.3015095435676598\n",
      "After 6180 epochs, Loss = 0.3014167360180599\n",
      "After 6190 epochs, Loss = 0.30132415379798466\n",
      "After 6200 epochs, Loss = 0.3012317960642192\n",
      "After 6210 epochs, Loss = 0.30113966197788755\n",
      "After 6220 epochs, Loss = 0.3010477507044249\n",
      "After 6230 epochs, Loss = 0.3009560614135476\n",
      "After 6240 epochs, Loss = 0.3008645959652352\n",
      "After 6250 epochs, Loss = 0.30077335117607323\n",
      "After 6260 epochs, Loss = 0.300682337672175\n",
      "After 6270 epochs, Loss = 0.30059154290808665\n",
      "After 6280 epochs, Loss = 0.3005009660311695\n",
      "After 6290 epochs, Loss = 0.3004106062362821\n",
      "After 6300 epochs, Loss = 0.30032046272236923\n",
      "After 6310 epochs, Loss = 0.30023053469243405\n",
      "After 6320 epochs, Loss = 0.30014082135351255\n",
      "After 6330 epochs, Loss = 0.30005132383034455\n",
      "After 6340 epochs, Loss = 0.2999620447463584\n",
      "After 6350 epochs, Loss = 0.29987297800542545\n",
      "After 6360 epochs, Loss = 0.29978412283043165\n",
      "After 6370 epochs, Loss = 0.2996954784481689\n",
      "After 6380 epochs, Loss = 0.2996070440893081\n",
      "After 6390 epochs, Loss = 0.2995188189883753\n",
      "After 6400 epochs, Loss = 0.29943080238372755\n",
      "After 6410 epochs, Loss = 0.2993429950998367\n",
      "After 6420 epochs, Loss = 0.2992553980573424\n",
      "After 6430 epochs, Loss = 0.2991680072489843\n",
      "After 6440 epochs, Loss = 0.2990808219282007\n",
      "After 6450 epochs, Loss = 0.29899384135213997\n",
      "After 6460 epochs, Loss = 0.2989070647816352\n",
      "After 6470 epochs, Loss = 0.29882049148118295\n",
      "After 6480 epochs, Loss = 0.2987341207189177\n",
      "After 6490 epochs, Loss = 0.29864795176659\n",
      "After 6500 epochs, Loss = 0.29856198389954275\n",
      "After 6510 epochs, Loss = 0.29847621806480734\n",
      "After 6520 epochs, Loss = 0.2983906519347165\n",
      "After 6530 epochs, Loss = 0.29830528473831536\n",
      "After 6540 epochs, Loss = 0.2982201157650451\n",
      "After 6550 epochs, Loss = 0.29813514430784815\n",
      "After 6560 epochs, Loss = 0.2980503696631402\n",
      "After 6570 epochs, Loss = 0.2979657911307734\n",
      "After 6580 epochs, Loss = 0.29788140801401675\n",
      "After 6590 epochs, Loss = 0.2977972211619468\n",
      "After 6600 epochs, Loss = 0.2977132350502938\n",
      "After 6610 epochs, Loss = 0.29762944227459465\n",
      "After 6620 epochs, Loss = 0.29754584215171387\n",
      "After 6630 epochs, Loss = 0.2974624340018267\n",
      "After 6640 epochs, Loss = 0.29737921714839194\n",
      "After 6650 epochs, Loss = 0.29729619315633854\n",
      "After 6660 epochs, Loss = 0.29721336182772223\n",
      "After 6670 epochs, Loss = 0.2971307197796485\n",
      "After 6680 epochs, Loss = 0.2970482663483937\n",
      "After 6690 epochs, Loss = 0.2969660008735812\n",
      "After 6700 epochs, Loss = 0.29688392269801117\n",
      "After 6710 epochs, Loss = 0.2968020311676274\n",
      "After 6720 epochs, Loss = 0.2967203256314967\n",
      "After 6730 epochs, Loss = 0.2966388054417931\n",
      "After 6740 epochs, Loss = 0.29655746995377563\n",
      "After 6750 epochs, Loss = 0.296476318525771\n",
      "After 6760 epochs, Loss = 0.2963953505191553\n",
      "After 6770 epochs, Loss = 0.29631456529833444\n",
      "After 6780 epochs, Loss = 0.29623396223072646\n",
      "After 6790 epochs, Loss = 0.2961535406867431\n",
      "After 6800 epochs, Loss = 0.29607330003977134\n",
      "After 6810 epochs, Loss = 0.295993239666156\n",
      "After 6820 epochs, Loss = 0.29591335894518084\n",
      "After 6830 epochs, Loss = 0.2958336572590511\n",
      "After 6840 epochs, Loss = 0.2957541339928777\n",
      "After 6850 epochs, Loss = 0.2956747885346574\n",
      "After 6860 epochs, Loss = 0.2955956203853163\n",
      "After 6870 epochs, Loss = 0.2955166323889245\n",
      "After 6880 epochs, Loss = 0.29543782038268535\n",
      "After 6890 epochs, Loss = 0.295359183765978\n",
      "After 6900 epochs, Loss = 0.2952807219409853\n",
      "After 6910 epochs, Loss = 0.2952024343126741\n",
      "After 6920 epochs, Loss = 0.2951243202887776\n",
      "After 6930 epochs, Loss = 0.2950463792797802\n",
      "After 6940 epochs, Loss = 0.29496861069890196\n",
      "After 6950 epochs, Loss = 0.294891013962079\n",
      "After 6960 epochs, Loss = 0.29481358848795075\n",
      "After 6970 epochs, Loss = 0.2947363336978421\n",
      "After 6980 epochs, Loss = 0.29465924901574775\n",
      "After 6990 epochs, Loss = 0.29458233386831706\n",
      "After 7000 epochs, Loss = 0.2945055876848372\n",
      "After 7010 epochs, Loss = 0.2944290098972198\n",
      "After 7020 epochs, Loss = 0.294352599939983\n",
      "After 7030 epochs, Loss = 0.29427635725023726\n",
      "After 7040 epochs, Loss = 0.2942002812676702\n",
      "After 7050 epochs, Loss = 0.2941243714345308\n",
      "After 7060 epochs, Loss = 0.29404862719561575\n",
      "After 7070 epochs, Loss = 0.2939730479982522\n",
      "After 7080 epochs, Loss = 0.29389764319951506\n",
      "After 7090 epochs, Loss = 0.29382240540190435\n",
      "After 7100 epochs, Loss = 0.29374734451442536\n",
      "After 7110 epochs, Loss = 0.29367244647741636\n",
      "After 7120 epochs, Loss = 0.2935977107506108\n",
      "After 7130 epochs, Loss = 0.2935231367961963\n",
      "After 7140 epochs, Loss = 0.293448724078791\n",
      "After 7150 epochs, Loss = 0.29337447206542905\n",
      "After 7160 epochs, Loss = 0.2933003802255464\n",
      "After 7170 epochs, Loss = 0.2932264480309662\n",
      "After 7180 epochs, Loss = 0.29315267495588637\n",
      "After 7190 epochs, Loss = 0.29307906047686494\n",
      "After 7200 epochs, Loss = 0.29300560407280746\n",
      "After 7210 epochs, Loss = 0.2929323060941092\n",
      "After 7220 epochs, Loss = 0.2928591684348061\n",
      "After 7230 epochs, Loss = 0.29278618730017686\n",
      "After 7240 epochs, Loss = 0.2927133621783706\n",
      "After 7250 epochs, Loss = 0.29264069255981723\n",
      "After 7260 epochs, Loss = 0.2925681779372142\n",
      "After 7270 epochs, Loss = 0.2924958178055133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 7280 epochs, Loss = 0.292423611661909\n",
      "After 7290 epochs, Loss = 0.2923515590058234\n",
      "After 7300 epochs, Loss = 0.2922796593388962\n",
      "After 7310 epochs, Loss = 0.29220791216496994\n",
      "After 7320 epochs, Loss = 0.29213631699007864\n",
      "After 7330 epochs, Loss = 0.29206487332243536\n",
      "After 7340 epochs, Loss = 0.2919935806724193\n",
      "After 7350 epochs, Loss = 0.29192243855256406\n",
      "After 7360 epochs, Loss = 0.2918514464775456\n",
      "After 7370 epochs, Loss = 0.291780603964169\n",
      "After 7380 epochs, Loss = 0.29170991053135864\n",
      "After 7390 epochs, Loss = 0.2916393657001438\n",
      "After 7400 epochs, Loss = 0.291568968993649\n",
      "After 7410 epochs, Loss = 0.29149871993708076\n",
      "After 7420 epochs, Loss = 0.2914286180577166\n",
      "After 7430 epochs, Loss = 0.29135866288489337\n",
      "After 7440 epochs, Loss = 0.29128885665447535\n",
      "After 7450 epochs, Loss = 0.2912191997425166\n",
      "After 7460 epochs, Loss = 0.29114968813742403\n",
      "After 7470 epochs, Loss = 0.2910803213766496\n",
      "After 7480 epochs, Loss = 0.29101109899964905\n",
      "After 7490 epochs, Loss = 0.2909420205478692\n",
      "After 7500 epochs, Loss = 0.29087308884081514\n",
      "After 7510 epochs, Loss = 0.2908043013149667\n",
      "After 7520 epochs, Loss = 0.2907356563516668\n",
      "After 7530 epochs, Loss = 0.2906671568749044\n",
      "After 7540 epochs, Loss = 0.2905988001246338\n",
      "After 7550 epochs, Loss = 0.29053058458852704\n",
      "After 7560 epochs, Loss = 0.29046250982166455\n",
      "After 7570 epochs, Loss = 0.29039457538103053\n",
      "After 7580 epochs, Loss = 0.2903267808255027\n",
      "After 7590 epochs, Loss = 0.290259127146469\n",
      "After 7600 epochs, Loss = 0.29019161372311597\n",
      "After 7610 epochs, Loss = 0.29012423887259203\n",
      "After 7620 epochs, Loss = 0.29005700216125346\n",
      "After 7630 epochs, Loss = 0.28998990315730144\n",
      "After 7640 epochs, Loss = 0.28992294143076996\n",
      "After 7650 epochs, Loss = 0.2898561165535143\n",
      "After 7660 epochs, Loss = 0.2897894280992046\n",
      "After 7670 epochs, Loss = 0.28972287564331245\n",
      "After 7680 epochs, Loss = 0.28965645876310386\n",
      "After 7690 epochs, Loss = 0.28959017703762674\n",
      "After 7700 epochs, Loss = 0.2895240300477047\n",
      "After 7710 epochs, Loss = 0.28945801737592447\n",
      "After 7720 epochs, Loss = 0.28939213860662744\n",
      "After 7730 epochs, Loss = 0.28932639332590177\n",
      "After 7740 epochs, Loss = 0.2892607811215689\n",
      "After 7750 epochs, Loss = 0.2891953015831799\n",
      "After 7760 epochs, Loss = 0.28912995430200117\n",
      "After 7770 epochs, Loss = 0.28906473887100703\n",
      "After 7780 epochs, Loss = 0.28899965488487184\n",
      "After 7790 epochs, Loss = 0.2889347019399596\n",
      "After 7800 epochs, Loss = 0.288869879634315\n",
      "After 7810 epochs, Loss = 0.2888051875676538\n",
      "After 7820 epochs, Loss = 0.2887406253413563\n",
      "After 7830 epochs, Loss = 0.288676192558455\n",
      "After 7840 epochs, Loss = 0.28861188882362937\n",
      "After 7850 epochs, Loss = 0.288547713743195\n",
      "After 7860 epochs, Loss = 0.28848366692509236\n",
      "After 7870 epochs, Loss = 0.288419755939827\n",
      "After 7880 epochs, Loss = 0.2883559734968931\n",
      "After 7890 epochs, Loss = 0.2882923181428803\n",
      "After 7900 epochs, Loss = 0.28822878949224207\n",
      "After 7910 epochs, Loss = 0.28816538716101414\n",
      "After 7920 epochs, Loss = 0.28810211076680775\n",
      "After 7930 epochs, Loss = 0.2880389599287988\n",
      "After 7940 epochs, Loss = 0.2879759342677215\n",
      "After 7950 epochs, Loss = 0.2879130334058596\n",
      "After 7960 epochs, Loss = 0.28785025696703903\n",
      "After 7970 epochs, Loss = 0.2877876045766181\n",
      "After 7980 epochs, Loss = 0.28772507586148155\n",
      "After 7990 epochs, Loss = 0.28766267683428554\n",
      "After 8000 epochs, Loss = 0.2876004037818893\n",
      "After 8010 epochs, Loss = 0.287538255574504\n",
      "After 8020 epochs, Loss = 0.2874762371421018\n",
      "After 8030 epochs, Loss = 0.2874143405405517\n",
      "After 8040 epochs, Loss = 0.28735256540573667\n",
      "After 8050 epochs, Loss = 0.287290916755884\n",
      "After 8060 epochs, Loss = 0.28722939086146076\n",
      "After 8070 epochs, Loss = 0.28716798534734744\n",
      "After 8080 epochs, Loss = 0.28710669985614073\n",
      "After 8090 epochs, Loss = 0.28704553590282583\n",
      "After 8100 epochs, Loss = 0.2869844934305987\n",
      "After 8110 epochs, Loss = 0.28692356992070356\n",
      "After 8120 epochs, Loss = 0.2868627650205321\n",
      "After 8130 epochs, Loss = 0.2868020783788881\n",
      "After 8140 epochs, Loss = 0.28674150964598266\n",
      "After 8150 epochs, Loss = 0.2866810584734245\n",
      "After 8160 epochs, Loss = 0.2866207245142132\n",
      "After 8170 epochs, Loss = 0.2865605074227337\n",
      "After 8180 epochs, Loss = 0.28650040685474754\n",
      "After 8190 epochs, Loss = 0.286440422467385\n",
      "After 8200 epochs, Loss = 0.28638055391914097\n",
      "After 8210 epochs, Loss = 0.286320802743019\n",
      "After 8220 epochs, Loss = 0.28626117249241956\n",
      "After 8230 epochs, Loss = 0.2862016570570605\n",
      "After 8240 epochs, Loss = 0.28614225610089716\n",
      "After 8250 epochs, Loss = 0.2860829692892393\n",
      "After 8260 epochs, Loss = 0.2860237962887159\n",
      "After 8270 epochs, Loss = 0.285964736767268\n",
      "After 8280 epochs, Loss = 0.28590579039414304\n",
      "After 8290 epochs, Loss = 0.2858469568398868\n",
      "After 8300 epochs, Loss = 0.28578823577633833\n",
      "After 8310 epochs, Loss = 0.2857296268766229\n",
      "After 8320 epochs, Loss = 0.28567112981514475\n",
      "After 8330 epochs, Loss = 0.2856127442675821\n",
      "After 8340 epochs, Loss = 0.2855544699108806\n",
      "After 8350 epochs, Loss = 0.2854963064232467\n",
      "After 8360 epochs, Loss = 0.2854382534841401\n",
      "After 8370 epochs, Loss = 0.2853803107742707\n",
      "After 8380 epochs, Loss = 0.28532247797558885\n",
      "After 8390 epochs, Loss = 0.2852647547712823\n",
      "After 8400 epochs, Loss = 0.2852071469294774\n",
      "After 8410 epochs, Loss = 0.2851496529679742\n",
      "After 8420 epochs, Loss = 0.2850922676530944\n",
      "After 8430 epochs, Loss = 0.2850349906730892\n",
      "After 8440 epochs, Loss = 0.2849778217174103\n",
      "After 8450 epochs, Loss = 0.28492076047670595\n",
      "After 8460 epochs, Loss = 0.2848638066428129\n",
      "After 8470 epochs, Loss = 0.2848069599087518\n",
      "After 8480 epochs, Loss = 0.2847502199687216\n",
      "After 8490 epochs, Loss = 0.2846935865180938\n",
      "After 8500 epochs, Loss = 0.2846370592534063\n",
      "After 8510 epochs, Loss = 0.28458063787235854\n",
      "After 8520 epochs, Loss = 0.28452432207380235\n",
      "After 8530 epochs, Loss = 0.2844681141122686\n",
      "After 8540 epochs, Loss = 0.28441201169782226\n",
      "After 8550 epochs, Loss = 0.28435601397113375\n",
      "After 8560 epochs, Loss = 0.2843001251002631\n",
      "After 8570 epochs, Loss = 0.28424434245899644\n",
      "After 8580 epochs, Loss = 0.2841886636142341\n",
      "After 8590 epochs, Loss = 0.2841330882727488\n",
      "After 8600 epochs, Loss = 0.2840776161424304\n",
      "After 8610 epochs, Loss = 0.28402224693227746\n",
      "After 8620 epochs, Loss = 0.28396698035239026\n",
      "After 8630 epochs, Loss = 0.2839118161139675\n",
      "After 8640 epochs, Loss = 0.2838567539292997\n",
      "After 8650 epochs, Loss = 0.2838017935117648\n",
      "After 8660 epochs, Loss = 0.28374693457582195\n",
      "After 8670 epochs, Loss = 0.28369217683700776\n",
      "After 8680 epochs, Loss = 0.2836375200119291\n",
      "After 8690 epochs, Loss = 0.2835829638182607\n",
      "After 8700 epochs, Loss = 0.2835285079747371\n",
      "After 8710 epochs, Loss = 0.28347415220115046\n",
      "After 8720 epochs, Loss = 0.28341989621834274\n",
      "After 8730 epochs, Loss = 0.2833657397482031\n",
      "After 8740 epochs, Loss = 0.28331168670898826\n",
      "After 8750 epochs, Loss = 0.2832577415942654\n",
      "After 8760 epochs, Loss = 0.2832038951546385\n",
      "After 8770 epochs, Loss = 0.283150147116273\n",
      "After 8780 epochs, Loss = 0.2830965030113355\n",
      "After 8790 epochs, Loss = 0.2830429617524907\n",
      "After 8800 epochs, Loss = 0.28298951808188944\n",
      "After 8810 epochs, Loss = 0.28293617172970736\n",
      "After 8820 epochs, Loss = 0.2828829224271203\n",
      "After 8830 epochs, Loss = 0.28282976990629727\n",
      "After 8840 epochs, Loss = 0.282776713900397\n",
      "After 8850 epochs, Loss = 0.2827237541435631\n",
      "After 8860 epochs, Loss = 0.2826708903709188\n",
      "After 8870 epochs, Loss = 0.2826181223185633\n",
      "After 8880 epochs, Loss = 0.2825654541428759\n",
      "After 8890 epochs, Loss = 0.2825128813612909\n",
      "After 8900 epochs, Loss = 0.2824604035128235\n",
      "After 8910 epochs, Loss = 0.2824080203374308\n",
      "After 8920 epochs, Loss = 0.28235573157602284\n",
      "After 8930 epochs, Loss = 0.28230353697045923\n",
      "After 8940 epochs, Loss = 0.2822514362635439\n",
      "After 8950 epochs, Loss = 0.2821994291990221\n",
      "After 8960 epochs, Loss = 0.28214751552157546\n",
      "After 8970 epochs, Loss = 0.2820956949768171\n",
      "After 8980 epochs, Loss = 0.2820439673112879\n",
      "After 8990 epochs, Loss = 0.2819923322724527\n",
      "After 9000 epochs, Loss = 0.28194078960869506\n",
      "After 9010 epochs, Loss = 0.2818893390693153\n",
      "After 9020 epochs, Loss = 0.2818379804045232\n",
      "After 9030 epochs, Loss = 0.2817867133654359\n",
      "After 9040 epochs, Loss = 0.2817355377040736\n",
      "After 9050 epochs, Loss = 0.2816844531733554\n",
      "After 9060 epochs, Loss = 0.28163345952709407\n",
      "After 9070 epochs, Loss = 0.28158255651999425\n",
      "After 9080 epochs, Loss = 0.28153174390764657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 9090 epochs, Loss = 0.28148102144652426\n",
      "After 9100 epochs, Loss = 0.2814303888939788\n",
      "After 9110 epochs, Loss = 0.28137984600823707\n",
      "After 9120 epochs, Loss = 0.2813293925483958\n",
      "After 9130 epochs, Loss = 0.28127902827441886\n",
      "After 9140 epochs, Loss = 0.2812287529471329\n",
      "After 9150 epochs, Loss = 0.2811785663282239\n",
      "After 9160 epochs, Loss = 0.2811284681802325\n",
      "After 9170 epochs, Loss = 0.2810784582665501\n",
      "After 9180 epochs, Loss = 0.2810285363514161\n",
      "After 9190 epochs, Loss = 0.2809787021999141\n",
      "After 9200 epochs, Loss = 0.2809289555779667\n",
      "After 9210 epochs, Loss = 0.2808792962523325\n",
      "After 9220 epochs, Loss = 0.28082972399060274\n",
      "After 9230 epochs, Loss = 0.28078023856119677\n",
      "After 9240 epochs, Loss = 0.28073083973335994\n",
      "After 9250 epochs, Loss = 0.28068152727715767\n",
      "After 9260 epochs, Loss = 0.2806323009634732\n",
      "After 9270 epochs, Loss = 0.28058316056400345\n",
      "After 9280 epochs, Loss = 0.28053410585125627\n",
      "After 9290 epochs, Loss = 0.2804851365985451\n",
      "After 9300 epochs, Loss = 0.2804362525799875\n",
      "After 9310 epochs, Loss = 0.2803874535704997\n",
      "After 9320 epochs, Loss = 0.2803387393457937\n",
      "After 9330 epochs, Loss = 0.28029010968237433\n",
      "After 9340 epochs, Loss = 0.2802415643575356\n",
      "After 9350 epochs, Loss = 0.2801931031493556\n",
      "After 9360 epochs, Loss = 0.28014472614385205\n",
      "After 9370 epochs, Loss = 0.2800964367010803\n",
      "After 9380 epochs, Loss = 0.2800482318997468\n",
      "After 9390 epochs, Loss = 0.2800001103352626\n",
      "After 9400 epochs, Loss = 0.27995207283637075\n",
      "After 9410 epochs, Loss = 0.2799041235578991\n",
      "After 9420 epochs, Loss = 0.2798562568619033\n",
      "After 9430 epochs, Loss = 0.27980847253262087\n",
      "After 9440 epochs, Loss = 0.27976077035504526\n",
      "After 9450 epochs, Loss = 0.27971315011492\n",
      "After 9460 epochs, Loss = 0.279665611598738\n",
      "After 9470 epochs, Loss = 0.27961815459373573\n",
      "After 9480 epochs, Loss = 0.2795707788878913\n",
      "After 9490 epochs, Loss = 0.2795234842699207\n",
      "After 9500 epochs, Loss = 0.2794762705292754\n",
      "After 9510 epochs, Loss = 0.27942913745613823\n",
      "After 9520 epochs, Loss = 0.2793820848414196\n",
      "After 9530 epochs, Loss = 0.2793351124767564\n",
      "After 9540 epochs, Loss = 0.27928822015450716\n",
      "After 9550 epochs, Loss = 0.27924140766774896\n",
      "After 9560 epochs, Loss = 0.2791946748102752\n",
      "After 9570 epochs, Loss = 0.2791480213765919\n",
      "After 9580 epochs, Loss = 0.27910144716191443\n",
      "After 9590 epochs, Loss = 0.2790549519621645\n",
      "After 9600 epochs, Loss = 0.27900853557396715\n",
      "After 9610 epochs, Loss = 0.27896219779464865\n",
      "After 9620 epochs, Loss = 0.2789159384222317\n",
      "After 9630 epochs, Loss = 0.2788697572554343\n",
      "After 9640 epochs, Loss = 0.2788236540936645\n",
      "After 9650 epochs, Loss = 0.2787776321778749\n",
      "After 9660 epochs, Loss = 0.2787316883772176\n",
      "After 9670 epochs, Loss = 0.2786858219810025\n",
      "After 9680 epochs, Loss = 0.27864003279137367\n",
      "After 9690 epochs, Loss = 0.27859432061115225\n",
      "After 9700 epochs, Loss = 0.2785486852438337\n",
      "After 9710 epochs, Loss = 0.27850312667567484\n",
      "After 9720 epochs, Loss = 0.27845764538815554\n",
      "After 9730 epochs, Loss = 0.27841224032776307\n",
      "After 9740 epochs, Loss = 0.2783669113006565\n",
      "After 9750 epochs, Loss = 0.27832165811365517\n",
      "After 9760 epochs, Loss = 0.27827648057423404\n",
      "After 9770 epochs, Loss = 0.27823138238685197\n",
      "After 9780 epochs, Loss = 0.27818635980621675\n",
      "After 9790 epochs, Loss = 0.27814141229908973\n",
      "After 9800 epochs, Loss = 0.2780965396755447\n",
      "After 9810 epochs, Loss = 0.27805174174629804\n",
      "After 9820 epochs, Loss = 0.2780070183227062\n",
      "After 9830 epochs, Loss = 0.2779623692167618\n",
      "After 9840 epochs, Loss = 0.2779177942410924\n",
      "After 9850 epochs, Loss = 0.2778732932089576\n",
      "After 9860 epochs, Loss = 0.27782886593424444\n",
      "After 9870 epochs, Loss = 0.27778451223146716\n",
      "After 9880 epochs, Loss = 0.2777402327651966\n",
      "After 9890 epochs, Loss = 0.27769603513767244\n",
      "After 9900 epochs, Loss = 0.27765191053208904\n",
      "After 9910 epochs, Loss = 0.2776078587654221\n",
      "After 9920 epochs, Loss = 0.2775638796552661\n",
      "After 9930 epochs, Loss = 0.27751997301982584\n",
      "After 9940 epochs, Loss = 0.2774761386779142\n",
      "After 9950 epochs, Loss = 0.27743237644894897\n",
      "After 9960 epochs, Loss = 0.2773886861529506\n",
      "After 9970 epochs, Loss = 0.2773450676105398\n",
      "After 9980 epochs, Loss = 0.27730152064293523\n",
      "After 9990 epochs, Loss = 0.2772580450719506\n",
      "After 10000 epochs, Loss = 0.27721464071999224\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "final_theta, cost_epochs = logistic_regression(x_train, y_train, 10000, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc21dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the given model on test data for logistic regression is  0.975\n"
     ]
    }
   ],
   "source": [
    "preds_prob = calc_h(x_test, final_theta)\n",
    "y_pred = preds_prob.round()\n",
    "print(\"The accuracy of the given model on test data for logistic regression is \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b69ea7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of the given model on test data for logistic regression is  [[64  1]\n",
      " [ 2 53]]\n"
     ]
    }
   ],
   "source": [
    "C_m_log = confusion_matrix(y_test,y_pred)\n",
    "print(\"The confusion matrix of the given model on test data for logistic regression is \",C_m_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27860f07",
   "metadata": {},
   "source": [
    "<h5> predicting via the neural network </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06d9f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of neural networks with 2layers on test data is  0.975\n"
     ]
    }
   ],
   "source": [
    "X_vector_test= vectorizer.transform(X_test_df)\n",
    "x_test_df_new = [np.reshape(x, (X_vector_test.shape[1], 1)) for x in X_vector_test.toarray()]\n",
    "y_test = y_test_df.array\n",
    "test_data = zip(x_test_df_new, y_test)\n",
    "\n",
    "pred = net.predict(test_data)\n",
    "#pred= pred.round()\n",
    "print(\"The accuracy score of neural networks with 2layers on test data is \",accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e444b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65,  0],\n",
       "       [ 3, 52]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213acace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
