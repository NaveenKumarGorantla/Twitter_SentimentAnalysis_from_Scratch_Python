{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, confusion_matrix, mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "list_stop_words = stopwords.words('english')\n",
    "list_stop_words.append(\"im\")\n",
    "stopwords1 = set(STOPWORDS)\n",
    "stopwords1.update([\"br\", \"href\",\"https\",\"t\",\"co\",\"c\",\"b'RT\",\"b'\",\"'\",\"neg\",\"b\",\"neg'\", \"I m\", \"I d\", \"i tt\", \"ift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25d06189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'twitter_training.csv')\n",
    "df.columns = ['id', 'topic', 'polarity', 'tweet']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bcdc44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        topic  polarity  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                               tweet  \n",
       "0  I am coming to the borders and I will kill you...  \n",
       "1  im getting on borderlands and i will kill you ...  \n",
       "2  im coming on borderlands and i will murder you...  \n",
       "3  im getting on borderlands 2 and i will murder ...  \n",
       "4  im getting into borderlands and i can murder y...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49c76139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73995, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd612c",
   "metadata": {},
   "source": [
    "<body>selecting the data based on the topic as the data is too large</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f24e05f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18443, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df['topic'].isin(['Amazon', 'Facebook', 'Xbox(Xseries)','Nvidia','Google','Microsoft', 'FIFA', 'HomeDepot'])]#.groupby('topic').count()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bcf504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the data\n",
    "def process_tweet(tweet):\n",
    "    char = ''\n",
    "    arr = []\n",
    "    empty_words = ['',' ', '  ']\n",
    "    \n",
    "    for word in str(tweet).replace(\",\", \" \").split():\n",
    "        #print(word)\n",
    "        if(word.lower().strip() in ['amazon','facebook','nvidia','google','microsoft']):\n",
    "            continue\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        word = re.sub(r'^RT[\\s]','', word)\n",
    "        word = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', word)\n",
    "        word = re.sub(r'\\s+','', word)\n",
    "        if(re.search('^@[\\s]?[a-zA-Z0-9]',word)):\n",
    "            continue\n",
    "        if(re.search('^@[\\s]+[a-zA-Z0-9]',word)):\n",
    "            #print(word)\n",
    "            continue \n",
    "        if(re.search('\\W',word)):\n",
    "            word = re.sub(r'\\W','',word)\n",
    "        if(re.search('pi.*om',word)):\n",
    "            continue\n",
    "        if(re.search(r'\\d',word)):\n",
    "            continue\n",
    "        if (re.search(r'\\ ',word)):\n",
    "            continue\n",
    "        if word.strip().lower() not in list_stop_words and word not in string.punctuation and word not in empty_words and word not in stopwords1:\n",
    "            arr.append(word.strip())\n",
    "    return \" \".join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6631b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "new_col = []\n",
    "for t in df['tweet']:\n",
    "    new_col.append(process_tweet(t))\n",
    "    \n",
    "df['unique_words'] = new_col\n",
    "df = df[df['polarity'].isin(['Positive','Negative'])]\n",
    "\n",
    "temp = [] \n",
    "for i in df['polarity']:\n",
    "    if i == 'Positive':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "\n",
    "df['Sentiment'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98fcb644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@amazon wtf .</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@ amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@ amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@amazon wtf?</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>7 @amazon wtf.</td>\n",
       "      <td>wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   topic  polarity           tweet unique_words  Sentiment\n",
       "4661   1  Amazon  Negative  @amazon wtf .           wtf          0\n",
       "4662   1  Amazon  Negative   @ amazon wtf.          wtf          0\n",
       "4663   1  Amazon  Negative   @ amazon wtf.          wtf          0\n",
       "4664   1  Amazon  Negative    @amazon wtf?          wtf          0\n",
       "4665   1  Amazon  Negative  7 @amazon wtf.          wtf          0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccdf1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the rare words from the tweets and getting the words that are occured only once \n",
    "rare_words = pd.Series(\" \".join(df[\"unique_words\"]).split()).value_counts()\n",
    "rare_words = rare_words[rare_words <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e678d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the words that occured only once\n",
    "df[\"unique_words\"] = df[\"unique_words\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\n",
    "\n",
    "#getting the features and labels\n",
    "X = df['unique_words']\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4222287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9894,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896170f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2020)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting and converting the data into numerical fearures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "vectorizer = TfidfVectorizer(min_df = 7)\n",
    "X_vector_train = vectorizer.fit_transform(X_train)\n",
    "X_vector_valid = vectorizer.transform(X_valid)\n",
    "X_vector_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c162f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2021)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the data into array and adding bias\n",
    "x_train_df = X_vector_train.toarray()\n",
    "x_valid_df = X_vector_valid.toarray()\n",
    "\n",
    "x_train_df_new = np.insert(x_train_df, 0, 1, axis = 1)\n",
    "x_valid_df_new = np.insert(x_valid_df, 0, 1, axis = 1)\n",
    "\n",
    "x_train_df = x_train_df_new\n",
    "x_valid_df = x_valid_df_new\n",
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the labels into arrays\n",
    "y_train_df = pd.Series(y_train).array\n",
    "y_valid_df = pd.Series(y_valid).array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a0465",
   "metadata": {},
   "source": [
    "<h3> Perceptron </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c575e7",
   "metadata": {},
   "source": [
    "<body> function to train the perceptron </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "457e9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, w, eta = 0.1, n = 1):\n",
    "    \n",
    "    k = 0\n",
    "    for it in range(n):\n",
    "        for ind, ex in enumerate(x):\n",
    "            prod = np.dot(ex, w)\n",
    "\n",
    "            if prod > 0:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "            true_label = y[ind]\n",
    "            if true_label != pred:\n",
    "                k+=1\n",
    "                for i in range(len(w)):\n",
    "\n",
    "                    w[i] += eta * (true_label - pred )*ex[i]\n",
    "                \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20833c51",
   "metadata": {},
   "source": [
    "<body> training the perceptron on train data and checking the accuracy on validation data</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1410449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score whene eta = 0.01 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.01 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.05 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.05 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.1 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.1 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.5 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.5 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.9 and n = 1 is 0.8514401212733704\n",
      "the confusion matrix whene eta = 0.9 and n = 1 is [[971 112]\n",
      " [182 714]]\n",
      "the accuracy score whene eta = 0.01 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.01 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.05 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.05 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.1 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.1 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.5 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.5 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.9 and n = 3 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.9 and n = 3 is [[1015   68]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.01 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.01 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.05 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.05 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.1 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.1 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.5 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.5 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.9 and n = 5 is 0.8807478524507327\n",
      "the confusion matrix whene eta = 0.9 and n = 5 is [[1023   60]\n",
      " [ 176  720]]\n",
      "the accuracy score whene eta = 0.01 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.01 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.05 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.05 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.1 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.1 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.5 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.5 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.9 and n = 7 is 0.8817584638706417\n",
      "the confusion matrix whene eta = 0.9 and n = 7 is [[1016   67]\n",
      " [ 167  729]]\n",
      "the accuracy score whene eta = 0.01 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.01 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.05 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.05 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.1 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.1 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.5 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.5 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.9 and n = 10 is 0.8913592723597776\n",
      "the confusion matrix whene eta = 0.9 and n = 10 is [[1024   59]\n",
      " [ 156  740]]\n",
      "the accuracy score whene eta = 0.01 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.01 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.05 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.05 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.1 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.1 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.5 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.5 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.9 and n = 20 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.9 and n = 20 is [[1031   52]\n",
      " [ 168  728]]\n",
      "the accuracy score whene eta = 0.01 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.01 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.05 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.05 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.1 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.1 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.5 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.5 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.9 and n = 30 is 0.8888327438100051\n",
      "the confusion matrix whene eta = 0.9 and n = 30 is [[1026   57]\n",
      " [ 163  733]]\n",
      "the accuracy score whene eta = 0.01 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.01 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.05 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.05 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.1 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.1 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.5 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.5 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n",
      "the accuracy score whene eta = 0.9 and n = 50 is 0.8832743810005053\n",
      "the confusion matrix whene eta = 0.9 and n = 50 is [[1026   57]\n",
      " [ 174  722]]\n"
     ]
    }
   ],
   "source": [
    "n = [1, 3, 5, 7, 10, 20,30, 50]\n",
    "eta = [0.01, 0.05, 0.1, 0.5, 0.9]\n",
    "precision_score_mat = {}\n",
    "confusion_matrix_mat = {}\n",
    "for i_n in n:\n",
    "    for j in eta:\n",
    "        W = np.zeros((x_train_df.shape[1], 1))\n",
    "        W = perceptron(x_train_df, y_train_df, W, j, i_n)\n",
    "        \n",
    "        pred_valid_y = []\n",
    "        for i in range(len(x_valid_df)):\n",
    "            val = np.dot(x_valid_df[i], W)\n",
    "\n",
    "            if val > 0:\n",
    "                pred_valid_y.append(1)\n",
    "            else:\n",
    "                pred_valid_y.append(0)\n",
    "        p_s = accuracy_score(y_valid_df,pred_valid_y)\n",
    "        C_m = confusion_matrix(y_valid_df,pred_valid_y)\n",
    "        print(f\"the accuracy score whene eta = {j} and n = {i_n} is {p_s}\")\n",
    "        precision_score_mat[str(i_n), str(j)] = p_s\n",
    "        print(f\"the confusion matrix whene eta = {j} and n = {i_n} is {C_m}\")\n",
    "        confusion_matrix_mat[str(i_n), str(j)] = C_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725d6a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 2021)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1133c264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '0.1') 0.8514401212733704\n",
      "('3', '0.1') 0.8807478524507327\n",
      "('5', '0.1') 0.8807478524507327\n",
      "('7', '0.1') 0.8817584638706417\n",
      "('10', '0.1') 0.8913592723597776\n",
      "('20', '0.1') 0.8888327438100051\n",
      "('30', '0.1') 0.8888327438100051\n",
      "('50', '0.1') 0.8832743810005053\n"
     ]
    }
   ],
   "source": [
    "iter_num = []\n",
    "prec = []\n",
    "for i in precision_score_mat.keys():\n",
    "    if i[1] == '0.1':\n",
    "        iter_num.append(i[0])\n",
    "        prec.append(precision_score_mat[i])\n",
    "        print(i, precision_score_mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be0d72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy score for different number of iteration for learning rate = 0.1 for perceptron')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iter_num, prec)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(\"Accuracy score for different number of iteration for learning rate = 0.1 for perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e4be",
   "metadata": {},
   "source": [
    "<h3> Logistic Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9402fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def calc_h(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8a6bd",
   "metadata": {},
   "source": [
    "<body> function to create a logistic regression model </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3240a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_inp, y, num_iter = 100, alpha = 0.1, lambda_value = 0.0000001):\n",
    "    cost_epochs = []\n",
    "    theta = np.zeros(X_inp.shape[1])\n",
    "    m = y.size\n",
    "    batchsize = 100\n",
    "    batches = floor(y.size/batchsize)\n",
    "    #lambda_value = 0.0000001\n",
    "    for epoch in range(num_iter):\n",
    "        cost_list = []\n",
    "        for i in range(batches):\n",
    "            s = i*batchsize\n",
    "            k= s+ batchsize\n",
    "            Y = y[s:k]\n",
    "            x = X_inp[s:k,:]\n",
    "            h = calc_h(x, theta)\n",
    "            lasso_reg_term = (lambda_value / 2 * batchsize) * sum(abs(theta))\n",
    "            cost = (-Y * np.log(h) - (1 - Y) * np.log(1 - h)).mean() + lasso_reg_term\n",
    "            cost_list.append(cost)\n",
    "\n",
    "            gradient = (np.dot(x.T, (h - Y)))/ batchsize  +  (lambda_value * abs(theta))\n",
    "            theta -= alpha * gradient\n",
    "\n",
    "        cost_epochs.append(np.average(cost_list))\n",
    "        if((epoch + 1 )%10 ==0):    \n",
    "            print(f\"After {epoch+1} epochs, Loss = {cost}\")\n",
    "            \n",
    "\n",
    "    #print('Adjusted coefficient: {}'.format(theta))\n",
    "    print(h.shape)\n",
    "    return theta, cost_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36912456",
   "metadata": {},
   "source": [
    "<body> training the logistic regression on various number of iterations </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835a45af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 epochs, Loss = 0.680182591932619\n",
      "After 20 epochs, Loss = 0.6518196287515539\n",
      "(100,)\n",
      "After 10 epochs, Loss = 0.680182591932619\n",
      "After 20 epochs, Loss = 0.6518196287515539\n",
      "After 30 epochs, Loss = 0.6273704927241284\n",
      "After 40 epochs, Loss = 0.6060520462298619\n",
      "After 50 epochs, Loss = 0.5872623826938489\n",
      "After 60 epochs, Loss = 0.5705390119711659\n",
      "After 70 epochs, Loss = 0.5555243016437804\n",
      "After 80 epochs, Loss = 0.541938969521321\n",
      "After 90 epochs, Loss = 0.5295623777279276\n",
      "After 100 epochs, Loss = 0.5182182626637097\n",
      "After 110 epochs, Loss = 0.507764129655649\n",
      "After 120 epochs, Loss = 0.49808356396380865\n",
      "After 130 epochs, Loss = 0.48908047928854376\n",
      "After 140 epochs, Loss = 0.48067485649211517\n",
      "After 150 epochs, Loss = 0.4727993952767928\n",
      "After 160 epochs, Loss = 0.46539712848006637\n",
      "After 170 epochs, Loss = 0.4584194547076961\n",
      "After 180 epochs, Loss = 0.4518245374309676\n",
      "After 190 epochs, Loss = 0.44557625942946033\n",
      "After 200 epochs, Loss = 0.43964320515692423\n",
      "(100,)\n",
      "After 10 epochs, Loss = 0.680182591932619\n",
      "After 20 epochs, Loss = 0.6518196287515539\n",
      "After 30 epochs, Loss = 0.6273704927241284\n",
      "After 40 epochs, Loss = 0.6060520462298619\n",
      "After 50 epochs, Loss = 0.5872623826938489\n",
      "After 60 epochs, Loss = 0.5705390119711659\n",
      "After 70 epochs, Loss = 0.5555243016437804\n",
      "After 80 epochs, Loss = 0.541938969521321\n",
      "After 90 epochs, Loss = 0.5295623777279276\n",
      "After 100 epochs, Loss = 0.5182182626637097\n",
      "After 110 epochs, Loss = 0.507764129655649\n",
      "After 120 epochs, Loss = 0.49808356396380865\n",
      "After 130 epochs, Loss = 0.48908047928854376\n",
      "After 140 epochs, Loss = 0.48067485649211517\n",
      "After 150 epochs, Loss = 0.4727993952767928\n",
      "After 160 epochs, Loss = 0.46539712848006637\n",
      "After 170 epochs, Loss = 0.4584194547076961\n",
      "After 180 epochs, Loss = 0.4518245374309676\n",
      "After 190 epochs, Loss = 0.44557625942946033\n",
      "After 200 epochs, Loss = 0.43964320515692423\n",
      "After 210 epochs, Loss = 0.4339979277619401\n",
      "After 220 epochs, Loss = 0.42861625016169447\n",
      "After 230 epochs, Loss = 0.42347684518747464\n",
      "After 240 epochs, Loss = 0.4185608759838345\n",
      "After 250 epochs, Loss = 0.41385147510196624\n",
      "After 260 epochs, Loss = 0.4093336369804187\n",
      "After 270 epochs, Loss = 0.40499386241123775\n",
      "After 280 epochs, Loss = 0.40081997431500105\n",
      "After 290 epochs, Loss = 0.3968010344017669\n",
      "After 300 epochs, Loss = 0.39292715667168804\n",
      "After 310 epochs, Loss = 0.38918927575795226\n",
      "After 320 epochs, Loss = 0.38557916613395954\n",
      "After 330 epochs, Loss = 0.3820892874470769\n",
      "After 340 epochs, Loss = 0.3787127847898408\n",
      "After 350 epochs, Loss = 0.3754433684029549\n",
      "After 360 epochs, Loss = 0.37227522465560325\n",
      "After 370 epochs, Loss = 0.36920299365029535\n",
      "After 380 epochs, Loss = 0.36622179102068203\n",
      "After 390 epochs, Loss = 0.3633269646549072\n",
      "After 400 epochs, Loss = 0.360514262420347\n",
      "After 410 epochs, Loss = 0.3577797411158695\n",
      "After 420 epochs, Loss = 0.3551196956385052\n",
      "After 430 epochs, Loss = 0.35253069367486994\n",
      "After 440 epochs, Loss = 0.35000953733951035\n",
      "After 450 epochs, Loss = 0.34755323592549586\n",
      "After 460 epochs, Loss = 0.3451590021564165\n",
      "After 470 epochs, Loss = 0.34282425154771606\n",
      "After 480 epochs, Loss = 0.34054650831475686\n",
      "After 490 epochs, Loss = 0.338323455183687\n",
      "After 500 epochs, Loss = 0.33615287553142376\n",
      "After 510 epochs, Loss = 0.334032749116565\n",
      "After 520 epochs, Loss = 0.3319611060270257\n",
      "After 530 epochs, Loss = 0.32993615478409577\n",
      "After 540 epochs, Loss = 0.3279561176580729\n",
      "After 550 epochs, Loss = 0.3260194007033547\n",
      "After 560 epochs, Loss = 0.3241243912218205\n",
      "After 570 epochs, Loss = 0.32226960263498916\n",
      "After 580 epochs, Loss = 0.3204536114151958\n",
      "After 590 epochs, Loss = 0.3186750799546421\n",
      "After 600 epochs, Loss = 0.31693273861017734\n",
      "After 610 epochs, Loss = 0.3152253994961318\n",
      "After 620 epochs, Loss = 0.313551963645704\n",
      "After 630 epochs, Loss = 0.31191129303300896\n",
      "After 640 epochs, Loss = 0.31030232933328555\n",
      "After 650 epochs, Loss = 0.30872406574032857\n",
      "After 660 epochs, Loss = 0.30717559077485024\n",
      "After 670 epochs, Loss = 0.30565604777967254\n",
      "After 680 epochs, Loss = 0.3041645113465122\n",
      "After 690 epochs, Loss = 0.30270011220026194\n",
      "After 700 epochs, Loss = 0.30126203818770014\n",
      "After 710 epochs, Loss = 0.299849511703305\n",
      "After 720 epochs, Loss = 0.29846180248661835\n",
      "After 730 epochs, Loss = 0.29709817922151754\n",
      "After 740 epochs, Loss = 0.2957579620362397\n",
      "After 750 epochs, Loss = 0.2944405126725743\n",
      "After 760 epochs, Loss = 0.2931451968589595\n",
      "After 770 epochs, Loss = 0.29187140714860427\n",
      "After 780 epochs, Loss = 0.2906185849380522\n",
      "After 790 epochs, Loss = 0.2893861496336295\n",
      "After 800 epochs, Loss = 0.2881735731465985\n",
      "After 810 epochs, Loss = 0.28698032497054354\n",
      "After 820 epochs, Loss = 0.2858059066433671\n",
      "After 830 epochs, Loss = 0.28464983330575466\n",
      "After 840 epochs, Loss = 0.28351169219054545\n",
      "After 850 epochs, Loss = 0.2823910016834218\n",
      "After 860 epochs, Loss = 0.2812873542394385\n",
      "After 870 epochs, Loss = 0.28020028676200454\n",
      "After 880 epochs, Loss = 0.2791293980992104\n",
      "After 890 epochs, Loss = 0.27807429224515784\n",
      "After 900 epochs, Loss = 0.2770345915441468\n",
      "After 910 epochs, Loss = 0.2760099409452121\n",
      "After 920 epochs, Loss = 0.2750000023126857\n",
      "After 930 epochs, Loss = 0.2740044083539814\n",
      "After 940 epochs, Loss = 0.27302282808726946\n",
      "After 950 epochs, Loss = 0.2720549465834959\n",
      "After 960 epochs, Loss = 0.27110048592809377\n",
      "After 970 epochs, Loss = 0.27015911026986483\n",
      "After 980 epochs, Loss = 0.2692305633457126\n",
      "After 990 epochs, Loss = 0.26831454678998534\n",
      "After 1000 epochs, Loss = 0.26741077427387544\n",
      "After 1010 epochs, Loss = 0.26651897652352613\n",
      "After 1020 epochs, Loss = 0.26563889942574914\n",
      "After 1030 epochs, Loss = 0.2647702889015616\n",
      "After 1040 epochs, Loss = 0.2639129064086342\n",
      "After 1050 epochs, Loss = 0.2630665149379724\n",
      "After 1060 epochs, Loss = 0.26223088528455923\n",
      "After 1070 epochs, Loss = 0.26140578602734477\n",
      "After 1080 epochs, Loss = 0.26059100091211274\n",
      "After 1090 epochs, Loss = 0.2597863197678567\n",
      "After 1100 epochs, Loss = 0.25899154754590614\n",
      "After 1110 epochs, Loss = 0.25820652019270285\n",
      "After 1120 epochs, Loss = 0.257431014398731\n",
      "After 1130 epochs, Loss = 0.2566648359440346\n",
      "After 1140 epochs, Loss = 0.25590779831242183\n",
      "After 1150 epochs, Loss = 0.2551597323050544\n",
      "After 1160 epochs, Loss = 0.2544204622748235\n",
      "After 1170 epochs, Loss = 0.25368981275261326\n",
      "After 1180 epochs, Loss = 0.252967618335195\n",
      "After 1190 epochs, Loss = 0.25225373464729006\n",
      "After 1200 epochs, Loss = 0.25154805526187163\n",
      "After 1210 epochs, Loss = 0.2508503620961792\n",
      "After 1220 epochs, Loss = 0.25016050730560146\n",
      "After 1230 epochs, Loss = 0.24947834679972208\n",
      "After 1240 epochs, Loss = 0.24880374249517723\n",
      "After 1250 epochs, Loss = 0.24813655645402113\n",
      "After 1260 epochs, Loss = 0.2474767189003882\n",
      "After 1270 epochs, Loss = 0.2468240793432228\n",
      "After 1280 epochs, Loss = 0.24617847715772678\n",
      "After 1290 epochs, Loss = 0.24553977739077676\n",
      "After 1300 epochs, Loss = 0.24490785898172582\n",
      "After 1310 epochs, Loss = 0.24428260390332437\n",
      "After 1320 epochs, Loss = 0.24366389695191612\n",
      "After 1330 epochs, Loss = 0.2430516256623532\n",
      "After 1340 epochs, Loss = 0.24244568022601065\n",
      "After 1350 epochs, Loss = 0.24184596064631794\n",
      "After 1360 epochs, Loss = 0.24125236115407364\n",
      "After 1370 epochs, Loss = 0.24066477327317518\n",
      "After 1380 epochs, Loss = 0.2400830995838222\n",
      "After 1390 epochs, Loss = 0.23950724091137643\n",
      "After 1400 epochs, Loss = 0.23893711523437378\n",
      "After 1410 epochs, Loss = 0.2383726271515153\n",
      "After 1420 epochs, Loss = 0.2378136771688975\n",
      "After 1430 epochs, Loss = 0.23726017215786313\n",
      "After 1440 epochs, Loss = 0.2367120249578987\n",
      "After 1450 epochs, Loss = 0.2361691503337364\n",
      "After 1460 epochs, Loss = 0.23563146492178824\n",
      "After 1470 epochs, Loss = 0.23509889152671087\n",
      "After 1480 epochs, Loss = 0.2345713716681381\n",
      "After 1490 epochs, Loss = 0.2340488322924961\n",
      "After 1500 epochs, Loss = 0.2335311664537416\n",
      "After 1510 epochs, Loss = 0.23301829939979096\n",
      "After 1520 epochs, Loss = 0.23251015796422553\n",
      "After 1530 epochs, Loss = 0.23200667073448833\n",
      "After 1540 epochs, Loss = 0.23150777961520386\n",
      "After 1550 epochs, Loss = 0.2310134037553895\n",
      "After 1560 epochs, Loss = 0.23052347774849685\n",
      "After 1570 epochs, Loss = 0.2300379509373657\n",
      "After 1580 epochs, Loss = 0.22955674184377362\n",
      "After 1590 epochs, Loss = 0.22907978728454995\n",
      "After 1600 epochs, Loss = 0.22860702535840813\n",
      "After 1610 epochs, Loss = 0.22813839541311404\n",
      "After 1620 epochs, Loss = 0.22767384329744772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1630 epochs, Loss = 0.22721332262961583\n",
      "After 1640 epochs, Loss = 0.2267567729184535\n",
      "After 1650 epochs, Loss = 0.22630413405588237\n",
      "After 1660 epochs, Loss = 0.225855355079352\n",
      "After 1670 epochs, Loss = 0.225410384529583\n",
      "After 1680 epochs, Loss = 0.22496915762947753\n",
      "After 1690 epochs, Loss = 0.2245316191770779\n",
      "After 1700 epochs, Loss = 0.22409771812819682\n",
      "After 1710 epochs, Loss = 0.22366740471822008\n",
      "After 1720 epochs, Loss = 0.2232406301297969\n",
      "After 1730 epochs, Loss = 0.22281735649303758\n",
      "After 1740 epochs, Loss = 0.2223975292496948\n",
      "After 1750 epochs, Loss = 0.2219810998312391\n",
      "After 1760 epochs, Loss = 0.22156802298765377\n",
      "After 1770 epochs, Loss = 0.22115825430845118\n",
      "After 1780 epochs, Loss = 0.22075175020293214\n",
      "After 1790 epochs, Loss = 0.22034846788100204\n",
      "After 1800 epochs, Loss = 0.21994836533452572\n",
      "After 1810 epochs, Loss = 0.21955140220031294\n",
      "After 1820 epochs, Loss = 0.219157543643049\n",
      "After 1830 epochs, Loss = 0.21876674654623188\n",
      "After 1840 epochs, Loss = 0.2183789774679938\n",
      "After 1850 epochs, Loss = 0.2179941923572368\n",
      "After 1860 epochs, Loss = 0.21761235678306423\n",
      "After 1870 epochs, Loss = 0.21723342743012639\n",
      "After 1880 epochs, Loss = 0.21685736793231647\n",
      "After 1890 epochs, Loss = 0.21648414813212097\n",
      "After 1900 epochs, Loss = 0.2161137388078167\n",
      "After 1910 epochs, Loss = 0.21574609400181363\n",
      "After 1920 epochs, Loss = 0.21538118721602054\n",
      "After 1930 epochs, Loss = 0.21501898087689125\n",
      "After 1940 epochs, Loss = 0.21465943913322644\n",
      "After 1950 epochs, Loss = 0.21430252978402617\n",
      "After 1960 epochs, Loss = 0.21394822824989468\n",
      "After 1970 epochs, Loss = 0.21359650871652197\n",
      "After 1980 epochs, Loss = 0.2132473499358974\n",
      "After 1990 epochs, Loss = 0.21290070002899186\n",
      "After 2000 epochs, Loss = 0.2125565454875279\n",
      "(100,)\n",
      "After 10 epochs, Loss = 0.680182591932619\n",
      "After 20 epochs, Loss = 0.6518196287515539\n",
      "After 30 epochs, Loss = 0.6273704927241284\n",
      "After 40 epochs, Loss = 0.6060520462298619\n",
      "After 50 epochs, Loss = 0.5872623826938489\n",
      "After 60 epochs, Loss = 0.5705390119711659\n",
      "After 70 epochs, Loss = 0.5555243016437804\n",
      "After 80 epochs, Loss = 0.541938969521321\n",
      "After 90 epochs, Loss = 0.5295623777279276\n",
      "After 100 epochs, Loss = 0.5182182626637097\n",
      "After 110 epochs, Loss = 0.507764129655649\n",
      "After 120 epochs, Loss = 0.49808356396380865\n",
      "After 130 epochs, Loss = 0.48908047928854376\n",
      "After 140 epochs, Loss = 0.48067485649211517\n",
      "After 150 epochs, Loss = 0.4727993952767928\n",
      "After 160 epochs, Loss = 0.46539712848006637\n",
      "After 170 epochs, Loss = 0.4584194547076961\n",
      "After 180 epochs, Loss = 0.4518245374309676\n",
      "After 190 epochs, Loss = 0.44557625942946033\n",
      "After 200 epochs, Loss = 0.43964320515692423\n",
      "After 210 epochs, Loss = 0.4339979277619401\n",
      "After 220 epochs, Loss = 0.42861625016169447\n",
      "After 230 epochs, Loss = 0.42347684518747464\n",
      "After 240 epochs, Loss = 0.4185608759838345\n",
      "After 250 epochs, Loss = 0.41385147510196624\n",
      "After 260 epochs, Loss = 0.4093336369804187\n",
      "After 270 epochs, Loss = 0.40499386241123775\n",
      "After 280 epochs, Loss = 0.40081997431500105\n",
      "After 290 epochs, Loss = 0.3968010344017669\n",
      "After 300 epochs, Loss = 0.39292715667168804\n",
      "After 310 epochs, Loss = 0.38918927575795226\n",
      "After 320 epochs, Loss = 0.38557916613395954\n",
      "After 330 epochs, Loss = 0.3820892874470769\n",
      "After 340 epochs, Loss = 0.3787127847898408\n",
      "After 350 epochs, Loss = 0.3754433684029549\n",
      "After 360 epochs, Loss = 0.37227522465560325\n",
      "After 370 epochs, Loss = 0.36920299365029535\n",
      "After 380 epochs, Loss = 0.36622179102068203\n",
      "After 390 epochs, Loss = 0.3633269646549072\n",
      "After 400 epochs, Loss = 0.360514262420347\n",
      "After 410 epochs, Loss = 0.3577797411158695\n",
      "After 420 epochs, Loss = 0.3551196956385052\n",
      "After 430 epochs, Loss = 0.35253069367486994\n",
      "After 440 epochs, Loss = 0.35000953733951035\n",
      "After 450 epochs, Loss = 0.34755323592549586\n",
      "After 460 epochs, Loss = 0.3451590021564165\n",
      "After 470 epochs, Loss = 0.34282425154771606\n",
      "After 480 epochs, Loss = 0.34054650831475686\n",
      "After 490 epochs, Loss = 0.338323455183687\n",
      "After 500 epochs, Loss = 0.33615287553142376\n",
      "After 510 epochs, Loss = 0.334032749116565\n",
      "After 520 epochs, Loss = 0.3319611060270257\n",
      "After 530 epochs, Loss = 0.32993615478409577\n",
      "After 540 epochs, Loss = 0.3279561176580729\n",
      "After 550 epochs, Loss = 0.3260194007033547\n",
      "After 560 epochs, Loss = 0.3241243912218205\n",
      "After 570 epochs, Loss = 0.32226960263498916\n",
      "After 580 epochs, Loss = 0.3204536114151958\n",
      "After 590 epochs, Loss = 0.3186750799546421\n",
      "After 600 epochs, Loss = 0.31693273861017734\n",
      "After 610 epochs, Loss = 0.3152253994961318\n",
      "After 620 epochs, Loss = 0.313551963645704\n",
      "After 630 epochs, Loss = 0.31191129303300896\n",
      "After 640 epochs, Loss = 0.31030232933328555\n",
      "After 650 epochs, Loss = 0.30872406574032857\n",
      "After 660 epochs, Loss = 0.30717559077485024\n",
      "After 670 epochs, Loss = 0.30565604777967254\n",
      "After 680 epochs, Loss = 0.3041645113465122\n",
      "After 690 epochs, Loss = 0.30270011220026194\n",
      "After 700 epochs, Loss = 0.30126203818770014\n",
      "After 710 epochs, Loss = 0.299849511703305\n",
      "After 720 epochs, Loss = 0.29846180248661835\n",
      "After 730 epochs, Loss = 0.29709817922151754\n",
      "After 740 epochs, Loss = 0.2957579620362397\n",
      "After 750 epochs, Loss = 0.2944405126725743\n",
      "After 760 epochs, Loss = 0.2931451968589595\n",
      "After 770 epochs, Loss = 0.29187140714860427\n",
      "After 780 epochs, Loss = 0.2906185849380522\n",
      "After 790 epochs, Loss = 0.2893861496336295\n",
      "After 800 epochs, Loss = 0.2881735731465985\n",
      "After 810 epochs, Loss = 0.28698032497054354\n",
      "After 820 epochs, Loss = 0.2858059066433671\n",
      "After 830 epochs, Loss = 0.28464983330575466\n",
      "After 840 epochs, Loss = 0.28351169219054545\n",
      "After 850 epochs, Loss = 0.2823910016834218\n",
      "After 860 epochs, Loss = 0.2812873542394385\n",
      "After 870 epochs, Loss = 0.28020028676200454\n",
      "After 880 epochs, Loss = 0.2791293980992104\n",
      "After 890 epochs, Loss = 0.27807429224515784\n",
      "After 900 epochs, Loss = 0.2770345915441468\n",
      "After 910 epochs, Loss = 0.2760099409452121\n",
      "After 920 epochs, Loss = 0.2750000023126857\n",
      "After 930 epochs, Loss = 0.2740044083539814\n",
      "After 940 epochs, Loss = 0.27302282808726946\n",
      "After 950 epochs, Loss = 0.2720549465834959\n",
      "After 960 epochs, Loss = 0.27110048592809377\n",
      "After 970 epochs, Loss = 0.27015911026986483\n",
      "After 980 epochs, Loss = 0.2692305633457126\n",
      "After 990 epochs, Loss = 0.26831454678998534\n",
      "After 1000 epochs, Loss = 0.26741077427387544\n",
      "After 1010 epochs, Loss = 0.26651897652352613\n",
      "After 1020 epochs, Loss = 0.26563889942574914\n",
      "After 1030 epochs, Loss = 0.2647702889015616\n",
      "After 1040 epochs, Loss = 0.2639129064086342\n",
      "After 1050 epochs, Loss = 0.2630665149379724\n",
      "After 1060 epochs, Loss = 0.26223088528455923\n",
      "After 1070 epochs, Loss = 0.26140578602734477\n",
      "After 1080 epochs, Loss = 0.26059100091211274\n",
      "After 1090 epochs, Loss = 0.2597863197678567\n",
      "After 1100 epochs, Loss = 0.25899154754590614\n",
      "After 1110 epochs, Loss = 0.25820652019270285\n",
      "After 1120 epochs, Loss = 0.257431014398731\n",
      "After 1130 epochs, Loss = 0.2566648359440346\n",
      "After 1140 epochs, Loss = 0.25590779831242183\n",
      "After 1150 epochs, Loss = 0.2551597323050544\n",
      "After 1160 epochs, Loss = 0.2544204622748235\n",
      "After 1170 epochs, Loss = 0.25368981275261326\n",
      "After 1180 epochs, Loss = 0.252967618335195\n",
      "After 1190 epochs, Loss = 0.25225373464729006\n",
      "After 1200 epochs, Loss = 0.25154805526187163\n",
      "After 1210 epochs, Loss = 0.2508503620961792\n",
      "After 1220 epochs, Loss = 0.25016050730560146\n",
      "After 1230 epochs, Loss = 0.24947834679972208\n",
      "After 1240 epochs, Loss = 0.24880374249517723\n",
      "After 1250 epochs, Loss = 0.24813655645402113\n",
      "After 1260 epochs, Loss = 0.2474767189003882\n",
      "After 1270 epochs, Loss = 0.2468240793432228\n",
      "After 1280 epochs, Loss = 0.24617847715772678\n",
      "After 1290 epochs, Loss = 0.24553977739077676\n",
      "After 1300 epochs, Loss = 0.24490785898172582\n",
      "After 1310 epochs, Loss = 0.24428260390332437\n",
      "After 1320 epochs, Loss = 0.24366389695191612\n",
      "After 1330 epochs, Loss = 0.2430516256623532\n",
      "After 1340 epochs, Loss = 0.24244568022601065\n",
      "After 1350 epochs, Loss = 0.24184596064631794\n",
      "After 1360 epochs, Loss = 0.24125236115407364\n",
      "After 1370 epochs, Loss = 0.24066477327317518\n",
      "After 1380 epochs, Loss = 0.2400830995838222\n",
      "After 1390 epochs, Loss = 0.23950724091137643\n",
      "After 1400 epochs, Loss = 0.23893711523437378\n",
      "After 1410 epochs, Loss = 0.2383726271515153\n",
      "After 1420 epochs, Loss = 0.2378136771688975\n",
      "After 1430 epochs, Loss = 0.23726017215786313\n",
      "After 1440 epochs, Loss = 0.2367120249578987\n",
      "After 1450 epochs, Loss = 0.2361691503337364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1460 epochs, Loss = 0.23563146492178824\n",
      "After 1470 epochs, Loss = 0.23509889152671087\n",
      "After 1480 epochs, Loss = 0.2345713716681381\n",
      "After 1490 epochs, Loss = 0.2340488322924961\n",
      "After 1500 epochs, Loss = 0.2335311664537416\n",
      "After 1510 epochs, Loss = 0.23301829939979096\n",
      "After 1520 epochs, Loss = 0.23251015796422553\n",
      "After 1530 epochs, Loss = 0.23200667073448833\n",
      "After 1540 epochs, Loss = 0.23150777961520386\n",
      "After 1550 epochs, Loss = 0.2310134037553895\n",
      "After 1560 epochs, Loss = 0.23052347774849685\n",
      "After 1570 epochs, Loss = 0.2300379509373657\n",
      "After 1580 epochs, Loss = 0.22955674184377362\n",
      "After 1590 epochs, Loss = 0.22907978728454995\n",
      "After 1600 epochs, Loss = 0.22860702535840813\n",
      "After 1610 epochs, Loss = 0.22813839541311404\n",
      "After 1620 epochs, Loss = 0.22767384329744772\n",
      "After 1630 epochs, Loss = 0.22721332262961583\n",
      "After 1640 epochs, Loss = 0.2267567729184535\n",
      "After 1650 epochs, Loss = 0.22630413405588237\n",
      "After 1660 epochs, Loss = 0.225855355079352\n",
      "After 1670 epochs, Loss = 0.225410384529583\n",
      "After 1680 epochs, Loss = 0.22496915762947753\n",
      "After 1690 epochs, Loss = 0.2245316191770779\n",
      "After 1700 epochs, Loss = 0.22409771812819682\n",
      "After 1710 epochs, Loss = 0.22366740471822008\n",
      "After 1720 epochs, Loss = 0.2232406301297969\n",
      "After 1730 epochs, Loss = 0.22281735649303758\n",
      "After 1740 epochs, Loss = 0.2223975292496948\n",
      "After 1750 epochs, Loss = 0.2219810998312391\n",
      "After 1760 epochs, Loss = 0.22156802298765377\n",
      "After 1770 epochs, Loss = 0.22115825430845118\n",
      "After 1780 epochs, Loss = 0.22075175020293214\n",
      "After 1790 epochs, Loss = 0.22034846788100204\n",
      "After 1800 epochs, Loss = 0.21994836533452572\n",
      "After 1810 epochs, Loss = 0.21955140220031294\n",
      "After 1820 epochs, Loss = 0.219157543643049\n",
      "After 1830 epochs, Loss = 0.21876674654623188\n",
      "After 1840 epochs, Loss = 0.2183789774679938\n",
      "After 1850 epochs, Loss = 0.2179941923572368\n",
      "After 1860 epochs, Loss = 0.21761235678306423\n",
      "After 1870 epochs, Loss = 0.21723342743012639\n",
      "After 1880 epochs, Loss = 0.21685736793231647\n",
      "After 1890 epochs, Loss = 0.21648414813212097\n",
      "After 1900 epochs, Loss = 0.2161137388078167\n",
      "After 1910 epochs, Loss = 0.21574609400181363\n",
      "After 1920 epochs, Loss = 0.21538118721602054\n",
      "After 1930 epochs, Loss = 0.21501898087689125\n",
      "After 1940 epochs, Loss = 0.21465943913322644\n",
      "After 1950 epochs, Loss = 0.21430252978402617\n",
      "After 1960 epochs, Loss = 0.21394822824989468\n",
      "After 1970 epochs, Loss = 0.21359650871652197\n",
      "After 1980 epochs, Loss = 0.2132473499358974\n",
      "After 1990 epochs, Loss = 0.21290070002899186\n",
      "After 2000 epochs, Loss = 0.2125565454875279\n",
      "After 2010 epochs, Loss = 0.21221484315840536\n",
      "After 2020 epochs, Loss = 0.2118755623618718\n",
      "After 2030 epochs, Loss = 0.21153867493830528\n",
      "After 2040 epochs, Loss = 0.2112041546689501\n",
      "After 2050 epochs, Loss = 0.2108719821135681\n",
      "After 2060 epochs, Loss = 0.2105421211734851\n",
      "After 2070 epochs, Loss = 0.21021454546700918\n",
      "After 2080 epochs, Loss = 0.20988922903430446\n",
      "After 2090 epochs, Loss = 0.2095661641181742\n",
      "After 2100 epochs, Loss = 0.20924532486161035\n",
      "After 2110 epochs, Loss = 0.20892667560262865\n",
      "After 2120 epochs, Loss = 0.20861020103765143\n",
      "After 2130 epochs, Loss = 0.20829586180513288\n",
      "After 2140 epochs, Loss = 0.20798363430375089\n",
      "After 2150 epochs, Loss = 0.20767350745456462\n",
      "After 2160 epochs, Loss = 0.20736545744665708\n",
      "After 2170 epochs, Loss = 0.20705945897575642\n",
      "After 2180 epochs, Loss = 0.20675548546661182\n",
      "After 2190 epochs, Loss = 0.2064535213947114\n",
      "After 2200 epochs, Loss = 0.20615353823602744\n",
      "After 2210 epochs, Loss = 0.20585552423870768\n",
      "After 2220 epochs, Loss = 0.205559449145831\n",
      "After 2230 epochs, Loss = 0.20526529216066453\n",
      "After 2240 epochs, Loss = 0.20497303687925406\n",
      "After 2250 epochs, Loss = 0.20468266312372616\n",
      "After 2260 epochs, Loss = 0.20439415554359283\n",
      "After 2270 epochs, Loss = 0.20410748758504682\n",
      "After 2280 epochs, Loss = 0.2038226430365775\n",
      "After 2290 epochs, Loss = 0.20353960075502855\n",
      "After 2300 epochs, Loss = 0.20325834169381052\n",
      "After 2310 epochs, Loss = 0.2029788435757975\n",
      "After 2320 epochs, Loss = 0.2027010884010268\n",
      "After 2330 epochs, Loss = 0.20242505842767328\n",
      "After 2340 epochs, Loss = 0.20215073616728832\n",
      "After 2350 epochs, Loss = 0.20187810438014497\n",
      "After 2360 epochs, Loss = 0.20160715843751803\n",
      "After 2370 epochs, Loss = 0.20133787202354428\n",
      "After 2380 epochs, Loss = 0.2010702258024153\n",
      "After 2390 epochs, Loss = 0.20080420348474648\n",
      "After 2400 epochs, Loss = 0.20053979296827062\n",
      "After 2410 epochs, Loss = 0.20027697848497295\n",
      "After 2420 epochs, Loss = 0.20001574039054945\n",
      "After 2430 epochs, Loss = 0.19975606619701097\n",
      "After 2440 epochs, Loss = 0.19949796275817044\n",
      "After 2450 epochs, Loss = 0.19924139136999033\n",
      "After 2460 epochs, Loss = 0.19898633603320137\n",
      "After 2470 epochs, Loss = 0.19873278216409693\n",
      "After 2480 epochs, Loss = 0.1984807153757109\n",
      "After 2490 epochs, Loss = 0.1982301214743958\n",
      "After 2500 epochs, Loss = 0.19798098645647078\n",
      "After 2510 epochs, Loss = 0.1977332965049436\n",
      "After 2520 epochs, Loss = 0.19748703798629752\n",
      "After 2530 epochs, Loss = 0.19724221606453698\n",
      "After 2540 epochs, Loss = 0.19699880571904382\n",
      "After 2550 epochs, Loss = 0.19675678700783178\n",
      "After 2560 epochs, Loss = 0.19651615135038583\n",
      "After 2570 epochs, Loss = 0.19627689376642873\n",
      "After 2580 epochs, Loss = 0.19603899146712883\n",
      "After 2590 epochs, Loss = 0.19580243875764336\n",
      "After 2600 epochs, Loss = 0.19556721467917174\n",
      "After 2610 epochs, Loss = 0.19533330711455302\n",
      "After 2620 epochs, Loss = 0.19510070410143987\n",
      "After 2630 epochs, Loss = 0.19486939382974308\n",
      "After 2640 epochs, Loss = 0.19463938261705183\n",
      "After 2650 epochs, Loss = 0.19441065096996232\n",
      "After 2660 epochs, Loss = 0.19418318120642686\n",
      "After 2670 epochs, Loss = 0.19395695836401614\n",
      "After 2680 epochs, Loss = 0.1937319713571492\n",
      "After 2690 epochs, Loss = 0.19350821480766398\n",
      "After 2700 epochs, Loss = 0.19328569774615684\n",
      "After 2710 epochs, Loss = 0.19306439367827288\n",
      "After 2720 epochs, Loss = 0.19284428252989178\n",
      "After 2730 epochs, Loss = 0.19262535388290492\n",
      "After 2740 epochs, Loss = 0.19240759744652122\n",
      "After 2750 epochs, Loss = 0.19219100305523604\n",
      "After 2760 epochs, Loss = 0.19197556066685698\n",
      "After 2770 epochs, Loss = 0.19176126036056798\n",
      "After 2780 epochs, Loss = 0.19154809233503198\n",
      "After 2790 epochs, Loss = 0.19133604690652833\n",
      "After 2800 epochs, Loss = 0.19112511450712583\n",
      "After 2810 epochs, Loss = 0.19091529679865613\n",
      "After 2820 epochs, Loss = 0.19070657856117793\n",
      "After 2830 epochs, Loss = 0.19049895009976242\n",
      "After 2840 epochs, Loss = 0.1902923974619103\n",
      "After 2850 epochs, Loss = 0.19008691163237137\n",
      "After 2860 epochs, Loss = 0.18988248370143257\n",
      "After 2870 epochs, Loss = 0.1896791048633184\n",
      "After 2880 epochs, Loss = 0.1894767664146139\n",
      "After 2890 epochs, Loss = 0.18927545978091548\n",
      "After 2900 epochs, Loss = 0.18907518404071394\n",
      "After 2910 epochs, Loss = 0.188875923174233\n",
      "After 2920 epochs, Loss = 0.18867766887136758\n",
      "After 2930 epochs, Loss = 0.18848046148185743\n",
      "After 2940 epochs, Loss = 0.1882842510017224\n",
      "After 2950 epochs, Loss = 0.18808902274303524\n",
      "After 2960 epochs, Loss = 0.1878947687669956\n",
      "After 2970 epochs, Loss = 0.18770148271649173\n",
      "After 2980 epochs, Loss = 0.18750918616915332\n",
      "After 2990 epochs, Loss = 0.18731784543864705\n",
      "After 3000 epochs, Loss = 0.18712744817221272\n",
      "After 3010 epochs, Loss = 0.1869379868650374\n",
      "After 3020 epochs, Loss = 0.18674946571692588\n",
      "After 3030 epochs, Loss = 0.18656186624183188\n",
      "After 3040 epochs, Loss = 0.18637518610424955\n",
      "After 3050 epochs, Loss = 0.18618942649313458\n",
      "After 3060 epochs, Loss = 0.18600457878219417\n",
      "After 3070 epochs, Loss = 0.1858206237186083\n",
      "After 3080 epochs, Loss = 0.18563755435467932\n",
      "After 3090 epochs, Loss = 0.18545536381798108\n",
      "After 3100 epochs, Loss = 0.18527404531030228\n",
      "After 3110 epochs, Loss = 0.18509359210660903\n",
      "After 3120 epochs, Loss = 0.18491399755402368\n",
      "After 3130 epochs, Loss = 0.18473525507082408\n",
      "After 3140 epochs, Loss = 0.18455735814545704\n",
      "After 3150 epochs, Loss = 0.18438030033557035\n",
      "After 3160 epochs, Loss = 0.1842040752670606\n",
      "After 3170 epochs, Loss = 0.1840286766331367\n",
      "After 3180 epochs, Loss = 0.18385409819339962\n",
      "After 3190 epochs, Loss = 0.18368033377293774\n",
      "After 3200 epochs, Loss = 0.18350737726143682\n",
      "After 3210 epochs, Loss = 0.1833352354009608\n",
      "After 3220 epochs, Loss = 0.18316390025571427\n",
      "After 3230 epochs, Loss = 0.18299335504855907\n",
      "After 3240 epochs, Loss = 0.1828236021601737\n",
      "After 3250 epochs, Loss = 0.18265464221253117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 3260 epochs, Loss = 0.18248646769180688\n",
      "After 3270 epochs, Loss = 0.18231906320857016\n",
      "After 3280 epochs, Loss = 0.18215242332817533\n",
      "After 3290 epochs, Loss = 0.1819865626602788\n",
      "After 3300 epochs, Loss = 0.18182147539769766\n",
      "After 3310 epochs, Loss = 0.18165713688631704\n",
      "After 3320 epochs, Loss = 0.1814935375530389\n",
      "After 3330 epochs, Loss = 0.1813306720522628\n",
      "After 3340 epochs, Loss = 0.18116854432186374\n",
      "After 3350 epochs, Loss = 0.18100714379503507\n",
      "After 3360 epochs, Loss = 0.18084646142737842\n",
      "After 3370 epochs, Loss = 0.18068649208317114\n",
      "After 3380 epochs, Loss = 0.18052723226710626\n",
      "After 3390 epochs, Loss = 0.1803686861890996\n",
      "After 3400 epochs, Loss = 0.1802108380589592\n",
      "After 3410 epochs, Loss = 0.18005369039352173\n",
      "After 3420 epochs, Loss = 0.17989725072709037\n",
      "After 3430 epochs, Loss = 0.17974149424721744\n",
      "After 3440 epochs, Loss = 0.179586416161586\n",
      "After 3450 epochs, Loss = 0.17943201172441905\n",
      "After 3460 epochs, Loss = 0.17927827623587597\n",
      "After 3470 epochs, Loss = 0.17912520504147722\n",
      "After 3480 epochs, Loss = 0.17897279353153733\n",
      "After 3490 epochs, Loss = 0.17882103714060724\n",
      "After 3500 epochs, Loss = 0.1786699313469247\n",
      "After 3510 epochs, Loss = 0.17851947167187412\n",
      "After 3520 epochs, Loss = 0.17836965367945218\n",
      "After 3530 epochs, Loss = 0.17822047297574578\n",
      "After 3540 epochs, Loss = 0.17807192520841353\n",
      "After 3550 epochs, Loss = 0.17792400798565494\n",
      "After 3560 epochs, Loss = 0.17777672023171143\n",
      "After 3570 epochs, Loss = 0.17763005261436443\n",
      "After 3580 epochs, Loss = 0.17748400094246522\n",
      "After 3590 epochs, Loss = 0.17733856106397045\n",
      "After 3600 epochs, Loss = 0.17719372886547052\n",
      "After 3610 epochs, Loss = 0.1770495052754653\n",
      "After 3620 epochs, Loss = 0.1769058869080098\n",
      "After 3630 epochs, Loss = 0.17676286489358506\n",
      "After 3640 epochs, Loss = 0.1766204345893188\n",
      "After 3650 epochs, Loss = 0.17647859195339707\n",
      "After 3660 epochs, Loss = 0.17633733309468877\n",
      "After 3670 epochs, Loss = 0.17619665415758498\n",
      "After 3680 epochs, Loss = 0.17605655132157919\n",
      "After 3690 epochs, Loss = 0.17591702080085586\n",
      "After 3700 epochs, Loss = 0.17577805884388303\n",
      "After 3710 epochs, Loss = 0.1756396617330114\n",
      "After 3720 epochs, Loss = 0.17550182578407988\n",
      "After 3730 epochs, Loss = 0.17536454734602586\n",
      "After 3740 epochs, Loss = 0.17522782280050123\n",
      "After 3750 epochs, Loss = 0.17509164856149467\n",
      "After 3760 epochs, Loss = 0.17495602107495867\n",
      "After 3770 epochs, Loss = 0.17482093681844138\n",
      "After 3780 epochs, Loss = 0.1746863923007246\n",
      "After 3790 epochs, Loss = 0.1745523877812516\n",
      "After 3800 epochs, Loss = 0.17441891803777954\n",
      "After 3810 epochs, Loss = 0.17428598558175765\n",
      "After 3820 epochs, Loss = 0.17415358280176718\n",
      "After 3830 epochs, Loss = 0.17402170429266253\n",
      "After 3840 epochs, Loss = 0.17389034520094931\n",
      "After 3850 epochs, Loss = 0.17375950224416478\n",
      "After 3860 epochs, Loss = 0.17362917216836196\n",
      "After 3870 epochs, Loss = 0.1734993517477901\n",
      "After 3880 epochs, Loss = 0.17337003778458054\n",
      "After 3890 epochs, Loss = 0.17324122710843518\n",
      "After 3900 epochs, Loss = 0.17311291657632102\n",
      "After 3910 epochs, Loss = 0.1729851030721675\n",
      "After 3920 epochs, Loss = 0.17285778350656858\n",
      "After 3930 epochs, Loss = 0.1727309548164897\n",
      "After 3940 epochs, Loss = 0.17260461396497628\n",
      "After 3950 epochs, Loss = 0.172478757940869\n",
      "After 3960 epochs, Loss = 0.1723533837585211\n",
      "After 3970 epochs, Loss = 0.1722284884575203\n",
      "After 3980 epochs, Loss = 0.172104069102414\n",
      "After 3990 epochs, Loss = 0.1719801277846912\n",
      "After 4000 epochs, Loss = 0.17185666310287037\n",
      "(100,)\n",
      "After 10 epochs, Loss = 0.680182591932619\n",
      "After 20 epochs, Loss = 0.6518196287515539\n",
      "After 30 epochs, Loss = 0.6273704927241284\n",
      "After 40 epochs, Loss = 0.6060520462298619\n",
      "After 50 epochs, Loss = 0.5872623826938489\n",
      "After 60 epochs, Loss = 0.5705390119711659\n",
      "After 70 epochs, Loss = 0.5555243016437804\n",
      "After 80 epochs, Loss = 0.541938969521321\n",
      "After 90 epochs, Loss = 0.5295623777279276\n",
      "After 100 epochs, Loss = 0.5182182626637097\n",
      "After 110 epochs, Loss = 0.507764129655649\n",
      "After 120 epochs, Loss = 0.49808356396380865\n",
      "After 130 epochs, Loss = 0.48908047928854376\n",
      "After 140 epochs, Loss = 0.48067485649211517\n",
      "After 150 epochs, Loss = 0.4727993952767928\n",
      "After 160 epochs, Loss = 0.46539712848006637\n",
      "After 170 epochs, Loss = 0.4584194547076961\n",
      "After 180 epochs, Loss = 0.4518245374309676\n",
      "After 190 epochs, Loss = 0.44557625942946033\n",
      "After 200 epochs, Loss = 0.43964320515692423\n",
      "After 210 epochs, Loss = 0.4339979277619401\n",
      "After 220 epochs, Loss = 0.42861625016169447\n",
      "After 230 epochs, Loss = 0.42347684518747464\n",
      "After 240 epochs, Loss = 0.4185608759838345\n",
      "After 250 epochs, Loss = 0.41385147510196624\n",
      "After 260 epochs, Loss = 0.4093336369804187\n",
      "After 270 epochs, Loss = 0.40499386241123775\n",
      "After 280 epochs, Loss = 0.40081997431500105\n",
      "After 290 epochs, Loss = 0.3968010344017669\n",
      "After 300 epochs, Loss = 0.39292715667168804\n",
      "After 310 epochs, Loss = 0.38918927575795226\n",
      "After 320 epochs, Loss = 0.38557916613395954\n",
      "After 330 epochs, Loss = 0.3820892874470769\n",
      "After 340 epochs, Loss = 0.3787127847898408\n",
      "After 350 epochs, Loss = 0.3754433684029549\n",
      "After 360 epochs, Loss = 0.37227522465560325\n",
      "After 370 epochs, Loss = 0.36920299365029535\n",
      "After 380 epochs, Loss = 0.36622179102068203\n",
      "After 390 epochs, Loss = 0.3633269646549072\n",
      "After 400 epochs, Loss = 0.360514262420347\n",
      "After 410 epochs, Loss = 0.3577797411158695\n",
      "After 420 epochs, Loss = 0.3551196956385052\n",
      "After 430 epochs, Loss = 0.35253069367486994\n",
      "After 440 epochs, Loss = 0.35000953733951035\n",
      "After 450 epochs, Loss = 0.34755323592549586\n",
      "After 460 epochs, Loss = 0.3451590021564165\n",
      "After 470 epochs, Loss = 0.34282425154771606\n",
      "After 480 epochs, Loss = 0.34054650831475686\n",
      "After 490 epochs, Loss = 0.338323455183687\n",
      "After 500 epochs, Loss = 0.33615287553142376\n",
      "After 510 epochs, Loss = 0.334032749116565\n",
      "After 520 epochs, Loss = 0.3319611060270257\n",
      "After 530 epochs, Loss = 0.32993615478409577\n",
      "After 540 epochs, Loss = 0.3279561176580729\n",
      "After 550 epochs, Loss = 0.3260194007033547\n",
      "After 560 epochs, Loss = 0.3241243912218205\n",
      "After 570 epochs, Loss = 0.32226960263498916\n",
      "After 580 epochs, Loss = 0.3204536114151958\n",
      "After 590 epochs, Loss = 0.3186750799546421\n",
      "After 600 epochs, Loss = 0.31693273861017734\n",
      "After 610 epochs, Loss = 0.3152253994961318\n",
      "After 620 epochs, Loss = 0.313551963645704\n",
      "After 630 epochs, Loss = 0.31191129303300896\n",
      "After 640 epochs, Loss = 0.31030232933328555\n",
      "After 650 epochs, Loss = 0.30872406574032857\n",
      "After 660 epochs, Loss = 0.30717559077485024\n",
      "After 670 epochs, Loss = 0.30565604777967254\n",
      "After 680 epochs, Loss = 0.3041645113465122\n",
      "After 690 epochs, Loss = 0.30270011220026194\n",
      "After 700 epochs, Loss = 0.30126203818770014\n",
      "After 710 epochs, Loss = 0.299849511703305\n",
      "After 720 epochs, Loss = 0.29846180248661835\n",
      "After 730 epochs, Loss = 0.29709817922151754\n",
      "After 740 epochs, Loss = 0.2957579620362397\n",
      "After 750 epochs, Loss = 0.2944405126725743\n",
      "After 760 epochs, Loss = 0.2931451968589595\n",
      "After 770 epochs, Loss = 0.29187140714860427\n",
      "After 780 epochs, Loss = 0.2906185849380522\n",
      "After 790 epochs, Loss = 0.2893861496336295\n",
      "After 800 epochs, Loss = 0.2881735731465985\n",
      "After 810 epochs, Loss = 0.28698032497054354\n",
      "After 820 epochs, Loss = 0.2858059066433671\n",
      "After 830 epochs, Loss = 0.28464983330575466\n",
      "After 840 epochs, Loss = 0.28351169219054545\n",
      "After 850 epochs, Loss = 0.2823910016834218\n",
      "After 860 epochs, Loss = 0.2812873542394385\n",
      "After 870 epochs, Loss = 0.28020028676200454\n",
      "After 880 epochs, Loss = 0.2791293980992104\n",
      "After 890 epochs, Loss = 0.27807429224515784\n",
      "After 900 epochs, Loss = 0.2770345915441468\n",
      "After 910 epochs, Loss = 0.2760099409452121\n",
      "After 920 epochs, Loss = 0.2750000023126857\n",
      "After 930 epochs, Loss = 0.2740044083539814\n",
      "After 940 epochs, Loss = 0.27302282808726946\n",
      "After 950 epochs, Loss = 0.2720549465834959\n",
      "After 960 epochs, Loss = 0.27110048592809377\n",
      "After 970 epochs, Loss = 0.27015911026986483\n",
      "After 980 epochs, Loss = 0.2692305633457126\n",
      "After 990 epochs, Loss = 0.26831454678998534\n",
      "After 1000 epochs, Loss = 0.26741077427387544\n",
      "After 1010 epochs, Loss = 0.26651897652352613\n",
      "After 1020 epochs, Loss = 0.26563889942574914\n",
      "After 1030 epochs, Loss = 0.2647702889015616\n",
      "After 1040 epochs, Loss = 0.2639129064086342\n",
      "After 1050 epochs, Loss = 0.2630665149379724\n",
      "After 1060 epochs, Loss = 0.26223088528455923\n",
      "After 1070 epochs, Loss = 0.26140578602734477\n",
      "After 1080 epochs, Loss = 0.26059100091211274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1090 epochs, Loss = 0.2597863197678567\n",
      "After 1100 epochs, Loss = 0.25899154754590614\n",
      "After 1110 epochs, Loss = 0.25820652019270285\n",
      "After 1120 epochs, Loss = 0.257431014398731\n",
      "After 1130 epochs, Loss = 0.2566648359440346\n",
      "After 1140 epochs, Loss = 0.25590779831242183\n",
      "After 1150 epochs, Loss = 0.2551597323050544\n",
      "After 1160 epochs, Loss = 0.2544204622748235\n",
      "After 1170 epochs, Loss = 0.25368981275261326\n",
      "After 1180 epochs, Loss = 0.252967618335195\n",
      "After 1190 epochs, Loss = 0.25225373464729006\n",
      "After 1200 epochs, Loss = 0.25154805526187163\n",
      "After 1210 epochs, Loss = 0.2508503620961792\n",
      "After 1220 epochs, Loss = 0.25016050730560146\n",
      "After 1230 epochs, Loss = 0.24947834679972208\n",
      "After 1240 epochs, Loss = 0.24880374249517723\n",
      "After 1250 epochs, Loss = 0.24813655645402113\n",
      "After 1260 epochs, Loss = 0.2474767189003882\n",
      "After 1270 epochs, Loss = 0.2468240793432228\n",
      "After 1280 epochs, Loss = 0.24617847715772678\n",
      "After 1290 epochs, Loss = 0.24553977739077676\n",
      "After 1300 epochs, Loss = 0.24490785898172582\n",
      "After 1310 epochs, Loss = 0.24428260390332437\n",
      "After 1320 epochs, Loss = 0.24366389695191612\n",
      "After 1330 epochs, Loss = 0.2430516256623532\n",
      "After 1340 epochs, Loss = 0.24244568022601065\n",
      "After 1350 epochs, Loss = 0.24184596064631794\n",
      "After 1360 epochs, Loss = 0.24125236115407364\n",
      "After 1370 epochs, Loss = 0.24066477327317518\n",
      "After 1380 epochs, Loss = 0.2400830995838222\n",
      "After 1390 epochs, Loss = 0.23950724091137643\n",
      "After 1400 epochs, Loss = 0.23893711523437378\n",
      "After 1410 epochs, Loss = 0.2383726271515153\n",
      "After 1420 epochs, Loss = 0.2378136771688975\n",
      "After 1430 epochs, Loss = 0.23726017215786313\n",
      "After 1440 epochs, Loss = 0.2367120249578987\n",
      "After 1450 epochs, Loss = 0.2361691503337364\n",
      "After 1460 epochs, Loss = 0.23563146492178824\n",
      "After 1470 epochs, Loss = 0.23509889152671087\n",
      "After 1480 epochs, Loss = 0.2345713716681381\n",
      "After 1490 epochs, Loss = 0.2340488322924961\n",
      "After 1500 epochs, Loss = 0.2335311664537416\n",
      "After 1510 epochs, Loss = 0.23301829939979096\n",
      "After 1520 epochs, Loss = 0.23251015796422553\n",
      "After 1530 epochs, Loss = 0.23200667073448833\n",
      "After 1540 epochs, Loss = 0.23150777961520386\n",
      "After 1550 epochs, Loss = 0.2310134037553895\n",
      "After 1560 epochs, Loss = 0.23052347774849685\n",
      "After 1570 epochs, Loss = 0.2300379509373657\n",
      "After 1580 epochs, Loss = 0.22955674184377362\n",
      "After 1590 epochs, Loss = 0.22907978728454995\n",
      "After 1600 epochs, Loss = 0.22860702535840813\n",
      "After 1610 epochs, Loss = 0.22813839541311404\n",
      "After 1620 epochs, Loss = 0.22767384329744772\n",
      "After 1630 epochs, Loss = 0.22721332262961583\n",
      "After 1640 epochs, Loss = 0.2267567729184535\n",
      "After 1650 epochs, Loss = 0.22630413405588237\n",
      "After 1660 epochs, Loss = 0.225855355079352\n",
      "After 1670 epochs, Loss = 0.225410384529583\n",
      "After 1680 epochs, Loss = 0.22496915762947753\n",
      "After 1690 epochs, Loss = 0.2245316191770779\n",
      "After 1700 epochs, Loss = 0.22409771812819682\n",
      "After 1710 epochs, Loss = 0.22366740471822008\n",
      "After 1720 epochs, Loss = 0.2232406301297969\n",
      "After 1730 epochs, Loss = 0.22281735649303758\n",
      "After 1740 epochs, Loss = 0.2223975292496948\n",
      "After 1750 epochs, Loss = 0.2219810998312391\n",
      "After 1760 epochs, Loss = 0.22156802298765377\n",
      "After 1770 epochs, Loss = 0.22115825430845118\n",
      "After 1780 epochs, Loss = 0.22075175020293214\n",
      "After 1790 epochs, Loss = 0.22034846788100204\n",
      "After 1800 epochs, Loss = 0.21994836533452572\n",
      "After 1810 epochs, Loss = 0.21955140220031294\n",
      "After 1820 epochs, Loss = 0.219157543643049\n",
      "After 1830 epochs, Loss = 0.21876674654623188\n",
      "After 1840 epochs, Loss = 0.2183789774679938\n",
      "After 1850 epochs, Loss = 0.2179941923572368\n",
      "After 1860 epochs, Loss = 0.21761235678306423\n",
      "After 1870 epochs, Loss = 0.21723342743012639\n",
      "After 1880 epochs, Loss = 0.21685736793231647\n",
      "After 1890 epochs, Loss = 0.21648414813212097\n",
      "After 1900 epochs, Loss = 0.2161137388078167\n",
      "After 1910 epochs, Loss = 0.21574609400181363\n",
      "After 1920 epochs, Loss = 0.21538118721602054\n",
      "After 1930 epochs, Loss = 0.21501898087689125\n",
      "After 1940 epochs, Loss = 0.21465943913322644\n",
      "After 1950 epochs, Loss = 0.21430252978402617\n",
      "After 1960 epochs, Loss = 0.21394822824989468\n",
      "After 1970 epochs, Loss = 0.21359650871652197\n",
      "After 1980 epochs, Loss = 0.2132473499358974\n",
      "After 1990 epochs, Loss = 0.21290070002899186\n",
      "After 2000 epochs, Loss = 0.2125565454875279\n",
      "After 2010 epochs, Loss = 0.21221484315840536\n",
      "After 2020 epochs, Loss = 0.2118755623618718\n",
      "After 2030 epochs, Loss = 0.21153867493830528\n",
      "After 2040 epochs, Loss = 0.2112041546689501\n",
      "After 2050 epochs, Loss = 0.2108719821135681\n",
      "After 2060 epochs, Loss = 0.2105421211734851\n",
      "After 2070 epochs, Loss = 0.21021454546700918\n",
      "After 2080 epochs, Loss = 0.20988922903430446\n",
      "After 2090 epochs, Loss = 0.2095661641181742\n",
      "After 2100 epochs, Loss = 0.20924532486161035\n",
      "After 2110 epochs, Loss = 0.20892667560262865\n",
      "After 2120 epochs, Loss = 0.20861020103765143\n",
      "After 2130 epochs, Loss = 0.20829586180513288\n",
      "After 2140 epochs, Loss = 0.20798363430375089\n",
      "After 2150 epochs, Loss = 0.20767350745456462\n",
      "After 2160 epochs, Loss = 0.20736545744665708\n",
      "After 2170 epochs, Loss = 0.20705945897575642\n",
      "After 2180 epochs, Loss = 0.20675548546661182\n",
      "After 2190 epochs, Loss = 0.2064535213947114\n",
      "After 2200 epochs, Loss = 0.20615353823602744\n",
      "After 2210 epochs, Loss = 0.20585552423870768\n",
      "After 2220 epochs, Loss = 0.205559449145831\n",
      "After 2230 epochs, Loss = 0.20526529216066453\n",
      "After 2240 epochs, Loss = 0.20497303687925406\n",
      "After 2250 epochs, Loss = 0.20468266312372616\n",
      "After 2260 epochs, Loss = 0.20439415554359283\n",
      "After 2270 epochs, Loss = 0.20410748758504682\n",
      "After 2280 epochs, Loss = 0.2038226430365775\n",
      "After 2290 epochs, Loss = 0.20353960075502855\n",
      "After 2300 epochs, Loss = 0.20325834169381052\n",
      "After 2310 epochs, Loss = 0.2029788435757975\n",
      "After 2320 epochs, Loss = 0.2027010884010268\n",
      "After 2330 epochs, Loss = 0.20242505842767328\n",
      "After 2340 epochs, Loss = 0.20215073616728832\n",
      "After 2350 epochs, Loss = 0.20187810438014497\n",
      "After 2360 epochs, Loss = 0.20160715843751803\n",
      "After 2370 epochs, Loss = 0.20133787202354428\n",
      "After 2380 epochs, Loss = 0.2010702258024153\n",
      "After 2390 epochs, Loss = 0.20080420348474648\n",
      "After 2400 epochs, Loss = 0.20053979296827062\n",
      "After 2410 epochs, Loss = 0.20027697848497295\n",
      "After 2420 epochs, Loss = 0.20001574039054945\n",
      "After 2430 epochs, Loss = 0.19975606619701097\n",
      "After 2440 epochs, Loss = 0.19949796275817044\n",
      "After 2450 epochs, Loss = 0.19924139136999033\n",
      "After 2460 epochs, Loss = 0.19898633603320137\n",
      "After 2470 epochs, Loss = 0.19873278216409693\n",
      "After 2480 epochs, Loss = 0.1984807153757109\n",
      "After 2490 epochs, Loss = 0.1982301214743958\n",
      "After 2500 epochs, Loss = 0.19798098645647078\n",
      "After 2510 epochs, Loss = 0.1977332965049436\n",
      "After 2520 epochs, Loss = 0.19748703798629752\n",
      "After 2530 epochs, Loss = 0.19724221606453698\n",
      "After 2540 epochs, Loss = 0.19699880571904382\n",
      "After 2550 epochs, Loss = 0.19675678700783178\n",
      "After 2560 epochs, Loss = 0.19651615135038583\n",
      "After 2570 epochs, Loss = 0.19627689376642873\n",
      "After 2580 epochs, Loss = 0.19603899146712883\n",
      "After 2590 epochs, Loss = 0.19580243875764336\n",
      "After 2600 epochs, Loss = 0.19556721467917174\n",
      "After 2610 epochs, Loss = 0.19533330711455302\n",
      "After 2620 epochs, Loss = 0.19510070410143987\n",
      "After 2630 epochs, Loss = 0.19486939382974308\n",
      "After 2640 epochs, Loss = 0.19463938261705183\n",
      "After 2650 epochs, Loss = 0.19441065096996232\n",
      "After 2660 epochs, Loss = 0.19418318120642686\n",
      "After 2670 epochs, Loss = 0.19395695836401614\n",
      "After 2680 epochs, Loss = 0.1937319713571492\n",
      "After 2690 epochs, Loss = 0.19350821480766398\n",
      "After 2700 epochs, Loss = 0.19328569774615684\n",
      "After 2710 epochs, Loss = 0.19306439367827288\n",
      "After 2720 epochs, Loss = 0.19284428252989178\n",
      "After 2730 epochs, Loss = 0.19262535388290492\n",
      "After 2740 epochs, Loss = 0.19240759744652122\n",
      "After 2750 epochs, Loss = 0.19219100305523604\n",
      "After 2760 epochs, Loss = 0.19197556066685698\n",
      "After 2770 epochs, Loss = 0.19176126036056798\n",
      "After 2780 epochs, Loss = 0.19154809233503198\n",
      "After 2790 epochs, Loss = 0.19133604690652833\n",
      "After 2800 epochs, Loss = 0.19112511450712583\n",
      "After 2810 epochs, Loss = 0.19091529679865613\n",
      "After 2820 epochs, Loss = 0.19070657856117793\n",
      "After 2830 epochs, Loss = 0.19049895009976242\n",
      "After 2840 epochs, Loss = 0.1902923974619103\n",
      "After 2850 epochs, Loss = 0.19008691163237137\n",
      "After 2860 epochs, Loss = 0.18988248370143257\n",
      "After 2870 epochs, Loss = 0.1896791048633184\n",
      "After 2880 epochs, Loss = 0.1894767664146139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2890 epochs, Loss = 0.18927545978091548\n",
      "After 2900 epochs, Loss = 0.18907518404071394\n",
      "After 2910 epochs, Loss = 0.188875923174233\n",
      "After 2920 epochs, Loss = 0.18867766887136758\n",
      "After 2930 epochs, Loss = 0.18848046148185743\n",
      "After 2940 epochs, Loss = 0.1882842510017224\n",
      "After 2950 epochs, Loss = 0.18808902274303524\n",
      "After 2960 epochs, Loss = 0.1878947687669956\n",
      "After 2970 epochs, Loss = 0.18770148271649173\n",
      "After 2980 epochs, Loss = 0.18750918616915332\n",
      "After 2990 epochs, Loss = 0.18731784543864705\n",
      "After 3000 epochs, Loss = 0.18712744817221272\n",
      "After 3010 epochs, Loss = 0.1869379868650374\n",
      "After 3020 epochs, Loss = 0.18674946571692588\n",
      "After 3030 epochs, Loss = 0.18656186624183188\n",
      "After 3040 epochs, Loss = 0.18637518610424955\n",
      "After 3050 epochs, Loss = 0.18618942649313458\n",
      "After 3060 epochs, Loss = 0.18600457878219417\n",
      "After 3070 epochs, Loss = 0.1858206237186083\n",
      "After 3080 epochs, Loss = 0.18563755435467932\n",
      "After 3090 epochs, Loss = 0.18545536381798108\n",
      "After 3100 epochs, Loss = 0.18527404531030228\n",
      "After 3110 epochs, Loss = 0.18509359210660903\n",
      "After 3120 epochs, Loss = 0.18491399755402368\n",
      "After 3130 epochs, Loss = 0.18473525507082408\n",
      "After 3140 epochs, Loss = 0.18455735814545704\n",
      "After 3150 epochs, Loss = 0.18438030033557035\n",
      "After 3160 epochs, Loss = 0.1842040752670606\n",
      "After 3170 epochs, Loss = 0.1840286766331367\n",
      "After 3180 epochs, Loss = 0.18385409819339962\n",
      "After 3190 epochs, Loss = 0.18368033377293774\n",
      "After 3200 epochs, Loss = 0.18350737726143682\n",
      "After 3210 epochs, Loss = 0.1833352354009608\n",
      "After 3220 epochs, Loss = 0.18316390025571427\n",
      "After 3230 epochs, Loss = 0.18299335504855907\n",
      "After 3240 epochs, Loss = 0.1828236021601737\n",
      "After 3250 epochs, Loss = 0.18265464221253117\n",
      "After 3260 epochs, Loss = 0.18248646769180688\n",
      "After 3270 epochs, Loss = 0.18231906320857016\n",
      "After 3280 epochs, Loss = 0.18215242332817533\n",
      "After 3290 epochs, Loss = 0.1819865626602788\n",
      "After 3300 epochs, Loss = 0.18182147539769766\n",
      "After 3310 epochs, Loss = 0.18165713688631704\n",
      "After 3320 epochs, Loss = 0.1814935375530389\n",
      "After 3330 epochs, Loss = 0.1813306720522628\n",
      "After 3340 epochs, Loss = 0.18116854432186374\n",
      "After 3350 epochs, Loss = 0.18100714379503507\n",
      "After 3360 epochs, Loss = 0.18084646142737842\n",
      "After 3370 epochs, Loss = 0.18068649208317114\n",
      "After 3380 epochs, Loss = 0.18052723226710626\n",
      "After 3390 epochs, Loss = 0.1803686861890996\n",
      "After 3400 epochs, Loss = 0.1802108380589592\n",
      "After 3410 epochs, Loss = 0.18005369039352173\n",
      "After 3420 epochs, Loss = 0.17989725072709037\n",
      "After 3430 epochs, Loss = 0.17974149424721744\n",
      "After 3440 epochs, Loss = 0.179586416161586\n",
      "After 3450 epochs, Loss = 0.17943201172441905\n",
      "After 3460 epochs, Loss = 0.17927827623587597\n",
      "After 3470 epochs, Loss = 0.17912520504147722\n",
      "After 3480 epochs, Loss = 0.17897279353153733\n",
      "After 3490 epochs, Loss = 0.17882103714060724\n",
      "After 3500 epochs, Loss = 0.1786699313469247\n",
      "After 3510 epochs, Loss = 0.17851947167187412\n",
      "After 3520 epochs, Loss = 0.17836965367945218\n",
      "After 3530 epochs, Loss = 0.17822047297574578\n",
      "After 3540 epochs, Loss = 0.17807192520841353\n",
      "After 3550 epochs, Loss = 0.17792400798565494\n",
      "After 3560 epochs, Loss = 0.17777672023171143\n",
      "After 3570 epochs, Loss = 0.17763005261436443\n",
      "After 3580 epochs, Loss = 0.17748400094246522\n",
      "After 3590 epochs, Loss = 0.17733856106397045\n",
      "After 3600 epochs, Loss = 0.17719372886547052\n",
      "After 3610 epochs, Loss = 0.1770495052754653\n",
      "After 3620 epochs, Loss = 0.1769058869080098\n",
      "After 3630 epochs, Loss = 0.17676286489358506\n",
      "After 3640 epochs, Loss = 0.1766204345893188\n",
      "After 3650 epochs, Loss = 0.17647859195339707\n",
      "After 3660 epochs, Loss = 0.17633733309468877\n",
      "After 3670 epochs, Loss = 0.17619665415758498\n",
      "After 3680 epochs, Loss = 0.17605655132157919\n",
      "After 3690 epochs, Loss = 0.17591702080085586\n",
      "After 3700 epochs, Loss = 0.17577805884388303\n",
      "After 3710 epochs, Loss = 0.1756396617330114\n",
      "After 3720 epochs, Loss = 0.17550182578407988\n",
      "After 3730 epochs, Loss = 0.17536454734602586\n",
      "After 3740 epochs, Loss = 0.17522782280050123\n",
      "After 3750 epochs, Loss = 0.17509164856149467\n",
      "After 3760 epochs, Loss = 0.17495602107495867\n",
      "After 3770 epochs, Loss = 0.17482093681844138\n",
      "After 3780 epochs, Loss = 0.1746863923007246\n",
      "After 3790 epochs, Loss = 0.1745523877812516\n",
      "After 3800 epochs, Loss = 0.17441891803777954\n",
      "After 3810 epochs, Loss = 0.17428598558175765\n",
      "After 3820 epochs, Loss = 0.17415358280176718\n",
      "After 3830 epochs, Loss = 0.17402170429266253\n",
      "After 3840 epochs, Loss = 0.17389034520094931\n",
      "After 3850 epochs, Loss = 0.17375950224416478\n",
      "After 3860 epochs, Loss = 0.17362917216836196\n",
      "After 3870 epochs, Loss = 0.1734993517477901\n",
      "After 3880 epochs, Loss = 0.17337003778458054\n",
      "After 3890 epochs, Loss = 0.17324122710843518\n",
      "After 3900 epochs, Loss = 0.17311291657632102\n",
      "After 3910 epochs, Loss = 0.1729851030721675\n",
      "After 3920 epochs, Loss = 0.17285778350656858\n",
      "After 3930 epochs, Loss = 0.1727309548164897\n",
      "After 3940 epochs, Loss = 0.17260461396497628\n",
      "After 3950 epochs, Loss = 0.172478757940869\n",
      "After 3960 epochs, Loss = 0.1723533837585211\n",
      "After 3970 epochs, Loss = 0.1722284884575203\n",
      "After 3980 epochs, Loss = 0.172104069102414\n",
      "After 3990 epochs, Loss = 0.1719801277846912\n",
      "After 4000 epochs, Loss = 0.17185666310287037\n",
      "After 4010 epochs, Loss = 0.17173366571611967\n",
      "After 4020 epochs, Loss = 0.17161113278599785\n",
      "After 4030 epochs, Loss = 0.1714890614977079\n",
      "After 4040 epochs, Loss = 0.17136744905984225\n",
      "After 4050 epochs, Loss = 0.17124629270413336\n",
      "After 4060 epochs, Loss = 0.17112558968520666\n",
      "After 4070 epochs, Loss = 0.17100533728033673\n",
      "After 4080 epochs, Loss = 0.17088553452602778\n",
      "After 4090 epochs, Loss = 0.17076617957276122\n",
      "After 4100 epochs, Loss = 0.17064726719059548\n",
      "After 4110 epochs, Loss = 0.17052879474504246\n",
      "After 4120 epochs, Loss = 0.1704107596230946\n",
      "After 4130 epochs, Loss = 0.17029315923299793\n",
      "After 4140 epochs, Loss = 0.1701759910040298\n",
      "After 4150 epochs, Loss = 0.17005925238627898\n",
      "After 4160 epochs, Loss = 0.16994294085043035\n",
      "After 4170 epochs, Loss = 0.1698270623324366\n",
      "After 4180 epochs, Loss = 0.16971160912364677\n",
      "After 4190 epochs, Loss = 0.16959657554083596\n",
      "After 4200 epochs, Loss = 0.16948195913435823\n",
      "After 4210 epochs, Loss = 0.1693677574741185\n",
      "After 4220 epochs, Loss = 0.16925396814939508\n",
      "After 4230 epochs, Loss = 0.16914058876864094\n",
      "After 4240 epochs, Loss = 0.16902761695928684\n",
      "After 4250 epochs, Loss = 0.16891505036754834\n",
      "After 4260 epochs, Loss = 0.1688028866582344\n",
      "After 4270 epochs, Loss = 0.16869112783489526\n",
      "After 4280 epochs, Loss = 0.16857976829823001\n",
      "After 4290 epochs, Loss = 0.16846880475066892\n",
      "After 4300 epochs, Loss = 0.1683582471359649\n",
      "After 4310 epochs, Loss = 0.16824808588865148\n",
      "After 4320 epochs, Loss = 0.16813832594921346\n",
      "After 4330 epochs, Loss = 0.16802895306272336\n",
      "After 4340 epochs, Loss = 0.1679199650359079\n",
      "After 4350 epochs, Loss = 0.16781135969247576\n",
      "After 4360 epochs, Loss = 0.1677031348729433\n",
      "After 4370 epochs, Loss = 0.1675952884344686\n",
      "After 4380 epochs, Loss = 0.1674878182506865\n",
      "After 4390 epochs, Loss = 0.16738072413002048\n",
      "After 4400 epochs, Loss = 0.16727400465590023\n",
      "After 4410 epochs, Loss = 0.1671676566428173\n",
      "After 4420 epochs, Loss = 0.16706168276407463\n",
      "After 4430 epochs, Loss = 0.1669560747340759\n",
      "After 4440 epochs, Loss = 0.166850830521988\n",
      "After 4450 epochs, Loss = 0.16674594811245175\n",
      "After 4460 epochs, Loss = 0.166641425505332\n",
      "After 4470 epochs, Loss = 0.16653726071557096\n",
      "After 4480 epochs, Loss = 0.16643345311952662\n",
      "After 4490 epochs, Loss = 0.16633001404951991\n",
      "After 4500 epochs, Loss = 0.1662269305762504\n",
      "After 4510 epochs, Loss = 0.16612419740262796\n",
      "After 4520 epochs, Loss = 0.16602181335467475\n",
      "After 4530 epochs, Loss = 0.16591977550871928\n",
      "After 4540 epochs, Loss = 0.16581808368268397\n",
      "After 4550 epochs, Loss = 0.1657167376099526\n",
      "After 4560 epochs, Loss = 0.1656157321385821\n",
      "After 4570 epochs, Loss = 0.16551506542655162\n",
      "After 4580 epochs, Loss = 0.1654147356454151\n",
      "After 4590 epochs, Loss = 0.1653147409801681\n",
      "After 4600 epochs, Loss = 0.16521507962912216\n",
      "After 4610 epochs, Loss = 0.16511574980377913\n",
      "After 4620 epochs, Loss = 0.16501674972870856\n",
      "After 4630 epochs, Loss = 0.1649180776414252\n",
      "After 4640 epochs, Loss = 0.16481973947837986\n",
      "After 4650 epochs, Loss = 0.16472172697551146\n",
      "After 4660 epochs, Loss = 0.16462404150679866\n",
      "After 4670 epochs, Loss = 0.16452668163898898\n",
      "After 4680 epochs, Loss = 0.1644296513440896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 4690 epochs, Loss = 0.16433293872217442\n",
      "After 4700 epochs, Loss = 0.16423654209808541\n",
      "After 4710 epochs, Loss = 0.1641404598086837\n",
      "After 4720 epochs, Loss = 0.16404469020273274\n",
      "After 4730 epochs, Loss = 0.1639492316407887\n",
      "After 4740 epochs, Loss = 0.16385408249509373\n",
      "After 4750 epochs, Loss = 0.1637592411494695\n",
      "After 4760 epochs, Loss = 0.16366471098985905\n",
      "After 4770 epochs, Loss = 0.16357049432343077\n",
      "After 4780 epochs, Loss = 0.163476586501227\n",
      "After 4790 epochs, Loss = 0.1633829822659185\n",
      "After 4800 epochs, Loss = 0.1632896780823869\n",
      "After 4810 epochs, Loss = 0.16319667440487723\n",
      "After 4820 epochs, Loss = 0.1631039706119767\n",
      "After 4830 epochs, Loss = 0.16301156205892403\n",
      "After 4840 epochs, Loss = 0.16291944722917673\n",
      "After 4850 epochs, Loss = 0.16282762461677014\n",
      "After 4860 epochs, Loss = 0.16273609272622866\n",
      "After 4870 epochs, Loss = 0.16264485007247292\n",
      "After 4880 epochs, Loss = 0.16255389518072624\n",
      "After 4890 epochs, Loss = 0.16246322658642287\n",
      "After 4900 epochs, Loss = 0.1623728428351191\n",
      "After 4910 epochs, Loss = 0.16228274248240332\n",
      "After 4920 epochs, Loss = 0.16219292409380917\n",
      "After 4930 epochs, Loss = 0.16210338624472728\n",
      "After 4940 epochs, Loss = 0.16201412752031993\n",
      "After 4950 epochs, Loss = 0.16192514651543566\n",
      "After 4960 epochs, Loss = 0.16183644183452528\n",
      "After 4970 epochs, Loss = 0.161748012091558\n",
      "After 4980 epochs, Loss = 0.16165985590993898\n",
      "After 4990 epochs, Loss = 0.1615719719224283\n",
      "After 5000 epochs, Loss = 0.16148435877105974\n",
      "After 5010 epochs, Loss = 0.16139701510706034\n",
      "After 5020 epochs, Loss = 0.16130993959077283\n",
      "After 5030 epochs, Loss = 0.16122313160229007\n",
      "After 5040 epochs, Loss = 0.1611365956475438\n",
      "After 5050 epochs, Loss = 0.1610503238729818\n",
      "After 5060 epochs, Loss = 0.16096431497462424\n",
      "After 5070 epochs, Loss = 0.16087856765722502\n",
      "After 5080 epochs, Loss = 0.16079308063419553\n",
      "After 5090 epochs, Loss = 0.16070785262753154\n",
      "After 5100 epochs, Loss = 0.16062288236773975\n",
      "After 5110 epochs, Loss = 0.16053816859376674\n",
      "After 5120 epochs, Loss = 0.1604537151716736\n",
      "After 5130 epochs, Loss = 0.16036951920138548\n",
      "After 5140 epochs, Loss = 0.16028557597334123\n",
      "After 5150 epochs, Loss = 0.16020188425954343\n",
      "After 5160 epochs, Loss = 0.16011844284008503\n",
      "After 5170 epochs, Loss = 0.16003525050308195\n",
      "After 5180 epochs, Loss = 0.1599523060446062\n",
      "After 5190 epochs, Loss = 0.1598696082686196\n",
      "After 5200 epochs, Loss = 0.15978715598690782\n",
      "After 5210 epochs, Loss = 0.15970494801901639\n",
      "After 5220 epochs, Loss = 0.15962298319218587\n",
      "After 5230 epochs, Loss = 0.15954126034128835\n",
      "After 5240 epochs, Loss = 0.15945977830876457\n",
      "After 5250 epochs, Loss = 0.15937853594456186\n",
      "After 5260 epochs, Loss = 0.1592975321060719\n",
      "After 5270 epochs, Loss = 0.15921676565807055\n",
      "After 5280 epochs, Loss = 0.15913623547265654\n",
      "After 5290 epochs, Loss = 0.15905594042919205\n",
      "After 5300 epochs, Loss = 0.15897587941424354\n",
      "After 5310 epochs, Loss = 0.1588960513215234\n",
      "After 5320 epochs, Loss = 0.1588164550518305\n",
      "After 5330 epochs, Loss = 0.1587370895129947\n",
      "After 5340 epochs, Loss = 0.15865795361981838\n",
      "After 5350 epochs, Loss = 0.1585790462940209\n",
      "After 5360 epochs, Loss = 0.15850036646418228\n",
      "After 5370 epochs, Loss = 0.15842191306568898\n",
      "After 5380 epochs, Loss = 0.15834368504067753\n",
      "After 5390 epochs, Loss = 0.15826568133798177\n",
      "After 5400 epochs, Loss = 0.15818790169135274\n",
      "After 5410 epochs, Loss = 0.15811034587566275\n",
      "After 5420 epochs, Loss = 0.1580330112665228\n",
      "After 5430 epochs, Loss = 0.15795589683901004\n",
      "After 5440 epochs, Loss = 0.1578790015746355\n",
      "After 5450 epochs, Loss = 0.15780232446129375\n",
      "After 5460 epochs, Loss = 0.15772586449321185\n",
      "After 5470 epochs, Loss = 0.1576496206708986\n",
      "After 5480 epochs, Loss = 0.15757359200109583\n",
      "After 5490 epochs, Loss = 0.15749777749672975\n",
      "After 5500 epochs, Loss = 0.1574221761768602\n",
      "After 5510 epochs, Loss = 0.15734678706663519\n",
      "After 5520 epochs, Loss = 0.157271609197241\n",
      "After 5530 epochs, Loss = 0.15719664160585625\n",
      "After 5540 epochs, Loss = 0.157121883335604\n",
      "After 5550 epochs, Loss = 0.15704733343550714\n",
      "After 5560 epochs, Loss = 0.15697299096044032\n",
      "After 5570 epochs, Loss = 0.15689885497108624\n",
      "After 5580 epochs, Loss = 0.1568249245338896\n",
      "After 5590 epochs, Loss = 0.15675119872101292\n",
      "After 5600 epochs, Loss = 0.15667767661029194\n",
      "After 5610 epochs, Loss = 0.15660436679063935\n",
      "After 5620 epochs, Loss = 0.1565312613301196\n",
      "After 5630 epochs, Loss = 0.15645835683187082\n",
      "After 5640 epochs, Loss = 0.15638565239598184\n",
      "After 5650 epochs, Loss = 0.15631314712800062\n",
      "After 5660 epochs, Loss = 0.15624084013889183\n",
      "After 5670 epochs, Loss = 0.15616873054499641\n",
      "After 5680 epochs, Loss = 0.15609681746798992\n",
      "After 5690 epochs, Loss = 0.15602510003484216\n",
      "After 5700 epochs, Loss = 0.15595358624377645\n",
      "After 5710 epochs, Loss = 0.1558822675085904\n",
      "After 5720 epochs, Loss = 0.15581114182855016\n",
      "After 5730 epochs, Loss = 0.15574020835138289\n",
      "After 5740 epochs, Loss = 0.15566946811512555\n",
      "After 5750 epochs, Loss = 0.1555989211523977\n",
      "After 5760 epochs, Loss = 0.15552856386486966\n",
      "After 5770 epochs, Loss = 0.15545839542041698\n",
      "After 5780 epochs, Loss = 0.15538841499189754\n",
      "After 5790 epochs, Loss = 0.15531862175707642\n",
      "After 5800 epochs, Loss = 0.15524901489858975\n",
      "After 5810 epochs, Loss = 0.15517959514951526\n",
      "After 5820 epochs, Loss = 0.15511036123714197\n",
      "After 5830 epochs, Loss = 0.15504131127841697\n",
      "After 5840 epochs, Loss = 0.1549724568514229\n",
      "After 5850 epochs, Loss = 0.15490378585576053\n",
      "After 5860 epochs, Loss = 0.15483529643640376\n",
      "After 5870 epochs, Loss = 0.154766987808998\n",
      "After 5880 epochs, Loss = 0.1546988591937794\n",
      "After 5890 epochs, Loss = 0.15463090981554029\n",
      "After 5900 epochs, Loss = 0.15456313890359635\n",
      "After 5910 epochs, Loss = 0.15449554569175306\n",
      "After 5920 epochs, Loss = 0.15442812941827314\n",
      "After 5930 epochs, Loss = 0.15436088932584383\n",
      "After 5940 epochs, Loss = 0.15429382466154515\n",
      "After 5950 epochs, Loss = 0.1542269346768173\n",
      "After 5960 epochs, Loss = 0.1541602186274291\n",
      "After 5970 epochs, Loss = 0.154093675773447\n",
      "After 5980 epochs, Loss = 0.15402730537920345\n",
      "After 5990 epochs, Loss = 0.15396110671326654\n",
      "After 6000 epochs, Loss = 0.15389507904840877\n",
      "After 6010 epochs, Loss = 0.15382922166157756\n",
      "After 6020 epochs, Loss = 0.15376353383386435\n",
      "After 6030 epochs, Loss = 0.15369801485047532\n",
      "After 6040 epochs, Loss = 0.15363266400070164\n",
      "After 6050 epochs, Loss = 0.1535674805778906\n",
      "After 6060 epochs, Loss = 0.1535024638794158\n",
      "After 6070 epochs, Loss = 0.1534376132066491\n",
      "After 6080 epochs, Loss = 0.15337292786493173\n",
      "After 6090 epochs, Loss = 0.15330840716354635\n",
      "After 6100 epochs, Loss = 0.1532440504156883\n",
      "After 6110 epochs, Loss = 0.15317985876262338\n",
      "After 6120 epochs, Loss = 0.15311583650003036\n",
      "After 6130 epochs, Loss = 0.1530519761319924\n",
      "After 6140 epochs, Loss = 0.15298827698725642\n",
      "After 6150 epochs, Loss = 0.15292473839835977\n",
      "After 6160 epochs, Loss = 0.15286135970158501\n",
      "After 6170 epochs, Loss = 0.1527981402369342\n",
      "After 6180 epochs, Loss = 0.15273507934810288\n",
      "After 6190 epochs, Loss = 0.15267217781997372\n",
      "After 6200 epochs, Loss = 0.1526094359900901\n",
      "After 6210 epochs, Loss = 0.15254685079013275\n",
      "After 6220 epochs, Loss = 0.15248442157830322\n",
      "After 6230 epochs, Loss = 0.15242214771636917\n",
      "After 6240 epochs, Loss = 0.1523600285696404\n",
      "After 6250 epochs, Loss = 0.1522980635069442\n",
      "After 6260 epochs, Loss = 0.15223625190060075\n",
      "After 6270 epochs, Loss = 0.15217459312639922\n",
      "After 6280 epochs, Loss = 0.15211309378577303\n",
      "After 6290 epochs, Loss = 0.15205174838134208\n",
      "After 6300 epochs, Loss = 0.15199055396937483\n",
      "After 6310 epochs, Loss = 0.15192950993918847\n",
      "After 6320 epochs, Loss = 0.15186861568345406\n",
      "After 6330 epochs, Loss = 0.15180789167996211\n",
      "After 6340 epochs, Loss = 0.15174731804645003\n",
      "After 6350 epochs, Loss = 0.1516868923941445\n",
      "After 6360 epochs, Loss = 0.15162661412883516\n",
      "After 6370 epochs, Loss = 0.15156649420914048\n",
      "After 6380 epochs, Loss = 0.15150652124099095\n",
      "After 6390 epochs, Loss = 0.15144669389098303\n",
      "After 6400 epochs, Loss = 0.15138701157773535\n",
      "After 6410 epochs, Loss = 0.15132747464961288\n",
      "After 6420 epochs, Loss = 0.15126808878932543\n",
      "After 6430 epochs, Loss = 0.15120884999388834\n",
      "After 6440 epochs, Loss = 0.15114975588934265\n",
      "After 6450 epochs, Loss = 0.15109080395711083\n",
      "After 6460 epochs, Loss = 0.15103199363441017\n",
      "After 6470 epochs, Loss = 0.15097332436148617\n",
      "After 6480 epochs, Loss = 0.15091479558159113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 6490 epochs, Loss = 0.15085640674096476\n",
      "After 6500 epochs, Loss = 0.15079815728881438\n",
      "After 6510 epochs, Loss = 0.1507400466772938\n",
      "After 6520 epochs, Loss = 0.15068207436148556\n",
      "After 6530 epochs, Loss = 0.15062423979937925\n",
      "After 6540 epochs, Loss = 0.15056654245185394\n",
      "After 6550 epochs, Loss = 0.15050898178265784\n",
      "After 6560 epochs, Loss = 0.1504515572583901\n",
      "After 6570 epochs, Loss = 0.15039426834848058\n",
      "After 6580 epochs, Loss = 0.1503371145251725\n",
      "After 6590 epochs, Loss = 0.15028009526350294\n",
      "After 6600 epochs, Loss = 0.1502232100412851\n",
      "After 6610 epochs, Loss = 0.15016645833908895\n",
      "After 6620 epochs, Loss = 0.15010983964022417\n",
      "After 6630 epochs, Loss = 0.15005335343072151\n",
      "After 6640 epochs, Loss = 0.1499969991993149\n",
      "After 6650 epochs, Loss = 0.14994077643742404\n",
      "After 6660 epochs, Loss = 0.1498846846391363\n",
      "After 6670 epochs, Loss = 0.14982872330118985\n",
      "After 6680 epochs, Loss = 0.14977289192295568\n",
      "After 6690 epochs, Loss = 0.14971719000642142\n",
      "After 6700 epochs, Loss = 0.14966161705617312\n",
      "After 6710 epochs, Loss = 0.1496061725793791\n",
      "After 6720 epochs, Loss = 0.14955085608577307\n",
      "After 6730 epochs, Loss = 0.14949567087067694\n",
      "After 6740 epochs, Loss = 0.149440619318268\n",
      "After 6750 epochs, Loss = 0.14938569429576376\n",
      "After 6760 epochs, Loss = 0.14933089532297028\n",
      "After 6770 epochs, Loss = 0.14927622192217785\n",
      "After 6780 epochs, Loss = 0.14922167361814512\n",
      "After 6790 epochs, Loss = 0.1491672499380815\n",
      "After 6800 epochs, Loss = 0.1491129518707453\n",
      "After 6810 epochs, Loss = 0.14905878411206683\n",
      "After 6820 epochs, Loss = 0.14900473955740923\n",
      "After 6830 epochs, Loss = 0.14895081774300153\n",
      "After 6840 epochs, Loss = 0.14889701820818402\n",
      "After 6850 epochs, Loss = 0.14884334049465936\n",
      "After 6860 epochs, Loss = 0.14878978414647429\n",
      "After 6870 epochs, Loss = 0.14873635064454313\n",
      "After 6880 epochs, Loss = 0.14868303886163703\n",
      "After 6890 epochs, Loss = 0.1486298470873916\n",
      "After 6900 epochs, Loss = 0.1485767748750759\n",
      "After 6910 epochs, Loss = 0.14852382178022838\n",
      "After 6920 epochs, Loss = 0.1484709873606439\n",
      "After 6930 epochs, Loss = 0.14841827117635809\n",
      "After 6940 epochs, Loss = 0.14836567278963383\n",
      "After 6950 epochs, Loss = 0.14831319176494717\n",
      "After 6960 epochs, Loss = 0.14826082766897297\n",
      "After 6970 epochs, Loss = 0.14820858007057142\n",
      "After 6980 epochs, Loss = 0.148156448540774\n",
      "After 6990 epochs, Loss = 0.14810443265277\n",
      "After 7000 epochs, Loss = 0.14805253198189305\n",
      "After 7010 epochs, Loss = 0.14800074610560707\n",
      "After 7020 epochs, Loss = 0.1479490746034941\n",
      "After 7030 epochs, Loss = 0.14789751705723983\n",
      "After 7040 epochs, Loss = 0.14784607305062136\n",
      "After 7050 epochs, Loss = 0.14779474216949334\n",
      "After 7060 epochs, Loss = 0.14774352400177607\n",
      "After 7070 epochs, Loss = 0.14769241813744174\n",
      "After 7080 epochs, Loss = 0.14764142416850237\n",
      "After 7090 epochs, Loss = 0.14759054168899663\n",
      "After 7100 epochs, Loss = 0.1475397702949779\n",
      "After 7110 epochs, Loss = 0.14748911047266308\n",
      "After 7120 epochs, Loss = 0.14743856888129792\n",
      "After 7130 epochs, Loss = 0.14738813716881655\n",
      "After 7140 epochs, Loss = 0.1473378149392384\n",
      "After 7150 epochs, Loss = 0.14728760179855643\n",
      "After 7160 epochs, Loss = 0.14723749735470526\n",
      "After 7170 epochs, Loss = 0.14718750121754867\n",
      "After 7180 epochs, Loss = 0.14713761417155813\n",
      "After 7190 epochs, Loss = 0.14708783531485134\n",
      "After 7200 epochs, Loss = 0.14703816360699118\n",
      "After 7210 epochs, Loss = 0.14698859866543776\n",
      "After 7220 epochs, Loss = 0.1469391401095223\n",
      "After 7230 epochs, Loss = 0.14688978756043594\n",
      "After 7240 epochs, Loss = 0.146840540641218\n",
      "After 7250 epochs, Loss = 0.14679139897674454\n",
      "After 7260 epochs, Loss = 0.1467423636237492\n",
      "After 7270 epochs, Loss = 0.14669344753451202\n",
      "After 7280 epochs, Loss = 0.14664463558864538\n",
      "After 7290 epochs, Loss = 0.14659592741816876\n",
      "After 7300 epochs, Loss = 0.14654732265692913\n",
      "After 7310 epochs, Loss = 0.14649882094054711\n",
      "After 7320 epochs, Loss = 0.1464504219064046\n",
      "After 7330 epochs, Loss = 0.1464021251936356\n",
      "After 7340 epochs, Loss = 0.1463539304431146\n",
      "After 7350 epochs, Loss = 0.14630583729744623\n",
      "After 7360 epochs, Loss = 0.14625784540095513\n",
      "After 7370 epochs, Loss = 0.1462099543996758\n",
      "After 7380 epochs, Loss = 0.14616216394134152\n",
      "After 7390 epochs, Loss = 0.14611447367537483\n",
      "After 7400 epochs, Loss = 0.14606688325287764\n",
      "After 7410 epochs, Loss = 0.14601939232662006\n",
      "After 7420 epochs, Loss = 0.14597200055103177\n",
      "After 7430 epochs, Loss = 0.14592470758219137\n",
      "After 7440 epochs, Loss = 0.14587751307781677\n",
      "After 7450 epochs, Loss = 0.1458304166972553\n",
      "After 7460 epochs, Loss = 0.1457834181014742\n",
      "After 7470 epochs, Loss = 0.14573651695305123\n",
      "After 7480 epochs, Loss = 0.14568971291616464\n",
      "After 7490 epochs, Loss = 0.14564300565658417\n",
      "After 7500 epochs, Loss = 0.14559639484166112\n",
      "After 7510 epochs, Loss = 0.14554988014031983\n",
      "After 7520 epochs, Loss = 0.14550346122304772\n",
      "After 7530 epochs, Loss = 0.1454571377618863\n",
      "After 7540 epochs, Loss = 0.14541090943042242\n",
      "After 7550 epochs, Loss = 0.14536477836596595\n",
      "After 7560 epochs, Loss = 0.14531874694941052\n",
      "After 7570 epochs, Loss = 0.1452728097052754\n",
      "After 7580 epochs, Loss = 0.14522696631317458\n",
      "After 7590 epochs, Loss = 0.14518121645424203\n",
      "After 7600 epochs, Loss = 0.14513555981110435\n",
      "After 7610 epochs, Loss = 0.14508999606787154\n",
      "After 7620 epochs, Loss = 0.1450445249101286\n",
      "After 7630 epochs, Loss = 0.14499914602492694\n",
      "After 7640 epochs, Loss = 0.14495385910077563\n",
      "After 7650 epochs, Loss = 0.14490866382763334\n",
      "After 7660 epochs, Loss = 0.14486355989689947\n",
      "After 7670 epochs, Loss = 0.1448185470014064\n",
      "After 7680 epochs, Loss = 0.14477362483541034\n",
      "After 7690 epochs, Loss = 0.14472879309458403\n",
      "After 7700 epochs, Loss = 0.14468405147595823\n",
      "After 7710 epochs, Loss = 0.14463940145583165\n",
      "After 7720 epochs, Loss = 0.14459484252404925\n",
      "After 7730 epochs, Loss = 0.14455037281277971\n",
      "After 7740 epochs, Loss = 0.14450599202467315\n",
      "After 7750 epochs, Loss = 0.14446169986375113\n",
      "After 7760 epochs, Loss = 0.14441749603539517\n",
      "After 7770 epochs, Loss = 0.14437338024633956\n",
      "After 7780 epochs, Loss = 0.14432935220466264\n",
      "After 7790 epochs, Loss = 0.14428541161977979\n",
      "After 7800 epochs, Loss = 0.14424155820243584\n",
      "After 7810 epochs, Loss = 0.14419779166469726\n",
      "After 7820 epochs, Loss = 0.14415411171994444\n",
      "After 7830 epochs, Loss = 0.14411052356972864\n",
      "After 7840 epochs, Loss = 0.14406702273991182\n",
      "After 7850 epochs, Loss = 0.14402361089000076\n",
      "After 7860 epochs, Loss = 0.14398028457852885\n",
      "After 7870 epochs, Loss = 0.14393704345105995\n",
      "After 7880 epochs, Loss = 0.14389388722868304\n",
      "After 7890 epochs, Loss = 0.1438508156337501\n",
      "After 7900 epochs, Loss = 0.1438078283898691\n",
      "After 7910 epochs, Loss = 0.1437649252218969\n",
      "After 7920 epochs, Loss = 0.14372210585593198\n",
      "After 7930 epochs, Loss = 0.1436793700193086\n",
      "After 7940 epochs, Loss = 0.1436367174405885\n",
      "After 7950 epochs, Loss = 0.1435941478495546\n",
      "After 7960 epochs, Loss = 0.14355166097720434\n",
      "After 7970 epochs, Loss = 0.14350925655574245\n",
      "After 7980 epochs, Loss = 0.1434669343185748\n",
      "After 7990 epochs, Loss = 0.14342469400030103\n",
      "After 8000 epochs, Loss = 0.1433825353367086\n",
      "After 8010 epochs, Loss = 0.14334045806476553\n",
      "After 8020 epochs, Loss = 0.14329846192261447\n",
      "After 8030 epochs, Loss = 0.14325654664956547\n",
      "After 8040 epochs, Loss = 0.1432147119860903\n",
      "After 8050 epochs, Loss = 0.1431729576738153\n",
      "After 8060 epochs, Loss = 0.14313128345551573\n",
      "After 8070 epochs, Loss = 0.14308968907510838\n",
      "After 8080 epochs, Loss = 0.14304817427764666\n",
      "After 8090 epochs, Loss = 0.1430067388093131\n",
      "After 8100 epochs, Loss = 0.14296538241741386\n",
      "After 8110 epochs, Loss = 0.14292410485037216\n",
      "After 8120 epochs, Loss = 0.1428829058577222\n",
      "After 8130 epochs, Loss = 0.1428417905814303\n",
      "After 8140 epochs, Loss = 0.14280075360274042\n",
      "After 8150 epochs, Loss = 0.1427597944558163\n",
      "After 8160 epochs, Loss = 0.14271891289450284\n",
      "After 8170 epochs, Loss = 0.1426781086737351\n",
      "After 8180 epochs, Loss = 0.1426373815495231\n",
      "After 8190 epochs, Loss = 0.14259673127894731\n",
      "After 8200 epochs, Loss = 0.1425561576201508\n",
      "After 8210 epochs, Loss = 0.14251566033233562\n",
      "After 8220 epochs, Loss = 0.14247523917575564\n",
      "After 8230 epochs, Loss = 0.14243489391171138\n",
      "After 8240 epochs, Loss = 0.1423946243025444\n",
      "After 8250 epochs, Loss = 0.14235443011163135\n",
      "After 8260 epochs, Loss = 0.14231431110337897\n",
      "After 8270 epochs, Loss = 0.14227426704321808\n",
      "After 8280 epochs, Loss = 0.14223429996079387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 8290 epochs, Loss = 0.14219441227757976\n",
      "After 8300 epochs, Loss = 0.14215459884463327\n",
      "After 8310 epochs, Loss = 0.14211485943140958\n",
      "After 8320 epochs, Loss = 0.14207519380836184\n",
      "After 8330 epochs, Loss = 0.14203560174693522\n",
      "After 8340 epochs, Loss = 0.14199608301956107\n",
      "After 8350 epochs, Loss = 0.14195663973912137\n",
      "After 8360 epochs, Loss = 0.14191727383676064\n",
      "After 8370 epochs, Loss = 0.14187798058983608\n",
      "After 8380 epochs, Loss = 0.14183875977465749\n",
      "After 8390 epochs, Loss = 0.14179961116849615\n",
      "After 8400 epochs, Loss = 0.14176053454958032\n",
      "After 8410 epochs, Loss = 0.14172152969708862\n",
      "After 8420 epochs, Loss = 0.1416826024036947\n",
      "After 8430 epochs, Loss = 0.14164375389988465\n",
      "After 8440 epochs, Loss = 0.14160497649768988\n",
      "After 8450 epochs, Loss = 0.1415662699800435\n",
      "After 8460 epochs, Loss = 0.141527634130809\n",
      "After 8470 epochs, Loss = 0.1414890687347704\n",
      "After 8480 epochs, Loss = 0.14145057357762714\n",
      "After 8490 epochs, Loss = 0.14141214844599004\n",
      "After 8500 epochs, Loss = 0.1413737931273754\n",
      "After 8510 epochs, Loss = 0.14133550741020137\n",
      "After 8520 epochs, Loss = 0.14129729108378208\n",
      "After 8530 epochs, Loss = 0.14125914393832353\n",
      "After 8540 epochs, Loss = 0.14122106576491905\n",
      "After 8550 epochs, Loss = 0.1411830563555442\n",
      "After 8560 epochs, Loss = 0.14114511550305203\n",
      "After 8570 epochs, Loss = 0.1411072430984534\n",
      "After 8580 epochs, Loss = 0.14106944309350025\n",
      "After 8590 epochs, Loss = 0.14103171102823803\n",
      "After 8600 epochs, Loss = 0.1409940466989877\n",
      "After 8610 epochs, Loss = 0.14095644990292377\n",
      "After 8620 epochs, Loss = 0.14091892700424205\n",
      "After 8630 epochs, Loss = 0.14088147244468227\n",
      "After 8640 epochs, Loss = 0.14084408481329674\n",
      "After 8650 epochs, Loss = 0.14080676391058478\n",
      "After 8660 epochs, Loss = 0.14076950953787978\n",
      "After 8670 epochs, Loss = 0.14073232149734408\n",
      "After 8680 epochs, Loss = 0.14069519959196508\n",
      "After 8690 epochs, Loss = 0.14065814439846008\n",
      "After 8700 epochs, Loss = 0.14062116201638397\n",
      "After 8710 epochs, Loss = 0.1405842451799632\n",
      "After 8720 epochs, Loss = 0.14054739369543315\n",
      "After 8730 epochs, Loss = 0.14051060736983964\n",
      "After 8740 epochs, Loss = 0.1404738860110283\n",
      "After 8750 epochs, Loss = 0.14043722942764023\n",
      "After 8760 epochs, Loss = 0.14040063742910838\n",
      "After 8770 epochs, Loss = 0.14036410982565253\n",
      "After 8780 epochs, Loss = 0.1403276464282762\n",
      "After 8790 epochs, Loss = 0.1402912478720083\n",
      "After 8800 epochs, Loss = 0.1402549212092146\n",
      "After 8810 epochs, Loss = 0.14021866447192688\n",
      "After 8820 epochs, Loss = 0.14018247117745952\n",
      "After 8830 epochs, Loss = 0.1401463411406903\n",
      "After 8840 epochs, Loss = 0.14011027417725772\n",
      "After 8850 epochs, Loss = 0.1400742701035554\n",
      "After 8860 epochs, Loss = 0.14003832873672778\n",
      "After 8870 epochs, Loss = 0.14000244989466715\n",
      "After 8880 epochs, Loss = 0.13996663339600904\n",
      "After 8890 epochs, Loss = 0.13993087906012902\n",
      "After 8900 epochs, Loss = 0.13989518670713844\n",
      "After 8910 epochs, Loss = 0.13985956088260668\n",
      "After 8920 epochs, Loss = 0.13982399677617194\n",
      "After 8930 epochs, Loss = 0.13978849411590016\n",
      "After 8940 epochs, Loss = 0.1397530527248077\n",
      "After 8950 epochs, Loss = 0.13971767242662927\n",
      "After 8960 epochs, Loss = 0.13968235304581308\n",
      "After 8970 epochs, Loss = 0.1396470944075185\n",
      "After 8980 epochs, Loss = 0.13961189633761173\n",
      "After 8990 epochs, Loss = 0.13957675866266223\n",
      "After 9000 epochs, Loss = 0.13954168120993923\n",
      "After 9010 epochs, Loss = 0.13950666380740906\n",
      "After 9020 epochs, Loss = 0.13947170628373007\n",
      "After 9030 epochs, Loss = 0.13943680846825066\n",
      "After 9040 epochs, Loss = 0.1394019701910052\n",
      "After 9050 epochs, Loss = 0.13936719128271038\n",
      "After 9060 epochs, Loss = 0.13933247157476206\n",
      "After 9070 epochs, Loss = 0.13929781444928596\n",
      "After 9080 epochs, Loss = 0.13926321690600504\n",
      "After 9090 epochs, Loss = 0.1392286780552945\n",
      "After 9100 epochs, Loss = 0.13919419792657503\n",
      "After 9110 epochs, Loss = 0.1391597780232821\n",
      "After 9120 epochs, Loss = 0.13912541631676317\n",
      "After 9130 epochs, Loss = 0.13909111264307616\n",
      "After 9140 epochs, Loss = 0.13905686683893256\n",
      "After 9150 epochs, Loss = 0.13902267874169336\n",
      "After 9160 epochs, Loss = 0.1389885481893658\n",
      "After 9170 epochs, Loss = 0.13895447502059982\n",
      "After 9180 epochs, Loss = 0.13892045907468548\n",
      "After 9190 epochs, Loss = 0.1388865001915496\n",
      "After 9200 epochs, Loss = 0.1388525982117521\n",
      "After 9210 epochs, Loss = 0.13881875297648344\n",
      "After 9220 epochs, Loss = 0.1387849643275616\n",
      "After 9230 epochs, Loss = 0.1387512321074284\n",
      "After 9240 epochs, Loss = 0.1387175561591469\n",
      "After 9250 epochs, Loss = 0.13868393632639833\n",
      "After 9260 epochs, Loss = 0.13865037245347872\n",
      "After 9270 epochs, Loss = 0.1386168643852962\n",
      "After 9280 epochs, Loss = 0.1385834119673677\n",
      "After 9290 epochs, Loss = 0.1385500150458167\n",
      "After 9300 epochs, Loss = 0.1385166737952726\n",
      "After 9310 epochs, Loss = 0.1384833918030146\n",
      "After 9320 epochs, Loss = 0.138450164849166\n",
      "After 9330 epochs, Loss = 0.13841699278223993\n",
      "After 9340 epochs, Loss = 0.13838387545134254\n",
      "After 9350 epochs, Loss = 0.1383508127061679\n",
      "After 9360 epochs, Loss = 0.13831780439699562\n",
      "After 9370 epochs, Loss = 0.13828485817343963\n",
      "After 9380 epochs, Loss = 0.13825196697491032\n",
      "After 9390 epochs, Loss = 0.1382191297619858\n",
      "After 9400 epochs, Loss = 0.13818634638725516\n",
      "After 9410 epochs, Loss = 0.1381536167038788\n",
      "After 9420 epochs, Loss = 0.13812094056558677\n",
      "After 9430 epochs, Loss = 0.1380883178266758\n",
      "After 9440 epochs, Loss = 0.13805574834200598\n",
      "After 9450 epochs, Loss = 0.13802323196699864\n",
      "After 9460 epochs, Loss = 0.13799076855763312\n",
      "After 9470 epochs, Loss = 0.1379583579704444\n",
      "After 9480 epochs, Loss = 0.13792600006252018\n",
      "After 9490 epochs, Loss = 0.13789369469149843\n",
      "After 9500 epochs, Loss = 0.13786144171556433\n",
      "After 9510 epochs, Loss = 0.13782924099344815\n",
      "After 9520 epochs, Loss = 0.13779709238442206\n",
      "After 9530 epochs, Loss = 0.13776499574829792\n",
      "After 9540 epochs, Loss = 0.13773295094542432\n",
      "After 9550 epochs, Loss = 0.1377009578366844\n",
      "After 9560 epochs, Loss = 0.13766901628349315\n",
      "After 9570 epochs, Loss = 0.13763712614779444\n",
      "After 9580 epochs, Loss = 0.13760528729205898\n",
      "After 9590 epochs, Loss = 0.13757349957928147\n",
      "After 9600 epochs, Loss = 0.1375417628729787\n",
      "After 9610 epochs, Loss = 0.13751007703718618\n",
      "After 9620 epochs, Loss = 0.13747844193645578\n",
      "After 9630 epochs, Loss = 0.13744685743585416\n",
      "After 9640 epochs, Loss = 0.1374153234009593\n",
      "After 9650 epochs, Loss = 0.1373838396978585\n",
      "After 9660 epochs, Loss = 0.13735240619314618\n",
      "After 9670 epochs, Loss = 0.1373210227539208\n",
      "After 9680 epochs, Loss = 0.13728968924778293\n",
      "After 9690 epochs, Loss = 0.137258405542833\n",
      "After 9700 epochs, Loss = 0.13722717150766855\n",
      "After 9710 epochs, Loss = 0.13719598701138191\n",
      "After 9720 epochs, Loss = 0.13716485192355826\n",
      "After 9730 epochs, Loss = 0.13713376611427297\n",
      "After 9740 epochs, Loss = 0.13710272945408922\n",
      "After 9750 epochs, Loss = 0.13707174181405576\n",
      "After 9760 epochs, Loss = 0.13704080306570476\n",
      "After 9770 epochs, Loss = 0.13700991308104962\n",
      "After 9780 epochs, Loss = 0.13697907173258198\n",
      "After 9790 epochs, Loss = 0.13694828823316021\n",
      "After 9800 epochs, Loss = 0.1369175555676357\n",
      "After 9810 epochs, Loss = 0.13688687115180614\n",
      "After 9820 epochs, Loss = 0.13685623486007462\n",
      "After 9830 epochs, Loss = 0.13682564656731203\n",
      "After 9840 epochs, Loss = 0.13679510614885437\n",
      "After 9850 epochs, Loss = 0.13676461348050029\n",
      "After 9860 epochs, Loss = 0.13673416843850925\n",
      "After 9870 epochs, Loss = 0.13670377089959912\n",
      "After 9880 epochs, Loss = 0.13667342074094405\n",
      "After 9890 epochs, Loss = 0.1366431178401729\n",
      "After 9900 epochs, Loss = 0.1366128620753662\n",
      "After 9910 epochs, Loss = 0.1365826533250551\n",
      "After 9920 epochs, Loss = 0.13655249146821816\n",
      "After 9930 epochs, Loss = 0.13652237638428016\n",
      "After 9940 epochs, Loss = 0.1364923079531099\n",
      "After 9950 epochs, Loss = 0.1364622860550176\n",
      "After 9960 epochs, Loss = 0.13643231057075347\n",
      "After 9970 epochs, Loss = 0.13640238138150568\n",
      "After 9980 epochs, Loss = 0.13637249836889792\n",
      "After 9990 epochs, Loss = 0.13634266141498733\n",
      "After 10000 epochs, Loss = 0.13631287040226334\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "valid_accuracy = []\n",
    "ite = [20, 200, 2000, 4000, 10000]\n",
    "lambda_val = [0.0000001, 0.00001, 0.001, 1, 3, 5, 5000]\n",
    "for i in ite:\n",
    "    theta, cost_epochs = logistic_regression(x_train_df, y_train_df, i, alpha = 0.1)\n",
    "    preds_prob = calc_h(x_valid_df, theta)\n",
    "    y_pred = preds_prob.round()\n",
    "    valid_accuracy.append(accuracy_score(y_valid_df,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c6908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy score for logistic regression for learning rate = 0.1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ite, valid_accuracy)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(\"Accuracy score for logistic regression for learning rate = 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae0cb0",
   "metadata": {},
   "source": [
    "<h3> Neural Networks </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdaaf94",
   "metadata": {},
   "source": [
    "<body> class that creates and trains a neural network model </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11bdde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    " \n",
    "# helpers\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    " \n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    " \n",
    "class Neural_Network:\n",
    "    # sizes is a list of the number of nodes in each layer\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.accuracy_mat = []\n",
    "        \n",
    "        #creating weights and bias\n",
    "        self.bias = []\n",
    "        self.weights = []\n",
    "        for layer in range(len(sizes)):\n",
    "            if layer != 0:\n",
    "                self.bias.append(np.random.randn(sizes[layer], 1))\n",
    "                \n",
    "                inp_layer = sizes[layer-1]\n",
    "                self.weights.append(np.random.randn(sizes[layer], inp_layer))\n",
    "    \n",
    "    def forwardPropagation(self, a):\n",
    "        for b, w in zip(self.bias, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "   \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        samples = len(training_data)\n",
    "       \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "       \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                            for k in range(0, samples, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                #print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "                accuracy = self.evaluate(test_data)\n",
    "                self.accuracy_mat.append(accuracy)\n",
    "                print(f\"Epoch {j}: {accuracy}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "   \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return(output_activations - y)\n",
    "   \n",
    "    def backpropagation(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # forwardPropagation\n",
    "        activation = x\n",
    "        activations = [x] # stores activations layer by layer\n",
    "        zs = [] # stores z vectors layer by layer\n",
    "        for b, w in zip(self.bias, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "       \n",
    "       \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_derivative(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "       \n",
    "        for _layer in range(2, self.num_layers):\n",
    "            z = zs[-_layer]\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-_layer+1].transpose(), delta) * sp\n",
    "            nabla_b[-_layer] = delta\n",
    "            nabla_w[-_layer] = np.dot(delta, activations[-_layer-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "   \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.bias = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.bias, nabla_b)]\n",
    "       \n",
    "    def evaluate(self, test_data):\n",
    "        pred = []\n",
    "        true_label = []\n",
    "        for (x, y) in test_data:\n",
    "            #print(x, y)\n",
    "            if y[0] == 1:\n",
    "                true_label.append(0)\n",
    "            else:\n",
    "                true_label.append(1)\n",
    "                \n",
    "            pred.append(np.argmax(self.forwardPropagation(x)))\n",
    "        return accuracy_score(true_label, pred)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        prediction = [np.argmax(self.forwardPropagation(x)) for (x, y) in data]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f197c",
   "metadata": {},
   "source": [
    "<body> function for doing one hot encoding. This is typically used on labels as the perceptron has two output neurons.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    encoded = np.zeros((2, 1))\n",
    "    encoded[y] = 1.0\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ba674",
   "metadata": {},
   "source": [
    "<body> doing the one hot encoding on the labels. </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb23fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_df = X_vector_train.toarray()\n",
    "x_valid_df = X_vector_valid.toarray()\n",
    "y_train_df = np.array(y_train)\n",
    "y_valid_df = np.array(y_valid)\n",
    "\n",
    "x_train_df = [np.reshape(x, (X_vector_train.shape[1], 1)) for x in x_train_df]\n",
    "y_train_df = [one_hot_encode(y) for y in y_train_df]\n",
    "\n",
    "x_valid_df = [np.reshape(x, (X_vector_train.shape[1], 1)) for x in x_valid_df]\n",
    "y_valid_df = [one_hot_encode(y) for y in y_valid_df]\n",
    "\n",
    "training_data = zip(x_train_df,y_train_df)\n",
    "valid_data = zip(x_valid_df,y_valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578ded8",
   "metadata": {},
   "source": [
    "<body> creating and training the networkk. </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51eb8cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.5522991409802931\n",
      "Epoch 1: 0.5694795351187468\n",
      "Epoch 2: 0.5750378979282466\n",
      "Epoch 3: 0.5775644264780192\n",
      "Epoch 4: 0.584133400707428\n",
      "Epoch 5: 0.5811015664477008\n",
      "Epoch 6: 0.5811015664477008\n",
      "Epoch 7: 0.5917129863567459\n",
      "Epoch 8: 0.5932289034866094\n",
      "Epoch 9: 0.5886811520970187\n",
      "Epoch 10: 0.5901970692268823\n",
      "Epoch 11: 0.5896917635169278\n",
      "Epoch 12: 0.596766043456291\n",
      "Epoch 13: 0.5932289034866094\n",
      "Epoch 14: 0.5922182920667004\n",
      "Epoch 15: 0.5952501263264275\n",
      "Epoch 16: 0.5997978777160182\n",
      "Epoch 17: 0.595755432036382\n",
      "Epoch 18: 0.6008084891359272\n",
      "Epoch 19: 0.6028297119757453\n",
      "Epoch 20: 0.6063668519454269\n",
      "Epoch 21: 0.6068721576553815\n",
      "Epoch 22: 0.6068721576553815\n",
      "Epoch 23: 0.6038403233956544\n",
      "Epoch 24: 0.608388074785245\n",
      "Epoch 25: 0.6038403233956544\n",
      "Epoch 26: 0.6099039919151087\n",
      "Epoch 27: 0.6124305204648812\n",
      "Epoch 28: 0.6220313289540171\n",
      "Epoch 29: 0.6124305204648812\n",
      "Epoch 30: 0.6184941889843355\n",
      "Epoch 31: 0.6255684689236989\n",
      "Epoch 32: 0.617988883274381\n",
      "Epoch 33: 0.6235472460838808\n",
      "Epoch 34: 0.6280949974734714\n",
      "Epoch 35: 0.6326427488630622\n",
      "Epoch 36: 0.6260737746336533\n",
      "Epoch 37: 0.6361798888327438\n",
      "Epoch 38: 0.6356745831227892\n",
      "Epoch 39: 0.6321374431531076\n",
      "Epoch 40: 0.6351692774128348\n",
      "Epoch 41: 0.6376958059626073\n",
      "Epoch 42: 0.6417382516422435\n",
      "Epoch 43: 0.6407276402223345\n",
      "Epoch 44: 0.6422435573521981\n",
      "Epoch 45: 0.6427488630621526\n",
      "Epoch 46: 0.6462860030318343\n",
      "Epoch 47: 0.6467913087417888\n",
      "Epoch 48: 0.6513390601313794\n",
      "Epoch 49: 0.651844365841334\n",
      "Epoch 50: 0.6513390601313794\n",
      "Epoch 51: 0.6553815058110156\n",
      "Epoch 52: 0.6584133400707428\n",
      "Epoch 53: 0.6619504800404244\n",
      "Epoch 54: 0.6604345629105609\n",
      "Epoch 55: 0.6629610914603336\n",
      "Epoch 56: 0.6614451743304699\n",
      "Epoch 57: 0.663466397170288\n",
      "Epoch 58: 0.6670035371399696\n",
      "Epoch 59: 0.6670035371399696\n",
      "Epoch 60: 0.6664982314300152\n",
      "Epoch 61: 0.6735725113693785\n",
      "Epoch 62: 0.6690247599797877\n",
      "Epoch 63: 0.6755937342091966\n",
      "Epoch 64: 0.6755937342091966\n",
      "Epoch 65: 0.6786255684689237\n",
      "Epoch 66: 0.6816574027286508\n",
      "Epoch 67: 0.6786255684689237\n",
      "Epoch 68: 0.6836786255684689\n",
      "Epoch 69: 0.6882263769580597\n",
      "Epoch 70: 0.6821627084386054\n",
      "Epoch 71: 0.6821627084386054\n",
      "Epoch 72: 0.6872157655381506\n",
      "Epoch 73: 0.6937847397675594\n",
      "Epoch 74: 0.6907529055078322\n",
      "Epoch 75: 0.6907529055078322\n",
      "Epoch 76: 0.696311268317332\n",
      "Epoch 77: 0.6917635169277413\n",
      "Epoch 78: 0.6912582112177867\n",
      "Epoch 79: 0.6937847397675594\n",
      "Epoch 80: 0.6953006568974229\n",
      "Epoch 81: 0.7003537139969682\n",
      "Epoch 82: 0.6988377968671046\n",
      "Epoch 83: 0.6998484082870137\n",
      "Epoch 84: 0.6983324911571501\n",
      "Epoch 85: 0.7018696311268318\n",
      "Epoch 86: 0.7043961596766043\n",
      "Epoch 87: 0.7074279939363315\n",
      "Epoch 88: 0.7043961596766043\n",
      "Epoch 89: 0.7054067710965134\n",
      "Epoch 90: 0.708943911066195\n",
      "Epoch 91: 0.7099545224861041\n",
      "Epoch 92: 0.7109651339060131\n",
      "Epoch 93: 0.7109651339060131\n",
      "Epoch 94: 0.7099545224861041\n",
      "Epoch 95: 0.7114704396159677\n",
      "Epoch 96: 0.7150075795856493\n",
      "Epoch 97: 0.717534108135422\n",
      "Epoch 98: 0.718544719555331\n",
      "Epoch 99: 0.720565942395149\n",
      "Epoch 100: 0.7190500252652855\n",
      "Epoch 101: 0.7165234967155129\n",
      "Epoch 102: 0.7246083880747852\n",
      "Epoch 103: 0.7220818595250126\n",
      "Epoch 104: 0.7261243052046488\n",
      "Epoch 105: 0.7200606366851945\n",
      "Epoch 106: 0.7286508337544214\n",
      "Epoch 107: 0.730166750884285\n",
      "Epoch 108: 0.7271349166245579\n",
      "Epoch 109: 0.732187973724103\n",
      "Epoch 110: 0.7316826680141486\n",
      "Epoch 111: 0.7342091965639211\n",
      "Epoch 112: 0.7316826680141486\n",
      "Epoch 113: 0.7342091965639211\n",
      "Epoch 114: 0.7337038908539667\n",
      "Epoch 115: 0.7397675593734209\n",
      "Epoch 116: 0.7362304194037392\n",
      "Epoch 117: 0.7427993936331481\n",
      "Epoch 118: 0.7377463365336028\n",
      "Epoch 119: 0.7453259221829207\n",
      "Epoch 120: 0.7443153107630116\n",
      "Epoch 121: 0.7468418393127843\n",
      "Epoch 122: 0.7478524507326932\n",
      "Epoch 123: 0.7448206164729662\n",
      "Epoch 124: 0.7458312278928751\n",
      "Epoch 125: 0.7473471450227388\n",
      "Epoch 126: 0.7488630621526023\n",
      "Epoch 127: 0.7503789792824659\n",
      "Epoch 128: 0.752400202122284\n",
      "Epoch 129: 0.7503789792824659\n",
      "Epoch 130: 0.7529055078322385\n",
      "Epoch 131: 0.7554320363820111\n",
      "Epoch 132: 0.7579585649317837\n",
      "Epoch 133: 0.7579585649317837\n",
      "Epoch 134: 0.7544214249621021\n",
      "Epoch 135: 0.7564426478019202\n",
      "Epoch 136: 0.7609903991915109\n",
      "Epoch 137: 0.7594744820616472\n",
      "Epoch 138: 0.7599797877716018\n",
      "Epoch 139: 0.7589691763516928\n",
      "Epoch 140: 0.7589691763516928\n",
      "Epoch 141: 0.7589691763516928\n",
      "Epoch 142: 0.7478524507326932\n",
      "Epoch 143: 0.7650328448711471\n",
      "Epoch 144: 0.7635169277412834\n",
      "Epoch 145: 0.7609903991915109\n",
      "Epoch 146: 0.7594744820616472\n",
      "Epoch 147: 0.7675593734209196\n",
      "Epoch 148: 0.7645275391611925\n",
      "Epoch 149: 0.7660434562910561\n",
      "Epoch 150: 0.7645275391611925\n",
      "Epoch 151: 0.7665487620010106\n",
      "Epoch 152: 0.7660434562910561\n",
      "Epoch 153: 0.7655381505811015\n",
      "Epoch 154: 0.7670540677109652\n",
      "Epoch 155: 0.7655381505811015\n",
      "Epoch 156: 0.7685699848408287\n",
      "Epoch 157: 0.775644264780192\n",
      "Epoch 158: 0.7665487620010106\n",
      "Epoch 159: 0.7635169277412834\n",
      "Epoch 160: 0.7761495704901465\n",
      "Epoch 161: 0.7751389590702374\n",
      "Epoch 162: 0.7695805962607377\n",
      "Epoch 163: 0.7766548762001011\n",
      "Epoch 164: 0.775644264780192\n",
      "Epoch 165: 0.7751389590702374\n",
      "Epoch 166: 0.7791814047498736\n",
      "Epoch 167: 0.775644264780192\n",
      "Epoch 168: 0.7705912076806468\n",
      "Epoch 169: 0.7761495704901465\n",
      "Epoch 170: 0.7751389590702374\n",
      "Epoch 171: 0.7801920161697827\n",
      "Epoch 172: 0.7786760990399192\n",
      "Epoch 173: 0.7771601819100555\n",
      "Epoch 174: 0.7812026275896917\n",
      "Epoch 175: 0.7852450732693279\n",
      "Epoch 176: 0.7832238504295098\n",
      "Epoch 177: 0.7751389590702374\n",
      "Epoch 178: 0.7827185447195554\n",
      "Epoch 179: 0.7832238504295098\n",
      "Epoch 180: 0.7857503789792825\n",
      "Epoch 181: 0.7832238504295098\n",
      "Epoch 182: 0.7766548762001011\n",
      "Epoch 183: 0.7892875189489641\n",
      "Epoch 184: 0.7887822132390097\n",
      "Epoch 185: 0.7882769075290551\n",
      "Epoch 186: 0.7791814047498736\n",
      "Epoch 187: 0.7852450732693279\n",
      "Epoch 188: 0.7908034360788276\n",
      "Epoch 189: 0.7908034360788276\n",
      "Epoch 190: 0.7897928246589186\n",
      "Epoch 191: 0.7832238504295098\n",
      "Epoch 192: 0.7822132390096008\n",
      "Epoch 193: 0.7948458817584638\n",
      "Epoch 194: 0.7938352703385548\n",
      "Epoch 195: 0.7943405760485094\n",
      "Epoch 196: 0.7948458817584638\n",
      "Epoch 197: 0.7943405760485094\n",
      "Epoch 198: 0.797877716018191\n",
      "Epoch 199: 0.7948458817584638\n",
      "Epoch 200: 0.7988883274381\n",
      "Epoch 201: 0.7973724103082365\n",
      "Epoch 202: 0.7983830217281456\n",
      "Epoch 203: 0.7973724103082365\n",
      "Epoch 204: 0.7908034360788276\n",
      "Epoch 205: 0.8004042445679637\n",
      "Epoch 206: 0.7897928246589186\n",
      "Epoch 207: 0.7923193532086913\n",
      "Epoch 208: 0.8014148559878727\n",
      "Epoch 209: 0.8039413845376453\n",
      "Epoch 210: 0.8059626073774634\n",
      "Epoch 211: 0.8034360788276907\n",
      "Epoch 212: 0.8064679130874179\n",
      "Epoch 213: 0.8084891359272359\n",
      "Epoch 214: 0.7943405760485094\n",
      "Epoch 215: 0.8069732187973724\n",
      "Epoch 216: 0.8069732187973724\n",
      "Epoch 217: 0.8029307731177362\n",
      "Epoch 218: 0.8064679130874179\n",
      "Epoch 219: 0.810510358767054\n",
      "Epoch 220: 0.8130368873168267\n",
      "Epoch 221: 0.8110156644770086\n",
      "Epoch 222: 0.8089944416371905\n",
      "Epoch 223: 0.8110156644770086\n",
      "Epoch 224: 0.8145528044466902\n",
      "Epoch 225: 0.7993936331480546\n",
      "Epoch 226: 0.8120262758969177\n",
      "Epoch 227: 0.8024254674077818\n",
      "Epoch 228: 0.8125315816068721\n",
      "Epoch 229: 0.8004042445679637\n",
      "Epoch 230: 0.8155634158665993\n",
      "Epoch 231: 0.8115209701869631\n",
      "Epoch 232: 0.8160687215765539\n",
      "Epoch 233: 0.8120262758969177\n",
      "Epoch 234: 0.7998989388580091\n",
      "Epoch 235: 0.8160687215765539\n",
      "Epoch 236: 0.8185952501263264\n",
      "Epoch 237: 0.8140474987367358\n",
      "Epoch 238: 0.8191005558362809\n",
      "Epoch 239: 0.8185952501263264\n",
      "Epoch 240: 0.7988883274381\n",
      "Epoch 241: 0.8206164729661445\n",
      "Epoch 242: 0.8130368873168267\n",
      "Epoch 243: 0.8216270843860536\n",
      "Epoch 244: 0.8241536129358262\n",
      "Epoch 245: 0.8231430015159171\n",
      "Epoch 246: 0.8140474987367358\n",
      "Epoch 247: 0.8206164729661445\n",
      "Epoch 248: 0.8236483072258717\n",
      "Epoch 249: 0.8246589186457807\n",
      "Epoch 250: 0.8266801414855988\n",
      "Epoch 251: 0.8165740272865083\n",
      "Epoch 252: 0.8271854471955533\n",
      "Epoch 253: 0.821121778676099\n",
      "Epoch 254: 0.8287013643254169\n",
      "Epoch 255: 0.8196058615462355\n",
      "Epoch 256: 0.8196058615462355\n",
      "Epoch 257: 0.8266801414855988\n",
      "Epoch 258: 0.8256695300656898\n",
      "Epoch 259: 0.8276907529055079\n",
      "Epoch 260: 0.8287013643254169\n",
      "Epoch 261: 0.829711975745326\n",
      "Epoch 262: 0.8206164729661445\n",
      "Epoch 263: 0.8140474987367358\n",
      "Epoch 264: 0.8322385042950985\n",
      "Epoch 265: 0.8322385042950985\n",
      "Epoch 266: 0.8241536129358262\n",
      "Epoch 267: 0.831733198585144\n",
      "Epoch 268: 0.8287013643254169\n",
      "Epoch 269: 0.8347650328448711\n",
      "Epoch 270: 0.8337544214249621\n",
      "Epoch 271: 0.8352703385548257\n",
      "Epoch 272: 0.8287013643254169\n",
      "Epoch 273: 0.8357756442647802\n",
      "Epoch 274: 0.8322385042950985\n",
      "Epoch 275: 0.8362809499747347\n",
      "Epoch 276: 0.8266801414855988\n",
      "Epoch 277: 0.8322385042950985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278: 0.8357756442647802\n",
      "Epoch 279: 0.8241536129358262\n",
      "Epoch 280: 0.8362809499747347\n",
      "Epoch 281: 0.8383021728145528\n",
      "Epoch 282: 0.8383021728145528\n",
      "Epoch 283: 0.8393127842344619\n",
      "Epoch 284: 0.831733198585144\n",
      "Epoch 285: 0.8383021728145528\n",
      "Epoch 286: 0.8362809499747347\n",
      "Epoch 287: 0.8388074785245073\n",
      "Epoch 288: 0.8393127842344619\n",
      "Epoch 289: 0.8388074785245073\n",
      "Epoch 290: 0.8408287013643254\n",
      "Epoch 291: 0.830722587165235\n",
      "Epoch 292: 0.842344618494189\n",
      "Epoch 293: 0.832743810005053\n",
      "Epoch 294: 0.84133400707428\n",
      "Epoch 295: 0.84133400707428\n",
      "Epoch 296: 0.842344618494189\n",
      "Epoch 297: 0.844365841334007\n",
      "Epoch 298: 0.8448711470439616\n",
      "Epoch 299: 0.8408287013643254\n",
      "Epoch 300: 0.8489135927235978\n",
      "Epoch 301: 0.8393127842344619\n",
      "Epoch 302: 0.8367862556846892\n",
      "Epoch 303: 0.8463870641738251\n",
      "Epoch 304: 0.8428499242041435\n",
      "Epoch 305: 0.8489135927235978\n",
      "Epoch 306: 0.8494188984335523\n",
      "Epoch 307: 0.844365841334007\n",
      "Epoch 308: 0.8499242041435068\n",
      "Epoch 309: 0.8438605356240525\n",
      "Epoch 310: 0.842344618494189\n",
      "Epoch 311: 0.8448711470439616\n",
      "Epoch 312: 0.8458817584638706\n",
      "Epoch 313: 0.8453764527539162\n",
      "Epoch 314: 0.8453764527539162\n",
      "Epoch 315: 0.8519454269833249\n",
      "Epoch 316: 0.8463870641738251\n",
      "Epoch 317: 0.8479029813036887\n",
      "Epoch 318: 0.8479029813036887\n",
      "Epoch 319: 0.8504295098534613\n",
      "Epoch 320: 0.8524507326932794\n",
      "Epoch 321: 0.8494188984335523\n",
      "Epoch 322: 0.852956038403234\n",
      "Epoch 323: 0.8489135927235978\n",
      "Epoch 324: 0.8489135927235978\n",
      "Epoch 325: 0.8514401212733704\n",
      "Epoch 326: 0.8509348155634159\n",
      "Epoch 327: 0.8524507326932794\n",
      "Epoch 328: 0.852956038403234\n",
      "Epoch 329: 0.8433552299140981\n",
      "Epoch 330: 0.852956038403234\n",
      "Epoch 331: 0.8544719555330975\n",
      "Epoch 332: 0.8549772612430521\n",
      "Epoch 333: 0.8519454269833249\n",
      "Epoch 334: 0.853966649823143\n",
      "Epoch 335: 0.852956038403234\n",
      "Epoch 336: 0.8514401212733704\n",
      "Epoch 337: 0.8352703385548257\n",
      "Epoch 338: 0.853966649823143\n",
      "Epoch 339: 0.8559878726629611\n",
      "Epoch 340: 0.8519454269833249\n",
      "Epoch 341: 0.8504295098534613\n",
      "Epoch 342: 0.8575037897928247\n",
      "Epoch 343: 0.8534613441131885\n",
      "Epoch 344: 0.8564931783729156\n",
      "Epoch 345: 0.8585144012127337\n",
      "Epoch 346: 0.8524507326932794\n",
      "Epoch 347: 0.8554825669530066\n",
      "Epoch 348: 0.8580090955027792\n",
      "Epoch 349: 0.8569984840828702\n",
      "Epoch 350: 0.8575037897928247\n",
      "Epoch 351: 0.8514401212733704\n",
      "Epoch 352: 0.8534613441131885\n",
      "Epoch 353: 0.8590197069226883\n",
      "Epoch 354: 0.8600303183425972\n",
      "Epoch 355: 0.8514401212733704\n",
      "Epoch 356: 0.8615462354724608\n",
      "Epoch 357: 0.8549772612430521\n",
      "Epoch 358: 0.8569984840828702\n",
      "Epoch 359: 0.8605356240525518\n",
      "Epoch 360: 0.8595250126326428\n",
      "Epoch 361: 0.8600303183425972\n",
      "Epoch 362: 0.8564931783729156\n",
      "Epoch 363: 0.8575037897928247\n",
      "Epoch 364: 0.8625568468923699\n",
      "Epoch 365: 0.8585144012127337\n",
      "Epoch 366: 0.8580090955027792\n",
      "Epoch 367: 0.8620515411824153\n",
      "Epoch 368: 0.8575037897928247\n",
      "Epoch 369: 0.8600303183425972\n",
      "Epoch 370: 0.8569984840828702\n",
      "Epoch 371: 0.8600303183425972\n",
      "Epoch 372: 0.8600303183425972\n",
      "Epoch 373: 0.865588681152097\n",
      "Epoch 374: 0.8595250126326428\n",
      "Epoch 375: 0.8580090955027792\n",
      "Epoch 376: 0.8650833754421425\n",
      "Epoch 377: 0.8625568468923699\n",
      "Epoch 378: 0.865588681152097\n",
      "Epoch 379: 0.8620515411824153\n",
      "Epoch 380: 0.8640727640222334\n",
      "Epoch 381: 0.8595250126326428\n",
      "Epoch 382: 0.8509348155634159\n",
      "Epoch 383: 0.865588681152097\n",
      "Epoch 384: 0.865588681152097\n",
      "Epoch 385: 0.864578069732188\n",
      "Epoch 386: 0.8660939868620515\n",
      "Epoch 387: 0.864578069732188\n",
      "Epoch 388: 0.8650833754421425\n",
      "Epoch 389: 0.8630621526023244\n",
      "Epoch 390: 0.8630621526023244\n",
      "Epoch 391: 0.8504295098534613\n",
      "Epoch 392: 0.8660939868620515\n",
      "Epoch 393: 0.864578069732188\n",
      "Epoch 394: 0.8630621526023244\n",
      "Epoch 395: 0.8650833754421425\n",
      "Epoch 396: 0.8600303183425972\n",
      "Epoch 397: 0.865588681152097\n",
      "Epoch 398: 0.865588681152097\n",
      "Epoch 399: 0.865588681152097\n",
      "Epoch 400: 0.864578069732188\n",
      "Epoch 401: 0.8610409297625063\n",
      "Epoch 402: 0.8671045982819606\n",
      "Epoch 403: 0.864578069732188\n",
      "Epoch 404: 0.865588681152097\n",
      "Epoch 405: 0.8676099039919151\n",
      "Epoch 406: 0.8630621526023244\n",
      "Epoch 407: 0.8676099039919151\n",
      "Epoch 408: 0.865588681152097\n",
      "Epoch 409: 0.8660939868620515\n",
      "Epoch 410: 0.8625568468923699\n",
      "Epoch 411: 0.8569984840828702\n",
      "Epoch 412: 0.865588681152097\n",
      "Epoch 413: 0.8686205154118242\n",
      "Epoch 414: 0.8691258211217787\n",
      "Epoch 415: 0.8605356240525518\n",
      "Epoch 416: 0.8696311268317332\n",
      "Epoch 417: 0.8676099039919151\n",
      "Epoch 418: 0.8615462354724608\n",
      "Epoch 419: 0.8691258211217787\n",
      "Epoch 420: 0.8716523496715513\n",
      "Epoch 421: 0.864578069732188\n",
      "Epoch 422: 0.8671045982819606\n",
      "Epoch 423: 0.864578069732188\n",
      "Epoch 424: 0.8650833754421425\n",
      "Epoch 425: 0.8681152097018696\n",
      "Epoch 426: 0.8711470439615968\n",
      "Epoch 427: 0.864578069732188\n",
      "Epoch 428: 0.8716523496715513\n",
      "Epoch 429: 0.864578069732188\n",
      "Epoch 430: 0.8701364325416877\n",
      "Epoch 431: 0.8686205154118242\n",
      "Epoch 432: 0.8665992925720061\n",
      "Epoch 433: 0.8691258211217787\n",
      "Epoch 434: 0.8671045982819606\n",
      "Epoch 435: 0.865588681152097\n",
      "Epoch 436: 0.8650833754421425\n",
      "Epoch 437: 0.8746841839312784\n",
      "Epoch 438: 0.8741788782213239\n",
      "Epoch 439: 0.8660939868620515\n",
      "Epoch 440: 0.8676099039919151\n",
      "Epoch 441: 0.8706417382516423\n",
      "Epoch 442: 0.8686205154118242\n",
      "Epoch 443: 0.8660939868620515\n",
      "Epoch 444: 0.8721576553815058\n",
      "Epoch 445: 0.8726629610914604\n",
      "Epoch 446: 0.8691258211217787\n",
      "Epoch 447: 0.8701364325416877\n",
      "Epoch 448: 0.8681152097018696\n",
      "Epoch 449: 0.8746841839312784\n",
      "Epoch 450: 0.8716523496715513\n",
      "Epoch 451: 0.8686205154118242\n",
      "Epoch 452: 0.8706417382516423\n",
      "Epoch 453: 0.8681152097018696\n",
      "Epoch 454: 0.8711470439615968\n",
      "Epoch 455: 0.8746841839312784\n",
      "Epoch 456: 0.8746841839312784\n",
      "Epoch 457: 0.8721576553815058\n",
      "Epoch 458: 0.8706417382516423\n",
      "Epoch 459: 0.8711470439615968\n",
      "Epoch 460: 0.8665992925720061\n",
      "Epoch 461: 0.8731682668014149\n",
      "Epoch 462: 0.8701364325416877\n",
      "Epoch 463: 0.8767054067710965\n",
      "Epoch 464: 0.8635674583122789\n",
      "Epoch 465: 0.8767054067710965\n",
      "Epoch 466: 0.8756947953511874\n",
      "Epoch 467: 0.8681152097018696\n",
      "Epoch 468: 0.876200101061142\n",
      "Epoch 469: 0.875189489641233\n",
      "Epoch 470: 0.8716523496715513\n",
      "Epoch 471: 0.875189489641233\n",
      "Epoch 472: 0.877210712481051\n",
      "Epoch 473: 0.8716523496715513\n",
      "Epoch 474: 0.8736735725113693\n",
      "Epoch 475: 0.8716523496715513\n",
      "Epoch 476: 0.8767054067710965\n",
      "Epoch 477: 0.8721576553815058\n",
      "Epoch 478: 0.8756947953511874\n",
      "Epoch 479: 0.8746841839312784\n",
      "Epoch 480: 0.8777160181910055\n",
      "Epoch 481: 0.8716523496715513\n",
      "Epoch 482: 0.8777160181910055\n",
      "Epoch 483: 0.8746841839312784\n",
      "Epoch 484: 0.875189489641233\n",
      "Epoch 485: 0.8741788782213239\n",
      "Epoch 486: 0.8731682668014149\n",
      "Epoch 487: 0.8777160181910055\n",
      "Epoch 488: 0.877210712481051\n",
      "Epoch 489: 0.877210712481051\n",
      "Epoch 490: 0.875189489641233\n",
      "Epoch 491: 0.8756947953511874\n",
      "Epoch 492: 0.8756947953511874\n",
      "Epoch 493: 0.8777160181910055\n",
      "Epoch 494: 0.877210712481051\n",
      "Epoch 495: 0.877210712481051\n",
      "Epoch 496: 0.8711470439615968\n",
      "Epoch 497: 0.877210712481051\n",
      "Epoch 498: 0.8777160181910055\n",
      "Epoch 499: 0.875189489641233\n"
     ]
    }
   ],
   "source": [
    "net = Neural_Network([X_vector_train.shape[1], 100, 25, 2])\n",
    "net.SGD(training_data, 500, 200, 0.5, test_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82919fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5522991409802931,\n",
       " 0.5694795351187468,\n",
       " 0.5750378979282466,\n",
       " 0.5775644264780192,\n",
       " 0.584133400707428,\n",
       " 0.5811015664477008,\n",
       " 0.5811015664477008,\n",
       " 0.5917129863567459,\n",
       " 0.5932289034866094,\n",
       " 0.5886811520970187,\n",
       " 0.5901970692268823,\n",
       " 0.5896917635169278,\n",
       " 0.596766043456291,\n",
       " 0.5932289034866094,\n",
       " 0.5922182920667004,\n",
       " 0.5952501263264275,\n",
       " 0.5997978777160182,\n",
       " 0.595755432036382,\n",
       " 0.6008084891359272,\n",
       " 0.6028297119757453,\n",
       " 0.6063668519454269,\n",
       " 0.6068721576553815,\n",
       " 0.6068721576553815,\n",
       " 0.6038403233956544,\n",
       " 0.608388074785245,\n",
       " 0.6038403233956544,\n",
       " 0.6099039919151087,\n",
       " 0.6124305204648812,\n",
       " 0.6220313289540171,\n",
       " 0.6124305204648812,\n",
       " 0.6184941889843355,\n",
       " 0.6255684689236989,\n",
       " 0.617988883274381,\n",
       " 0.6235472460838808,\n",
       " 0.6280949974734714,\n",
       " 0.6326427488630622,\n",
       " 0.6260737746336533,\n",
       " 0.6361798888327438,\n",
       " 0.6356745831227892,\n",
       " 0.6321374431531076,\n",
       " 0.6351692774128348,\n",
       " 0.6376958059626073,\n",
       " 0.6417382516422435,\n",
       " 0.6407276402223345,\n",
       " 0.6422435573521981,\n",
       " 0.6427488630621526,\n",
       " 0.6462860030318343,\n",
       " 0.6467913087417888,\n",
       " 0.6513390601313794,\n",
       " 0.651844365841334,\n",
       " 0.6513390601313794,\n",
       " 0.6553815058110156,\n",
       " 0.6584133400707428,\n",
       " 0.6619504800404244,\n",
       " 0.6604345629105609,\n",
       " 0.6629610914603336,\n",
       " 0.6614451743304699,\n",
       " 0.663466397170288,\n",
       " 0.6670035371399696,\n",
       " 0.6670035371399696,\n",
       " 0.6664982314300152,\n",
       " 0.6735725113693785,\n",
       " 0.6690247599797877,\n",
       " 0.6755937342091966,\n",
       " 0.6755937342091966,\n",
       " 0.6786255684689237,\n",
       " 0.6816574027286508,\n",
       " 0.6786255684689237,\n",
       " 0.6836786255684689,\n",
       " 0.6882263769580597,\n",
       " 0.6821627084386054,\n",
       " 0.6821627084386054,\n",
       " 0.6872157655381506,\n",
       " 0.6937847397675594,\n",
       " 0.6907529055078322,\n",
       " 0.6907529055078322,\n",
       " 0.696311268317332,\n",
       " 0.6917635169277413,\n",
       " 0.6912582112177867,\n",
       " 0.6937847397675594,\n",
       " 0.6953006568974229,\n",
       " 0.7003537139969682,\n",
       " 0.6988377968671046,\n",
       " 0.6998484082870137,\n",
       " 0.6983324911571501,\n",
       " 0.7018696311268318,\n",
       " 0.7043961596766043,\n",
       " 0.7074279939363315,\n",
       " 0.7043961596766043,\n",
       " 0.7054067710965134,\n",
       " 0.708943911066195,\n",
       " 0.7099545224861041,\n",
       " 0.7109651339060131,\n",
       " 0.7109651339060131,\n",
       " 0.7099545224861041,\n",
       " 0.7114704396159677,\n",
       " 0.7150075795856493,\n",
       " 0.717534108135422,\n",
       " 0.718544719555331,\n",
       " 0.720565942395149,\n",
       " 0.7190500252652855,\n",
       " 0.7165234967155129,\n",
       " 0.7246083880747852,\n",
       " 0.7220818595250126,\n",
       " 0.7261243052046488,\n",
       " 0.7200606366851945,\n",
       " 0.7286508337544214,\n",
       " 0.730166750884285,\n",
       " 0.7271349166245579,\n",
       " 0.732187973724103,\n",
       " 0.7316826680141486,\n",
       " 0.7342091965639211,\n",
       " 0.7316826680141486,\n",
       " 0.7342091965639211,\n",
       " 0.7337038908539667,\n",
       " 0.7397675593734209,\n",
       " 0.7362304194037392,\n",
       " 0.7427993936331481,\n",
       " 0.7377463365336028,\n",
       " 0.7453259221829207,\n",
       " 0.7443153107630116,\n",
       " 0.7468418393127843,\n",
       " 0.7478524507326932,\n",
       " 0.7448206164729662,\n",
       " 0.7458312278928751,\n",
       " 0.7473471450227388,\n",
       " 0.7488630621526023,\n",
       " 0.7503789792824659,\n",
       " 0.752400202122284,\n",
       " 0.7503789792824659,\n",
       " 0.7529055078322385,\n",
       " 0.7554320363820111,\n",
       " 0.7579585649317837,\n",
       " 0.7579585649317837,\n",
       " 0.7544214249621021,\n",
       " 0.7564426478019202,\n",
       " 0.7609903991915109,\n",
       " 0.7594744820616472,\n",
       " 0.7599797877716018,\n",
       " 0.7589691763516928,\n",
       " 0.7589691763516928,\n",
       " 0.7589691763516928,\n",
       " 0.7478524507326932,\n",
       " 0.7650328448711471,\n",
       " 0.7635169277412834,\n",
       " 0.7609903991915109,\n",
       " 0.7594744820616472,\n",
       " 0.7675593734209196,\n",
       " 0.7645275391611925,\n",
       " 0.7660434562910561,\n",
       " 0.7645275391611925,\n",
       " 0.7665487620010106,\n",
       " 0.7660434562910561,\n",
       " 0.7655381505811015,\n",
       " 0.7670540677109652,\n",
       " 0.7655381505811015,\n",
       " 0.7685699848408287,\n",
       " 0.775644264780192,\n",
       " 0.7665487620010106,\n",
       " 0.7635169277412834,\n",
       " 0.7761495704901465,\n",
       " 0.7751389590702374,\n",
       " 0.7695805962607377,\n",
       " 0.7766548762001011,\n",
       " 0.775644264780192,\n",
       " 0.7751389590702374,\n",
       " 0.7791814047498736,\n",
       " 0.775644264780192,\n",
       " 0.7705912076806468,\n",
       " 0.7761495704901465,\n",
       " 0.7751389590702374,\n",
       " 0.7801920161697827,\n",
       " 0.7786760990399192,\n",
       " 0.7771601819100555,\n",
       " 0.7812026275896917,\n",
       " 0.7852450732693279,\n",
       " 0.7832238504295098,\n",
       " 0.7751389590702374,\n",
       " 0.7827185447195554,\n",
       " 0.7832238504295098,\n",
       " 0.7857503789792825,\n",
       " 0.7832238504295098,\n",
       " 0.7766548762001011,\n",
       " 0.7892875189489641,\n",
       " 0.7887822132390097,\n",
       " 0.7882769075290551,\n",
       " 0.7791814047498736,\n",
       " 0.7852450732693279,\n",
       " 0.7908034360788276,\n",
       " 0.7908034360788276,\n",
       " 0.7897928246589186,\n",
       " 0.7832238504295098,\n",
       " 0.7822132390096008,\n",
       " 0.7948458817584638,\n",
       " 0.7938352703385548,\n",
       " 0.7943405760485094,\n",
       " 0.7948458817584638,\n",
       " 0.7943405760485094,\n",
       " 0.797877716018191,\n",
       " 0.7948458817584638,\n",
       " 0.7988883274381,\n",
       " 0.7973724103082365,\n",
       " 0.7983830217281456,\n",
       " 0.7973724103082365,\n",
       " 0.7908034360788276,\n",
       " 0.8004042445679637,\n",
       " 0.7897928246589186,\n",
       " 0.7923193532086913,\n",
       " 0.8014148559878727,\n",
       " 0.8039413845376453,\n",
       " 0.8059626073774634,\n",
       " 0.8034360788276907,\n",
       " 0.8064679130874179,\n",
       " 0.8084891359272359,\n",
       " 0.7943405760485094,\n",
       " 0.8069732187973724,\n",
       " 0.8069732187973724,\n",
       " 0.8029307731177362,\n",
       " 0.8064679130874179,\n",
       " 0.810510358767054,\n",
       " 0.8130368873168267,\n",
       " 0.8110156644770086,\n",
       " 0.8089944416371905,\n",
       " 0.8110156644770086,\n",
       " 0.8145528044466902,\n",
       " 0.7993936331480546,\n",
       " 0.8120262758969177,\n",
       " 0.8024254674077818,\n",
       " 0.8125315816068721,\n",
       " 0.8004042445679637,\n",
       " 0.8155634158665993,\n",
       " 0.8115209701869631,\n",
       " 0.8160687215765539,\n",
       " 0.8120262758969177,\n",
       " 0.7998989388580091,\n",
       " 0.8160687215765539,\n",
       " 0.8185952501263264,\n",
       " 0.8140474987367358,\n",
       " 0.8191005558362809,\n",
       " 0.8185952501263264,\n",
       " 0.7988883274381,\n",
       " 0.8206164729661445,\n",
       " 0.8130368873168267,\n",
       " 0.8216270843860536,\n",
       " 0.8241536129358262,\n",
       " 0.8231430015159171,\n",
       " 0.8140474987367358,\n",
       " 0.8206164729661445,\n",
       " 0.8236483072258717,\n",
       " 0.8246589186457807,\n",
       " 0.8266801414855988,\n",
       " 0.8165740272865083,\n",
       " 0.8271854471955533,\n",
       " 0.821121778676099,\n",
       " 0.8287013643254169,\n",
       " 0.8196058615462355,\n",
       " 0.8196058615462355,\n",
       " 0.8266801414855988,\n",
       " 0.8256695300656898,\n",
       " 0.8276907529055079,\n",
       " 0.8287013643254169,\n",
       " 0.829711975745326,\n",
       " 0.8206164729661445,\n",
       " 0.8140474987367358,\n",
       " 0.8322385042950985,\n",
       " 0.8322385042950985,\n",
       " 0.8241536129358262,\n",
       " 0.831733198585144,\n",
       " 0.8287013643254169,\n",
       " 0.8347650328448711,\n",
       " 0.8337544214249621,\n",
       " 0.8352703385548257,\n",
       " 0.8287013643254169,\n",
       " 0.8357756442647802,\n",
       " 0.8322385042950985,\n",
       " 0.8362809499747347,\n",
       " 0.8266801414855988,\n",
       " 0.8322385042950985,\n",
       " 0.8357756442647802,\n",
       " 0.8241536129358262,\n",
       " 0.8362809499747347,\n",
       " 0.8383021728145528,\n",
       " 0.8383021728145528,\n",
       " 0.8393127842344619,\n",
       " 0.831733198585144,\n",
       " 0.8383021728145528,\n",
       " 0.8362809499747347,\n",
       " 0.8388074785245073,\n",
       " 0.8393127842344619,\n",
       " 0.8388074785245073,\n",
       " 0.8408287013643254,\n",
       " 0.830722587165235,\n",
       " 0.842344618494189,\n",
       " 0.832743810005053,\n",
       " 0.84133400707428,\n",
       " 0.84133400707428,\n",
       " 0.842344618494189,\n",
       " 0.844365841334007,\n",
       " 0.8448711470439616,\n",
       " 0.8408287013643254,\n",
       " 0.8489135927235978,\n",
       " 0.8393127842344619,\n",
       " 0.8367862556846892,\n",
       " 0.8463870641738251,\n",
       " 0.8428499242041435,\n",
       " 0.8489135927235978,\n",
       " 0.8494188984335523,\n",
       " 0.844365841334007,\n",
       " 0.8499242041435068,\n",
       " 0.8438605356240525,\n",
       " 0.842344618494189,\n",
       " 0.8448711470439616,\n",
       " 0.8458817584638706,\n",
       " 0.8453764527539162,\n",
       " 0.8453764527539162,\n",
       " 0.8519454269833249,\n",
       " 0.8463870641738251,\n",
       " 0.8479029813036887,\n",
       " 0.8479029813036887,\n",
       " 0.8504295098534613,\n",
       " 0.8524507326932794,\n",
       " 0.8494188984335523,\n",
       " 0.852956038403234,\n",
       " 0.8489135927235978,\n",
       " 0.8489135927235978,\n",
       " 0.8514401212733704,\n",
       " 0.8509348155634159,\n",
       " 0.8524507326932794,\n",
       " 0.852956038403234,\n",
       " 0.8433552299140981,\n",
       " 0.852956038403234,\n",
       " 0.8544719555330975,\n",
       " 0.8549772612430521,\n",
       " 0.8519454269833249,\n",
       " 0.853966649823143,\n",
       " 0.852956038403234,\n",
       " 0.8514401212733704,\n",
       " 0.8352703385548257,\n",
       " 0.853966649823143,\n",
       " 0.8559878726629611,\n",
       " 0.8519454269833249,\n",
       " 0.8504295098534613,\n",
       " 0.8575037897928247,\n",
       " 0.8534613441131885,\n",
       " 0.8564931783729156,\n",
       " 0.8585144012127337,\n",
       " 0.8524507326932794,\n",
       " 0.8554825669530066,\n",
       " 0.8580090955027792,\n",
       " 0.8569984840828702,\n",
       " 0.8575037897928247,\n",
       " 0.8514401212733704,\n",
       " 0.8534613441131885,\n",
       " 0.8590197069226883,\n",
       " 0.8600303183425972,\n",
       " 0.8514401212733704,\n",
       " 0.8615462354724608,\n",
       " 0.8549772612430521,\n",
       " 0.8569984840828702,\n",
       " 0.8605356240525518,\n",
       " 0.8595250126326428,\n",
       " 0.8600303183425972,\n",
       " 0.8564931783729156,\n",
       " 0.8575037897928247,\n",
       " 0.8625568468923699,\n",
       " 0.8585144012127337,\n",
       " 0.8580090955027792,\n",
       " 0.8620515411824153,\n",
       " 0.8575037897928247,\n",
       " 0.8600303183425972,\n",
       " 0.8569984840828702,\n",
       " 0.8600303183425972,\n",
       " 0.8600303183425972,\n",
       " 0.865588681152097,\n",
       " 0.8595250126326428,\n",
       " 0.8580090955027792,\n",
       " 0.8650833754421425,\n",
       " 0.8625568468923699,\n",
       " 0.865588681152097,\n",
       " 0.8620515411824153,\n",
       " 0.8640727640222334,\n",
       " 0.8595250126326428,\n",
       " 0.8509348155634159,\n",
       " 0.865588681152097,\n",
       " 0.865588681152097,\n",
       " 0.864578069732188,\n",
       " 0.8660939868620515,\n",
       " 0.864578069732188,\n",
       " 0.8650833754421425,\n",
       " 0.8630621526023244,\n",
       " 0.8630621526023244,\n",
       " 0.8504295098534613,\n",
       " 0.8660939868620515,\n",
       " 0.864578069732188,\n",
       " 0.8630621526023244,\n",
       " 0.8650833754421425,\n",
       " 0.8600303183425972,\n",
       " 0.865588681152097,\n",
       " 0.865588681152097,\n",
       " 0.865588681152097,\n",
       " 0.864578069732188,\n",
       " 0.8610409297625063,\n",
       " 0.8671045982819606,\n",
       " 0.864578069732188,\n",
       " 0.865588681152097,\n",
       " 0.8676099039919151,\n",
       " 0.8630621526023244,\n",
       " 0.8676099039919151,\n",
       " 0.865588681152097,\n",
       " 0.8660939868620515,\n",
       " 0.8625568468923699,\n",
       " 0.8569984840828702,\n",
       " 0.865588681152097,\n",
       " 0.8686205154118242,\n",
       " 0.8691258211217787,\n",
       " 0.8605356240525518,\n",
       " 0.8696311268317332,\n",
       " 0.8676099039919151,\n",
       " 0.8615462354724608,\n",
       " 0.8691258211217787,\n",
       " 0.8716523496715513,\n",
       " 0.864578069732188,\n",
       " 0.8671045982819606,\n",
       " 0.864578069732188,\n",
       " 0.8650833754421425,\n",
       " 0.8681152097018696,\n",
       " 0.8711470439615968,\n",
       " 0.864578069732188,\n",
       " 0.8716523496715513,\n",
       " 0.864578069732188,\n",
       " 0.8701364325416877,\n",
       " 0.8686205154118242,\n",
       " 0.8665992925720061,\n",
       " 0.8691258211217787,\n",
       " 0.8671045982819606,\n",
       " 0.865588681152097,\n",
       " 0.8650833754421425,\n",
       " 0.8746841839312784,\n",
       " 0.8741788782213239,\n",
       " 0.8660939868620515,\n",
       " 0.8676099039919151,\n",
       " 0.8706417382516423,\n",
       " 0.8686205154118242,\n",
       " 0.8660939868620515,\n",
       " 0.8721576553815058,\n",
       " 0.8726629610914604,\n",
       " 0.8691258211217787,\n",
       " 0.8701364325416877,\n",
       " 0.8681152097018696,\n",
       " 0.8746841839312784,\n",
       " 0.8716523496715513,\n",
       " 0.8686205154118242,\n",
       " 0.8706417382516423,\n",
       " 0.8681152097018696,\n",
       " 0.8711470439615968,\n",
       " 0.8746841839312784,\n",
       " 0.8746841839312784,\n",
       " 0.8721576553815058,\n",
       " 0.8706417382516423,\n",
       " 0.8711470439615968,\n",
       " 0.8665992925720061,\n",
       " 0.8731682668014149,\n",
       " 0.8701364325416877,\n",
       " 0.8767054067710965,\n",
       " 0.8635674583122789,\n",
       " 0.8767054067710965,\n",
       " 0.8756947953511874,\n",
       " 0.8681152097018696,\n",
       " 0.876200101061142,\n",
       " 0.875189489641233,\n",
       " 0.8716523496715513,\n",
       " 0.875189489641233,\n",
       " 0.877210712481051,\n",
       " 0.8716523496715513,\n",
       " 0.8736735725113693,\n",
       " 0.8716523496715513,\n",
       " 0.8767054067710965,\n",
       " 0.8721576553815058,\n",
       " 0.8756947953511874,\n",
       " 0.8746841839312784,\n",
       " 0.8777160181910055,\n",
       " 0.8716523496715513,\n",
       " 0.8777160181910055,\n",
       " 0.8746841839312784,\n",
       " 0.875189489641233,\n",
       " 0.8741788782213239,\n",
       " 0.8731682668014149,\n",
       " 0.8777160181910055,\n",
       " 0.877210712481051,\n",
       " 0.877210712481051,\n",
       " 0.875189489641233,\n",
       " 0.8756947953511874,\n",
       " 0.8756947953511874,\n",
       " 0.8777160181910055,\n",
       " 0.877210712481051,\n",
       " 0.877210712481051,\n",
       " 0.8711470439615968,\n",
       " 0.877210712481051,\n",
       " 0.8777160181910055,\n",
       " 0.875189489641233]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = net.accuracy_mat\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca30c31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs number of epochs on validation data for Neural Networks with 2 layers')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy)\n",
    "plt.xlabel('# of epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accuracy vs number of epochs on validation data for Neural Networks with 2 layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c61dbc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Epoch 4 complete\n",
      "Epoch 5 complete\n",
      "Epoch 6 complete\n",
      "Epoch 7 complete\n",
      "Epoch 8 complete\n",
      "Epoch 9 complete\n",
      "Epoch 10 complete\n",
      "Epoch 11 complete\n",
      "Epoch 12 complete\n",
      "Epoch 13 complete\n",
      "Epoch 14 complete\n",
      "Epoch 15 complete\n",
      "Epoch 16 complete\n",
      "Epoch 17 complete\n",
      "Epoch 18 complete\n",
      "Epoch 19 complete\n",
      "Epoch 20 complete\n",
      "Epoch 21 complete\n",
      "Epoch 22 complete\n",
      "Epoch 23 complete\n",
      "Epoch 24 complete\n",
      "Epoch 25 complete\n",
      "Epoch 26 complete\n",
      "Epoch 27 complete\n",
      "Epoch 28 complete\n",
      "Epoch 29 complete\n",
      "Epoch 30 complete\n",
      "Epoch 31 complete\n",
      "Epoch 32 complete\n",
      "Epoch 33 complete\n",
      "Epoch 34 complete\n",
      "Epoch 35 complete\n",
      "Epoch 36 complete\n",
      "Epoch 37 complete\n",
      "Epoch 38 complete\n",
      "Epoch 39 complete\n",
      "Epoch 40 complete\n",
      "Epoch 41 complete\n",
      "Epoch 42 complete\n",
      "Epoch 43 complete\n",
      "Epoch 44 complete\n",
      "Epoch 45 complete\n",
      "Epoch 46 complete\n",
      "Epoch 47 complete\n",
      "Epoch 48 complete\n",
      "Epoch 49 complete\n",
      "Epoch 50 complete\n",
      "Epoch 51 complete\n",
      "Epoch 52 complete\n",
      "Epoch 53 complete\n",
      "Epoch 54 complete\n",
      "Epoch 55 complete\n",
      "Epoch 56 complete\n",
      "Epoch 57 complete\n",
      "Epoch 58 complete\n",
      "Epoch 59 complete\n",
      "Epoch 60 complete\n",
      "Epoch 61 complete\n",
      "Epoch 62 complete\n",
      "Epoch 63 complete\n",
      "Epoch 64 complete\n",
      "Epoch 65 complete\n",
      "Epoch 66 complete\n",
      "Epoch 67 complete\n",
      "Epoch 68 complete\n",
      "Epoch 69 complete\n",
      "Epoch 70 complete\n",
      "Epoch 71 complete\n",
      "Epoch 72 complete\n",
      "Epoch 73 complete\n",
      "Epoch 74 complete\n",
      "Epoch 75 complete\n",
      "Epoch 76 complete\n",
      "Epoch 77 complete\n",
      "Epoch 78 complete\n",
      "Epoch 79 complete\n",
      "Epoch 80 complete\n",
      "Epoch 81 complete\n",
      "Epoch 82 complete\n",
      "Epoch 83 complete\n",
      "Epoch 84 complete\n",
      "Epoch 85 complete\n",
      "Epoch 86 complete\n",
      "Epoch 87 complete\n",
      "Epoch 88 complete\n",
      "Epoch 89 complete\n",
      "Epoch 90 complete\n",
      "Epoch 91 complete\n",
      "Epoch 92 complete\n",
      "Epoch 93 complete\n",
      "Epoch 94 complete\n",
      "Epoch 95 complete\n",
      "Epoch 96 complete\n",
      "Epoch 97 complete\n",
      "Epoch 98 complete\n",
      "Epoch 99 complete\n",
      "Epoch 100 complete\n",
      "Epoch 101 complete\n",
      "Epoch 102 complete\n",
      "Epoch 103 complete\n",
      "Epoch 104 complete\n",
      "Epoch 105 complete\n",
      "Epoch 106 complete\n",
      "Epoch 107 complete\n",
      "Epoch 108 complete\n",
      "Epoch 109 complete\n",
      "Epoch 110 complete\n",
      "Epoch 111 complete\n",
      "Epoch 112 complete\n",
      "Epoch 113 complete\n",
      "Epoch 114 complete\n",
      "Epoch 115 complete\n",
      "Epoch 116 complete\n",
      "Epoch 117 complete\n",
      "Epoch 118 complete\n",
      "Epoch 119 complete\n",
      "Epoch 120 complete\n",
      "Epoch 121 complete\n",
      "Epoch 122 complete\n",
      "Epoch 123 complete\n",
      "Epoch 124 complete\n",
      "Epoch 125 complete\n",
      "Epoch 126 complete\n",
      "Epoch 127 complete\n",
      "Epoch 128 complete\n",
      "Epoch 129 complete\n",
      "Epoch 130 complete\n",
      "Epoch 131 complete\n",
      "Epoch 132 complete\n",
      "Epoch 133 complete\n",
      "Epoch 134 complete\n",
      "Epoch 135 complete\n",
      "Epoch 136 complete\n",
      "Epoch 137 complete\n",
      "Epoch 138 complete\n",
      "Epoch 139 complete\n",
      "Epoch 140 complete\n",
      "Epoch 141 complete\n",
      "Epoch 142 complete\n",
      "Epoch 143 complete\n",
      "Epoch 144 complete\n",
      "Epoch 145 complete\n",
      "Epoch 146 complete\n",
      "Epoch 147 complete\n",
      "Epoch 148 complete\n",
      "Epoch 149 complete\n",
      "Epoch 150 complete\n",
      "Epoch 151 complete\n",
      "Epoch 152 complete\n",
      "Epoch 153 complete\n",
      "Epoch 154 complete\n",
      "Epoch 155 complete\n",
      "Epoch 156 complete\n",
      "Epoch 157 complete\n",
      "Epoch 158 complete\n",
      "Epoch 159 complete\n",
      "Epoch 160 complete\n",
      "Epoch 161 complete\n",
      "Epoch 162 complete\n",
      "Epoch 163 complete\n",
      "Epoch 164 complete\n",
      "Epoch 165 complete\n",
      "Epoch 166 complete\n",
      "Epoch 167 complete\n",
      "Epoch 168 complete\n",
      "Epoch 169 complete\n",
      "Epoch 170 complete\n",
      "Epoch 171 complete\n",
      "Epoch 172 complete\n",
      "Epoch 173 complete\n",
      "Epoch 174 complete\n",
      "Epoch 175 complete\n",
      "Epoch 176 complete\n",
      "Epoch 177 complete\n",
      "Epoch 178 complete\n",
      "Epoch 179 complete\n",
      "Epoch 180 complete\n",
      "Epoch 181 complete\n",
      "Epoch 182 complete\n",
      "Epoch 183 complete\n",
      "Epoch 184 complete\n",
      "Epoch 185 complete\n",
      "Epoch 186 complete\n",
      "Epoch 187 complete\n",
      "Epoch 188 complete\n",
      "Epoch 189 complete\n",
      "Epoch 190 complete\n",
      "Epoch 191 complete\n",
      "Epoch 192 complete\n",
      "Epoch 193 complete\n",
      "Epoch 194 complete\n",
      "Epoch 195 complete\n",
      "Epoch 196 complete\n",
      "Epoch 197 complete\n",
      "Epoch 198 complete\n",
      "Epoch 199 complete\n",
      "Epoch 200 complete\n",
      "Epoch 201 complete\n",
      "Epoch 202 complete\n",
      "Epoch 203 complete\n",
      "Epoch 204 complete\n",
      "Epoch 205 complete\n",
      "Epoch 206 complete\n",
      "Epoch 207 complete\n",
      "Epoch 208 complete\n",
      "Epoch 209 complete\n",
      "Epoch 210 complete\n",
      "Epoch 211 complete\n",
      "Epoch 212 complete\n",
      "Epoch 213 complete\n",
      "Epoch 214 complete\n",
      "Epoch 215 complete\n",
      "Epoch 216 complete\n",
      "Epoch 217 complete\n",
      "Epoch 218 complete\n",
      "Epoch 219 complete\n",
      "Epoch 220 complete\n",
      "Epoch 221 complete\n",
      "Epoch 222 complete\n",
      "Epoch 223 complete\n",
      "Epoch 224 complete\n",
      "Epoch 225 complete\n",
      "Epoch 226 complete\n",
      "Epoch 227 complete\n",
      "Epoch 228 complete\n",
      "Epoch 229 complete\n",
      "Epoch 230 complete\n",
      "Epoch 231 complete\n",
      "Epoch 232 complete\n",
      "Epoch 233 complete\n",
      "Epoch 234 complete\n",
      "Epoch 235 complete\n",
      "Epoch 236 complete\n",
      "Epoch 237 complete\n",
      "Epoch 238 complete\n",
      "Epoch 239 complete\n",
      "Epoch 240 complete\n",
      "Epoch 241 complete\n",
      "Epoch 242 complete\n",
      "Epoch 243 complete\n",
      "Epoch 244 complete\n",
      "Epoch 245 complete\n",
      "Epoch 246 complete\n",
      "Epoch 247 complete\n",
      "Epoch 248 complete\n",
      "Epoch 249 complete\n",
      "Epoch 250 complete\n",
      "Epoch 251 complete\n",
      "Epoch 252 complete\n",
      "Epoch 253 complete\n",
      "Epoch 254 complete\n",
      "Epoch 255 complete\n",
      "Epoch 256 complete\n",
      "Epoch 257 complete\n",
      "Epoch 258 complete\n",
      "Epoch 259 complete\n",
      "Epoch 260 complete\n",
      "Epoch 261 complete\n",
      "Epoch 262 complete\n",
      "Epoch 263 complete\n",
      "Epoch 264 complete\n",
      "Epoch 265 complete\n",
      "Epoch 266 complete\n",
      "Epoch 267 complete\n",
      "Epoch 268 complete\n",
      "Epoch 269 complete\n",
      "Epoch 270 complete\n",
      "Epoch 271 complete\n",
      "Epoch 272 complete\n",
      "Epoch 273 complete\n",
      "Epoch 274 complete\n",
      "Epoch 275 complete\n",
      "Epoch 276 complete\n",
      "Epoch 277 complete\n",
      "Epoch 278 complete\n",
      "Epoch 279 complete\n",
      "Epoch 280 complete\n",
      "Epoch 281 complete\n",
      "Epoch 282 complete\n",
      "Epoch 283 complete\n",
      "Epoch 284 complete\n",
      "Epoch 285 complete\n",
      "Epoch 286 complete\n",
      "Epoch 287 complete\n",
      "Epoch 288 complete\n",
      "Epoch 289 complete\n",
      "Epoch 290 complete\n",
      "Epoch 291 complete\n",
      "Epoch 292 complete\n",
      "Epoch 293 complete\n",
      "Epoch 294 complete\n",
      "Epoch 295 complete\n",
      "Epoch 296 complete\n",
      "Epoch 297 complete\n",
      "Epoch 298 complete\n",
      "Epoch 299 complete\n"
     ]
    }
   ],
   "source": [
    "net_2 = Neural_Network([X_vector_train.shape[1], 200, 2])\n",
    "net_2.SGD(training_data, 300, 200, 0.5, test_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5169bf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_2layer = net_2.accuracy_mat\n",
    "accuracy_2layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7ca412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs number of epochs on validation data for Neural Networks with 1 layer')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_2layer)\n",
    "plt.xlabel('# of epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accuracy vs number of epochs on validation data for Neural Networks with 1 layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17fc47",
   "metadata": {},
   "source": [
    "<h3> prediction on test data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef979af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(r'C:\\Users\\Bhavesh Kilaru\\Desktop\\ML\\project2\\code and files\\data files\\twitter_validation.csv')\n",
    "df_test.columns = ['id', 'topic', 'polarity', 'tweet']\n",
    "df_test = df_test.loc[df_test['polarity'].isin(['Positive','Negative'])]\n",
    "df_test = df_test.dropna()\n",
    "df_test = df_test.loc[df_test['topic'].isin(['Amazon', 'Facebook', 'Xbox(Xseries)','Nvidia','Google','Microsoft', 'FIFA', 'HomeDepot'])]#.groupby('topic').count()\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad47b9",
   "metadata": {},
   "source": [
    "<body> label encoding for test data labels.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e592501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>pay WORD functions poorly Chromebook</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "      <td>Hi Ive Madeleine McCann cellar past years litt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9135</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Congrats to the NVIDIA NeMo team for the 1.0.0...</td>\n",
       "      <td>Congrats NeMo team release candidate Really ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8056</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>What does that say about Microsoft hardware &amp; ...</td>\n",
       "      <td>say hardware software security Man gets hacked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8857</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Watching NVIDIA position itself as not just a ...</td>\n",
       "      <td>Watching position leading hardware manufacture...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>8318</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Why didn’t anyone think of this acronym for BL...</td>\n",
       "      <td>didnt anyone think acronym BLM Batteries Lives...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>397</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Positive</td>\n",
       "      <td>#Amazon Best Seller!\\n\\nScreen Cleaner Kit - B...</td>\n",
       "      <td>Amazon Best Seller Screen Cleaner Kit Best LED...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>5708</td>\n",
       "      <td>HomeDepot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Thank you to Matching funds Home Depot RW paym...</td>\n",
       "      <td>Thank Matching funds Home Depot RW payment gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>314</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Please explain how this is possible! How can t...</td>\n",
       "      <td>Please explain possible let companies overchar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "      <td>Bought fraction today Small wins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      topic  polarity  \\\n",
       "1    8312  Microsoft  Negative   \n",
       "4    6273       FIFA  Negative   \n",
       "14   9135     Nvidia  Positive   \n",
       "18   8056  Microsoft  Negative   \n",
       "29   8857     Nvidia  Positive   \n",
       "..    ...        ...       ...   \n",
       "970  8318  Microsoft  Negative   \n",
       "973   397     Amazon  Positive   \n",
       "988  5708  HomeDepot  Positive   \n",
       "992   314     Amazon  Negative   \n",
       "997  8069  Microsoft  Positive   \n",
       "\n",
       "                                                 tweet  \\\n",
       "1    @Microsoft Why do I pay for WORD when it funct...   \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...   \n",
       "14   Congrats to the NVIDIA NeMo team for the 1.0.0...   \n",
       "18   What does that say about Microsoft hardware & ...   \n",
       "29   Watching NVIDIA position itself as not just a ...   \n",
       "..                                                 ...   \n",
       "970  Why didn’t anyone think of this acronym for BL...   \n",
       "973  #Amazon Best Seller!\\n\\nScreen Cleaner Kit - B...   \n",
       "988  Thank you to Matching funds Home Depot RW paym...   \n",
       "992  Please explain how this is possible! How can t...   \n",
       "997  Bought a fraction of Microsoft today. Small wins.   \n",
       "\n",
       "                                          unique_words  Sentiment  \n",
       "1                 pay WORD functions poorly Chromebook          0  \n",
       "4    Hi Ive Madeleine McCann cellar past years litt...          0  \n",
       "14   Congrats NeMo team release candidate Really ex...          1  \n",
       "18      say hardware software security Man gets hacked          0  \n",
       "29   Watching position leading hardware manufacture...          1  \n",
       "..                                                 ...        ...  \n",
       "970  didnt anyone think acronym BLM Batteries Lives...          0  \n",
       "973  Amazon Best Seller Screen Cleaner Kit Best LED...          1  \n",
       "988  Thank Matching funds Home Depot RW payment gen...          1  \n",
       "992  Please explain possible let companies overchar...          0  \n",
       "997                   Bought fraction today Small wins          1  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col = []\n",
    "for t in df_test['tweet']:\n",
    "    new_col.append(process_tweet(t))\n",
    "    \n",
    "df_test['unique_words'] = new_col\n",
    "\n",
    "temp = [] \n",
    "for i in df_test['polarity']:\n",
    "    if i == 'Positive':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "\n",
    "        \n",
    "df_test['Sentiment'] = temp\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfefe7",
   "metadata": {},
   "source": [
    "<body> removing the words that occured less than once in test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36756bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"unique_words\"] = df_test[\"unique_words\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\n",
    "X_test_df = df_test['unique_words']\n",
    "y_test_df = df_test['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b146e83",
   "metadata": {},
   "source": [
    "<body>converting the entire train data and validation data into numerical features and transforming the test data into numerical features.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9417d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_test = TfidfVectorizer(min_df = 7)\n",
    "Final_Data = vectorizer_test.fit_transform(X)\n",
    "X_vector_test= vectorizer_test.transform(X_test_df)\n",
    "x_train = Final_Data.toarray()\n",
    "x_test = X_vector_test.toarray()\n",
    "\n",
    "x_train_df_new = np.insert(x_train, 0, 1, axis = 1)\n",
    "x_test_df_new = np.insert(x_test, 0, 1, axis = 1)\n",
    "\n",
    "x_train = x_train_df_new\n",
    "x_test  = x_test_df_new\n",
    "\n",
    "y_train = pd.Series(y).array\n",
    "y_test = pd.Series(y_test_df).array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212acfbd",
   "metadata": {},
   "source": [
    "<body> training the perceptron on entire train and validation data, then checking its accuracy on test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b51d07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score for eta = 0.1 and n = 30 is 0.9166666666666666\n",
      "the confusion matrix for eta = 0.1 and n = 30 is [[65  0]\n",
      " [10 45]]\n"
     ]
    }
   ],
   "source": [
    "final_perceptron  = np.zeros((x_train.shape[1], 1))\n",
    "final_perceptron = perceptron(x_train, y_train, final_perceptron, 0.1,30)\n",
    "\n",
    "pred_test_y = []\n",
    "for i in range(len(x_test)):\n",
    "    val = np.dot(x_test[i], final_perceptron)\n",
    "\n",
    "    if val > 0:\n",
    "        pred_test_y.append(1)\n",
    "    else:\n",
    "        pred_test_y.append(0)\n",
    "        \n",
    "p_s = accuracy_score(y_test, pred_test_y)\n",
    "C_m = confusion_matrix(y_test,pred_test_y)\n",
    "print(f\"the accuracy score for eta = 0.1 and n = 30 is {p_s}\")\n",
    "print(f\"the confusion matrix for eta = 0.1 and n = 30 is {C_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31735f",
   "metadata": {},
   "source": [
    "<body> training the logistic regression model on entire train and validation data, then checking its accuracy on test data.</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46e3aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 epochs, Loss = 0.7123941009823659\n",
      "After 20 epochs, Loss = 0.7017669381789405\n",
      "After 30 epochs, Loss = 0.6934044239551864\n",
      "After 40 epochs, Loss = 0.6863446656848504\n",
      "After 50 epochs, Loss = 0.6800517192355431\n",
      "After 60 epochs, Loss = 0.6742306598713823\n",
      "After 70 epochs, Loss = 0.6687180759861779\n",
      "After 80 epochs, Loss = 0.6634221230645051\n",
      "After 90 epochs, Loss = 0.6582904798616883\n",
      "After 100 epochs, Loss = 0.6532930603637929\n",
      "After 110 epochs, Loss = 0.6484122535472623\n",
      "After 120 epochs, Loss = 0.6436377651132192\n",
      "After 130 epochs, Loss = 0.6389633404366314\n",
      "After 140 epochs, Loss = 0.634385119385512\n",
      "After 150 epochs, Loss = 0.6299007691262519\n",
      "After 160 epochs, Loss = 0.6255085086114144\n",
      "After 170 epochs, Loss = 0.6212067890276658\n",
      "After 180 epochs, Loss = 0.6169943648587661\n",
      "After 190 epochs, Loss = 0.6128700905646912\n",
      "After 200 epochs, Loss = 0.6088326420135717\n",
      "After 210 epochs, Loss = 0.6048806322186883\n",
      "After 220 epochs, Loss = 0.6010126659468128\n",
      "After 230 epochs, Loss = 0.5972273136465402\n",
      "After 240 epochs, Loss = 0.5935229849949796\n",
      "After 250 epochs, Loss = 0.589898210593352\n",
      "After 260 epochs, Loss = 0.586351331070001\n",
      "After 270 epochs, Loss = 0.5828806438774939\n",
      "After 280 epochs, Loss = 0.5794844529942021\n",
      "After 290 epochs, Loss = 0.5761610183747873\n",
      "After 300 epochs, Loss = 0.5729086903100296\n",
      "After 310 epochs, Loss = 0.5697257222174655\n",
      "After 320 epochs, Loss = 0.5666104225588322\n",
      "After 330 epochs, Loss = 0.5635610982202477\n",
      "After 340 epochs, Loss = 0.560576038917922\n",
      "After 350 epochs, Loss = 0.5576536647434515\n",
      "After 360 epochs, Loss = 0.5547923052485783\n",
      "After 370 epochs, Loss = 0.5519903115192568\n",
      "After 380 epochs, Loss = 0.5492461201772739\n",
      "After 390 epochs, Loss = 0.5465582088049925\n",
      "After 400 epochs, Loss = 0.543925154650761\n",
      "After 410 epochs, Loss = 0.5413454799557484\n",
      "After 420 epochs, Loss = 0.5388176493983102\n",
      "After 430 epochs, Loss = 0.5363402761835877\n",
      "After 440 epochs, Loss = 0.5339119898446906\n",
      "After 450 epochs, Loss = 0.5315314513318763\n",
      "After 460 epochs, Loss = 0.5291973985981762\n",
      "After 470 epochs, Loss = 0.5269086197805046\n",
      "After 480 epochs, Loss = 0.5246638378293659\n",
      "After 490 epochs, Loss = 0.5224618487521099\n",
      "After 500 epochs, Loss = 0.5203014953540177\n",
      "After 510 epochs, Loss = 0.518181696562996\n",
      "After 520 epochs, Loss = 0.5161013762605393\n",
      "After 530 epochs, Loss = 0.5140594504227144\n",
      "After 540 epochs, Loss = 0.5120548578297953\n",
      "After 550 epochs, Loss = 0.5100866390921699\n",
      "After 560 epochs, Loss = 0.5081539220877948\n",
      "After 570 epochs, Loss = 0.5062557357400894\n",
      "After 580 epochs, Loss = 0.5043911135864618\n",
      "After 590 epochs, Loss = 0.5025591289713439\n",
      "After 600 epochs, Loss = 0.5007589257805185\n",
      "After 610 epochs, Loss = 0.4989896737056964\n",
      "After 620 epochs, Loss = 0.49725058128473815\n",
      "After 630 epochs, Loss = 0.4955408622279812\n",
      "After 640 epochs, Loss = 0.4938597970512367\n",
      "After 650 epochs, Loss = 0.4922066079813101\n",
      "After 660 epochs, Loss = 0.49058061753683574\n",
      "After 670 epochs, Loss = 0.4889811986657529\n",
      "After 680 epochs, Loss = 0.48740760946814987\n",
      "After 690 epochs, Loss = 0.48585918843266174\n",
      "After 700 epochs, Loss = 0.48433529721403257\n",
      "After 710 epochs, Loss = 0.4828353372625978\n",
      "After 720 epochs, Loss = 0.48135869816749105\n",
      "After 730 epochs, Loss = 0.47990480579058914\n",
      "After 740 epochs, Loss = 0.4784731206018944\n",
      "After 750 epochs, Loss = 0.4770631077896515\n",
      "After 760 epochs, Loss = 0.4756742430329883\n",
      "After 770 epochs, Loss = 0.4743059858094196\n",
      "After 780 epochs, Loss = 0.47295785648892114\n",
      "After 790 epochs, Loss = 0.4716293767898249\n",
      "After 800 epochs, Loss = 0.47032008635016287\n",
      "After 810 epochs, Loss = 0.46902957761134856\n",
      "After 820 epochs, Loss = 0.46775736750631053\n",
      "After 830 epochs, Loss = 0.4665030251323616\n",
      "After 840 epochs, Loss = 0.46526613647550036\n",
      "After 850 epochs, Loss = 0.46404630414602765\n",
      "After 860 epochs, Loss = 0.4628431201225089\n",
      "After 870 epochs, Loss = 0.4616562285059549\n",
      "After 880 epochs, Loss = 0.4604852378483602\n",
      "After 890 epochs, Loss = 0.4593297908608827\n",
      "After 900 epochs, Loss = 0.4581895518191501\n",
      "After 910 epochs, Loss = 0.45706419861372877\n",
      "After 920 epochs, Loss = 0.45595338127102136\n",
      "After 930 epochs, Loss = 0.454856788301068\n",
      "After 940 epochs, Loss = 0.45377413275318795\n",
      "After 950 epochs, Loss = 0.4527050941900972\n",
      "After 960 epochs, Loss = 0.4516493975803043\n",
      "After 970 epochs, Loss = 0.4506067408363067\n",
      "After 980 epochs, Loss = 0.4495768486425922\n",
      "After 990 epochs, Loss = 0.44855944558560223\n",
      "After 1000 epochs, Loss = 0.4475542671428544\n",
      "After 1010 epochs, Loss = 0.4465610879059234\n",
      "After 1020 epochs, Loss = 0.44557964548211837\n",
      "After 1030 epochs, Loss = 0.44460968713062726\n",
      "After 1040 epochs, Loss = 0.4436510388846274\n",
      "After 1050 epochs, Loss = 0.44270346753889056\n",
      "After 1060 epochs, Loss = 0.441766706337661\n",
      "After 1070 epochs, Loss = 0.4408405352534301\n",
      "After 1080 epochs, Loss = 0.43992474469160825\n",
      "After 1090 epochs, Loss = 0.4390191274294281\n",
      "After 1100 epochs, Loss = 0.4381234849165616\n",
      "After 1110 epochs, Loss = 0.4372376410285593\n",
      "After 1120 epochs, Loss = 0.436361403243301\n",
      "After 1130 epochs, Loss = 0.43549458142216996\n",
      "After 1140 epochs, Loss = 0.43463699096901276\n",
      "After 1150 epochs, Loss = 0.4337884579103352\n",
      "After 1160 epochs, Loss = 0.4329488127489274\n",
      "After 1170 epochs, Loss = 0.43211789032831854\n",
      "After 1180 epochs, Loss = 0.431295529701681\n",
      "After 1190 epochs, Loss = 0.43048157400419557\n",
      "After 1200 epochs, Loss = 0.4296758787857381\n",
      "After 1210 epochs, Loss = 0.4288782886046254\n",
      "After 1220 epochs, Loss = 0.4280886561023309\n",
      "After 1230 epochs, Loss = 0.4273068395616625\n",
      "After 1240 epochs, Loss = 0.42653270077360256\n",
      "After 1250 epochs, Loss = 0.4257661049351784\n",
      "After 1260 epochs, Loss = 0.4250069205483845\n",
      "After 1270 epochs, Loss = 0.4242550577798791\n",
      "After 1280 epochs, Loss = 0.4235103847709407\n",
      "After 1290 epochs, Loss = 0.42277274747722776\n",
      "After 1300 epochs, Loss = 0.4220420325015453\n",
      "After 1310 epochs, Loss = 0.4213181183085193\n",
      "After 1320 epochs, Loss = 0.4206008906661101\n",
      "After 1330 epochs, Loss = 0.4198902386589592\n",
      "After 1340 epochs, Loss = 0.4191860539817822\n",
      "After 1350 epochs, Loss = 0.4184882376298646\n",
      "After 1360 epochs, Loss = 0.4177966856236631\n",
      "After 1370 epochs, Loss = 0.4171112955437226\n",
      "After 1380 epochs, Loss = 0.4164319745051467\n",
      "After 1390 epochs, Loss = 0.4157586188180595\n",
      "After 1400 epochs, Loss = 0.41509113918805496\n",
      "After 1410 epochs, Loss = 0.4144294610740334\n",
      "After 1420 epochs, Loss = 0.41377348099483646\n",
      "After 1430 epochs, Loss = 0.4131230992592948\n",
      "After 1440 epochs, Loss = 0.4124782506917781\n",
      "After 1450 epochs, Loss = 0.41183884937757964\n",
      "After 1460 epochs, Loss = 0.4112047941916917\n",
      "After 1470 epochs, Loss = 0.4105760060847771\n",
      "After 1480 epochs, Loss = 0.40995244152001836\n",
      "After 1490 epochs, Loss = 0.4093339966795706\n",
      "After 1500 epochs, Loss = 0.4087205982002776\n",
      "After 1510 epochs, Loss = 0.40811217383602505\n",
      "After 1520 epochs, Loss = 0.40750865497276495\n",
      "After 1530 epochs, Loss = 0.40690997116120187\n",
      "After 1540 epochs, Loss = 0.4063160386442311\n",
      "After 1550 epochs, Loss = 0.4057267883422833\n",
      "After 1560 epochs, Loss = 0.4051421649843739\n",
      "After 1570 epochs, Loss = 0.4045620930472335\n",
      "After 1580 epochs, Loss = 0.40398651308422157\n",
      "After 1590 epochs, Loss = 0.4034153691504603\n",
      "After 1600 epochs, Loss = 0.40284859138739404\n",
      "After 1610 epochs, Loss = 0.40228611915611096\n",
      "After 1620 epochs, Loss = 0.4017279178004703\n",
      "After 1630 epochs, Loss = 0.40117390788759705\n",
      "After 1640 epochs, Loss = 0.4006240348841119\n",
      "After 1650 epochs, Loss = 0.4000782397124415\n",
      "After 1660 epochs, Loss = 0.3995364684301619\n",
      "After 1670 epochs, Loss = 0.39899867297798164\n",
      "After 1680 epochs, Loss = 0.39846480178403654\n",
      "After 1690 epochs, Loss = 0.3979347989486648\n",
      "After 1700 epochs, Loss = 0.3974086158441801\n",
      "After 1710 epochs, Loss = 0.3968862038859955\n",
      "After 1720 epochs, Loss = 0.3963675310436007\n",
      "After 1730 epochs, Loss = 0.3958525359860588\n",
      "After 1740 epochs, Loss = 0.39534117953867803\n",
      "After 1750 epochs, Loss = 0.394833413517823\n",
      "After 1760 epochs, Loss = 0.3943291861545347\n",
      "After 1770 epochs, Loss = 0.3938284538029821\n",
      "After 1780 epochs, Loss = 0.3933311803022498\n",
      "After 1790 epochs, Loss = 0.3928373197497936\n",
      "After 1800 epochs, Loss = 0.39234683574455625\n",
      "After 1810 epochs, Loss = 0.3918596831849739\n",
      "After 1820 epochs, Loss = 0.3913758191123981\n",
      "After 1830 epochs, Loss = 0.3908952044267937\n",
      "After 1840 epochs, Loss = 0.3904178007249106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1850 epochs, Loss = 0.3899435702829643\n",
      "After 1860 epochs, Loss = 0.3894724780734383\n",
      "After 1870 epochs, Loss = 0.38900448605381366\n",
      "After 1880 epochs, Loss = 0.38853955800407625\n",
      "After 1890 epochs, Loss = 0.38807766006001965\n",
      "After 1900 epochs, Loss = 0.38761876921989796\n",
      "After 1910 epochs, Loss = 0.38716283865585277\n",
      "After 1920 epochs, Loss = 0.38670983499323364\n",
      "After 1930 epochs, Loss = 0.38625972542260156\n",
      "After 1940 epochs, Loss = 0.38581248780967053\n",
      "After 1950 epochs, Loss = 0.38536813470726383\n",
      "After 1960 epochs, Loss = 0.3849266017144179\n",
      "After 1970 epochs, Loss = 0.38448783686707383\n",
      "After 1980 epochs, Loss = 0.38405180999628125\n",
      "After 1990 epochs, Loss = 0.383618491427959\n",
      "After 2000 epochs, Loss = 0.3831878519714093\n",
      "After 2010 epochs, Loss = 0.382759871200485\n",
      "After 2020 epochs, Loss = 0.38233452995962625\n",
      "After 2030 epochs, Loss = 0.38191180317471307\n",
      "After 2040 epochs, Loss = 0.3814916608270477\n",
      "After 2050 epochs, Loss = 0.38107406439321095\n",
      "After 2060 epochs, Loss = 0.3806589932585456\n",
      "After 2070 epochs, Loss = 0.3802464455151805\n",
      "After 2080 epochs, Loss = 0.37983637247574087\n",
      "After 2090 epochs, Loss = 0.379428778369596\n",
      "After 2100 epochs, Loss = 0.3790236079251106\n",
      "After 2110 epochs, Loss = 0.37862082452147994\n",
      "After 2120 epochs, Loss = 0.37822040402299467\n",
      "After 2130 epochs, Loss = 0.37782232265993365\n",
      "After 2140 epochs, Loss = 0.37742655863827956\n",
      "After 2150 epochs, Loss = 0.37703309338499935\n",
      "After 2160 epochs, Loss = 0.3766418995262686\n",
      "After 2170 epochs, Loss = 0.3762529532590164\n",
      "After 2180 epochs, Loss = 0.3758662325348182\n",
      "After 2190 epochs, Loss = 0.37548171562902904\n",
      "After 2200 epochs, Loss = 0.37509938451441593\n",
      "After 2210 epochs, Loss = 0.3747192153647837\n",
      "After 2220 epochs, Loss = 0.37434121551863486\n",
      "After 2230 epochs, Loss = 0.37396534555730027\n",
      "After 2240 epochs, Loss = 0.3735915754213591\n",
      "After 2250 epochs, Loss = 0.37321988519859983\n",
      "After 2260 epochs, Loss = 0.37285025525884685\n",
      "After 2270 epochs, Loss = 0.37248266624854387\n",
      "After 2280 epochs, Loss = 0.37211709908546226\n",
      "After 2290 epochs, Loss = 0.37175353495354274\n",
      "After 2300 epochs, Loss = 0.3713919552978512\n",
      "After 2310 epochs, Loss = 0.3710323418196574\n",
      "After 2320 epochs, Loss = 0.370674676471629\n",
      "After 2330 epochs, Loss = 0.3703189414531358\n",
      "After 2340 epochs, Loss = 0.36996511920566394\n",
      "After 2350 epochs, Loss = 0.3696131988626067\n",
      "After 2360 epochs, Loss = 0.36926316697758577\n",
      "After 2370 epochs, Loss = 0.3689149965978114\n",
      "After 2380 epochs, Loss = 0.36856867308605995\n",
      "After 2390 epochs, Loss = 0.36822420546160267\n",
      "After 2400 epochs, Loss = 0.36788155005406176\n",
      "After 2410 epochs, Loss = 0.3675406908798523\n",
      "After 2420 epochs, Loss = 0.3672016121644463\n",
      "After 2430 epochs, Loss = 0.36686429973235435\n",
      "After 2440 epochs, Loss = 0.3665287409720476\n",
      "After 2450 epochs, Loss = 0.36619491859855796\n",
      "After 2460 epochs, Loss = 0.36586282842031326\n",
      "After 2470 epochs, Loss = 0.3655324428399797\n",
      "After 2480 epochs, Loss = 0.3652037472614504\n",
      "After 2490 epochs, Loss = 0.36487672849115904\n",
      "After 2500 epochs, Loss = 0.3645513856166954\n",
      "After 2510 epochs, Loss = 0.36422769006059225\n",
      "After 2520 epochs, Loss = 0.3639056279464792\n",
      "After 2530 epochs, Loss = 0.3635851855702943\n",
      "After 2540 epochs, Loss = 0.36326634939734564\n",
      "After 2550 epochs, Loss = 0.362949106059455\n",
      "After 2560 epochs, Loss = 0.36263344235216577\n",
      "After 2570 epochs, Loss = 0.36231934523200704\n",
      "After 2580 epochs, Loss = 0.3620068018138176\n",
      "After 2590 epochs, Loss = 0.3616958109331033\n",
      "After 2600 epochs, Loss = 0.3613863520572629\n",
      "After 2610 epochs, Loss = 0.36107841591336465\n",
      "After 2620 epochs, Loss = 0.3607719921010659\n",
      "After 2630 epochs, Loss = 0.36046706149103896\n",
      "After 2640 epochs, Loss = 0.36016361049920287\n",
      "After 2650 epochs, Loss = 0.3598616272777019\n",
      "After 2660 epochs, Loss = 0.35956110011763204\n",
      "After 2670 epochs, Loss = 0.35926201773579325\n",
      "After 2680 epochs, Loss = 0.3589643757945415\n",
      "After 2690 epochs, Loss = 0.35866815665980156\n",
      "After 2700 epochs, Loss = 0.35837334810718074\n",
      "After 2710 epochs, Loss = 0.35807994713091035\n",
      "After 2720 epochs, Loss = 0.3577879364688127\n",
      "After 2730 epochs, Loss = 0.35749730362273735\n",
      "After 2740 epochs, Loss = 0.3572080454655141\n",
      "After 2750 epochs, Loss = 0.3569201481979692\n",
      "After 2760 epochs, Loss = 0.3566335971193666\n",
      "After 2770 epochs, Loss = 0.35634838191045093\n",
      "After 2780 epochs, Loss = 0.35606449236692994\n",
      "After 2790 epochs, Loss = 0.35578191839773154\n",
      "After 2800 epochs, Loss = 0.3555006500232999\n",
      "After 2810 epochs, Loss = 0.3552206823758928\n",
      "After 2820 epochs, Loss = 0.3549420191948734\n",
      "After 2830 epochs, Loss = 0.3546646323135144\n",
      "After 2840 epochs, Loss = 0.35438851218147405\n",
      "After 2850 epochs, Loss = 0.3541136493518792\n",
      "After 2860 epochs, Loss = 0.35384003447979934\n",
      "After 2870 epochs, Loss = 0.3535676583207561\n",
      "After 2880 epochs, Loss = 0.35329652042952175\n",
      "After 2890 epochs, Loss = 0.3530266035086962\n",
      "After 2900 epochs, Loss = 0.35275791085696884\n",
      "After 2910 epochs, Loss = 0.3524904231608268\n",
      "After 2920 epochs, Loss = 0.35222412940738856\n",
      "After 2930 epochs, Loss = 0.3519590236345829\n",
      "After 2940 epochs, Loss = 0.3516950951782381\n",
      "After 2950 epochs, Loss = 0.35143233494791837\n",
      "After 2960 epochs, Loss = 0.3511707377176212\n",
      "After 2970 epochs, Loss = 0.3509102998667175\n",
      "After 2980 epochs, Loss = 0.35065101335694315\n",
      "After 2990 epochs, Loss = 0.3503928654422138\n",
      "After 3000 epochs, Loss = 0.3501358617687485\n",
      "After 3010 epochs, Loss = 0.3498799866724228\n",
      "After 3020 epochs, Loss = 0.3496252227430643\n",
      "After 3030 epochs, Loss = 0.34937156216145127\n",
      "After 3040 epochs, Loss = 0.34911899718725\n",
      "After 3050 epochs, Loss = 0.34886752015794487\n",
      "After 3060 epochs, Loss = 0.3486171325113718\n",
      "After 3070 epochs, Loss = 0.34836782342616474\n",
      "After 3080 epochs, Loss = 0.34811957977522967\n",
      "After 3090 epochs, Loss = 0.3478723941968815\n",
      "After 3100 epochs, Loss = 0.34762625941458497\n",
      "After 3110 epochs, Loss = 0.3473811817783727\n",
      "After 3120 epochs, Loss = 0.3471371453092654\n",
      "After 3130 epochs, Loss = 0.3468941381856028\n",
      "After 3140 epochs, Loss = 0.34665215340010436\n",
      "After 3150 epochs, Loss = 0.3464111911541604\n",
      "After 3160 epochs, Loss = 0.346171238211716\n",
      "After 3170 epochs, Loss = 0.34593228699718975\n",
      "After 3180 epochs, Loss = 0.34569433770383534\n",
      "After 3190 epochs, Loss = 0.3454573941401127\n",
      "After 3200 epochs, Loss = 0.3452214339770427\n",
      "After 3210 epochs, Loss = 0.34498644897880065\n",
      "After 3220 epochs, Loss = 0.34475243265742944\n",
      "After 3230 epochs, Loss = 0.3445193832074573\n",
      "After 3240 epochs, Loss = 0.3442873023200852\n",
      "After 3250 epochs, Loss = 0.3440561797265433\n",
      "After 3260 epochs, Loss = 0.3438260029853714\n",
      "After 3270 epochs, Loss = 0.3435967633973495\n",
      "After 3280 epochs, Loss = 0.3433684548310677\n",
      "After 3290 epochs, Loss = 0.34314107121203047\n",
      "After 3300 epochs, Loss = 0.3429146065219521\n",
      "After 3310 epochs, Loss = 0.34268905505393765\n",
      "After 3320 epochs, Loss = 0.3424644129312342\n",
      "After 3330 epochs, Loss = 0.34224067202350134\n",
      "After 3340 epochs, Loss = 0.3420178265304023\n",
      "After 3350 epochs, Loss = 0.34179587070445655\n",
      "After 3360 epochs, Loss = 0.3415747988504043\n",
      "After 3370 epochs, Loss = 0.34135460532456513\n",
      "After 3380 epochs, Loss = 0.3411352858950205\n",
      "After 3390 epochs, Loss = 0.34091683439587644\n",
      "After 3400 epochs, Loss = 0.34069924460131273\n",
      "After 3410 epochs, Loss = 0.3404825110678589\n",
      "After 3420 epochs, Loss = 0.3402666284006164\n",
      "After 3430 epochs, Loss = 0.34005159125268003\n",
      "After 3440 epochs, Loss = 0.33983739432457555\n",
      "After 3450 epochs, Loss = 0.33962403578138806\n",
      "After 3460 epochs, Loss = 0.3394115114185554\n",
      "After 3470 epochs, Loss = 0.3391998116623236\n",
      "After 3480 epochs, Loss = 0.3389889313974597\n",
      "After 3490 epochs, Loss = 0.3387788655534545\n",
      "After 3500 epochs, Loss = 0.33856960910400585\n",
      "After 3510 epochs, Loss = 0.3383611571420192\n",
      "After 3520 epochs, Loss = 0.33815350748446354\n",
      "After 3530 epochs, Loss = 0.33794665241375943\n",
      "After 3540 epochs, Loss = 0.3377405947528899\n",
      "After 3550 epochs, Loss = 0.3375353296976119\n",
      "After 3560 epochs, Loss = 0.3373308447725729\n",
      "After 3570 epochs, Loss = 0.33712713524701576\n",
      "After 3580 epochs, Loss = 0.3369241964305245\n",
      "After 3590 epochs, Loss = 0.3367220236725643\n",
      "After 3600 epochs, Loss = 0.3365206123620348\n",
      "After 3610 epochs, Loss = 0.33631997335543473\n",
      "After 3620 epochs, Loss = 0.3361200980774513\n",
      "After 3630 epochs, Loss = 0.33592097054261866\n",
      "After 3640 epochs, Loss = 0.33572258629757196\n",
      "After 3650 epochs, Loss = 0.33552494548084544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 3660 epochs, Loss = 0.33532804634594143\n",
      "After 3670 epochs, Loss = 0.33513187735456434\n",
      "After 3680 epochs, Loss = 0.3349364341987561\n",
      "After 3690 epochs, Loss = 0.334741712606038\n",
      "After 3700 epochs, Loss = 0.33454770833903025\n",
      "After 3710 epochs, Loss = 0.3343544171950821\n",
      "After 3720 epochs, Loss = 0.33416183500590696\n",
      "After 3730 epochs, Loss = 0.3339699576372207\n",
      "After 3740 epochs, Loss = 0.33377878098838604\n",
      "After 3750 epochs, Loss = 0.33358830099206194\n",
      "After 3760 epochs, Loss = 0.33339851361385353\n",
      "After 3770 epochs, Loss = 0.33320941499207063\n",
      "After 3780 epochs, Loss = 0.33302100834890797\n",
      "After 3790 epochs, Loss = 0.332833282405771\n",
      "After 3800 epochs, Loss = 0.3326462332563125\n",
      "After 3810 epochs, Loss = 0.33245985702572745\n",
      "After 3820 epochs, Loss = 0.3322741498701727\n",
      "After 3830 epochs, Loss = 0.3320891079764453\n",
      "After 3840 epochs, Loss = 0.3319047429278997\n",
      "After 3850 epochs, Loss = 0.3317210459421127\n",
      "After 3860 epochs, Loss = 0.33153800793165766\n",
      "After 3870 epochs, Loss = 0.33135562727379114\n",
      "After 3880 epochs, Loss = 0.3311738942347847\n",
      "After 3890 epochs, Loss = 0.3309928042048778\n",
      "After 3900 epochs, Loss = 0.3308123535765303\n",
      "After 3910 epochs, Loss = 0.33063253877043464\n",
      "After 3920 epochs, Loss = 0.3304533562352326\n",
      "After 3930 epochs, Loss = 0.3302748024472281\n",
      "After 3940 epochs, Loss = 0.33009687391720555\n",
      "After 3950 epochs, Loss = 0.329919567184574\n",
      "After 3960 epochs, Loss = 0.3297428788068128\n",
      "After 3970 epochs, Loss = 0.3295668053679727\n",
      "After 3980 epochs, Loss = 0.3293913434784124\n",
      "After 3990 epochs, Loss = 0.3292164897745382\n",
      "After 4000 epochs, Loss = 0.3290422409184829\n",
      "After 4010 epochs, Loss = 0.32886859956191866\n",
      "After 4020 epochs, Loss = 0.32869555727601146\n",
      "After 4030 epochs, Loss = 0.328523109951526\n",
      "After 4040 epochs, Loss = 0.3283512543515974\n",
      "After 4050 epochs, Loss = 0.32817998726390635\n",
      "After 4060 epochs, Loss = 0.32800930902956515\n",
      "After 4070 epochs, Loss = 0.32783921889693346\n",
      "After 4080 epochs, Loss = 0.32766970778926613\n",
      "After 4090 epochs, Loss = 0.327500772589922\n",
      "After 4100 epochs, Loss = 0.32733241020561155\n",
      "After 4110 epochs, Loss = 0.3271646175661768\n",
      "After 4120 epochs, Loss = 0.32699739162436503\n",
      "After 4130 epochs, Loss = 0.3268307293556105\n",
      "After 4140 epochs, Loss = 0.32666462775781613\n",
      "After 4150 epochs, Loss = 0.32649908385113874\n",
      "After 4160 epochs, Loss = 0.3263340959947731\n",
      "After 4170 epochs, Loss = 0.32616966435423955\n",
      "After 4180 epochs, Loss = 0.3260057815986042\n",
      "After 4190 epochs, Loss = 0.3258424448348791\n",
      "After 4200 epochs, Loss = 0.3256796511912952\n",
      "After 4210 epochs, Loss = 0.32551739781709904\n",
      "After 4220 epochs, Loss = 0.32535568188235503\n",
      "After 4230 epochs, Loss = 0.3251945005776594\n",
      "After 4240 epochs, Loss = 0.32503386364094344\n",
      "After 4250 epochs, Loss = 0.32487376092429454\n",
      "After 4260 epochs, Loss = 0.3247141845140863\n",
      "After 4270 epochs, Loss = 0.32455513168100325\n",
      "After 4280 epochs, Loss = 0.3243966026564059\n",
      "After 4290 epochs, Loss = 0.324238592012448\n",
      "After 4300 epochs, Loss = 0.3240810991489735\n",
      "After 4310 epochs, Loss = 0.3239241216403175\n",
      "After 4320 epochs, Loss = 0.3237676543410409\n",
      "After 4330 epochs, Loss = 0.3236116946376542\n",
      "After 4340 epochs, Loss = 0.3234562399353017\n",
      "After 4350 epochs, Loss = 0.32330128795933527\n",
      "After 4360 epochs, Loss = 0.3231468367274742\n",
      "After 4370 epochs, Loss = 0.3229928828219895\n",
      "After 4380 epochs, Loss = 0.32283942372080987\n",
      "After 4390 epochs, Loss = 0.32268645952120345\n",
      "After 4400 epochs, Loss = 0.322533987646947\n",
      "After 4410 epochs, Loss = 0.3223820053021008\n",
      "After 4420 epochs, Loss = 0.3222305078450373\n",
      "After 4430 epochs, Loss = 0.32207949284098736\n",
      "After 4440 epochs, Loss = 0.3219289578721743\n",
      "After 4450 epochs, Loss = 0.3217789005376585\n",
      "After 4460 epochs, Loss = 0.32162933193875953\n",
      "After 4470 epochs, Loss = 0.3214802371491451\n",
      "After 4480 epochs, Loss = 0.32133161288231893\n",
      "After 4490 epochs, Loss = 0.32118345680323557\n",
      "After 4500 epochs, Loss = 0.3210357665929437\n",
      "After 4510 epochs, Loss = 0.32088853994844047\n",
      "After 4520 epochs, Loss = 0.320741774582535\n",
      "After 4530 epochs, Loss = 0.3205954682237042\n",
      "After 4540 epochs, Loss = 0.32044961861595683\n",
      "After 4550 epochs, Loss = 0.32030423621338544\n",
      "After 4560 epochs, Loss = 0.3201593177283001\n",
      "After 4570 epochs, Loss = 0.32001484926682455\n",
      "After 4580 epochs, Loss = 0.31987083195149885\n",
      "After 4590 epochs, Loss = 0.31972726120680345\n",
      "After 4600 epochs, Loss = 0.3195841339459645\n",
      "After 4610 epochs, Loss = 0.3194414528377396\n",
      "After 4620 epochs, Loss = 0.31929921145096957\n",
      "After 4630 epochs, Loss = 0.319157407135618\n",
      "After 4640 epochs, Loss = 0.31901603778419135\n",
      "After 4650 epochs, Loss = 0.3188751013032882\n",
      "After 4660 epochs, Loss = 0.3187345956134768\n",
      "After 4670 epochs, Loss = 0.3185945186491768\n",
      "After 4680 epochs, Loss = 0.3184548683585395\n",
      "After 4690 epochs, Loss = 0.3183156427033314\n",
      "After 4700 epochs, Loss = 0.3181768396588187\n",
      "After 4710 epochs, Loss = 0.31803845721365187\n",
      "After 4720 epochs, Loss = 0.3179004933697502\n",
      "After 4730 epochs, Loss = 0.31776294614219386\n",
      "After 4740 epochs, Loss = 0.31762581355910857\n",
      "After 4750 epochs, Loss = 0.3174890936615582\n",
      "After 4760 epochs, Loss = 0.3173527845034331\n",
      "After 4770 epochs, Loss = 0.3172168857292851\n",
      "After 4780 epochs, Loss = 0.3170814028886254\n",
      "After 4790 epochs, Loss = 0.316946327564245\n",
      "After 4800 epochs, Loss = 0.31681166035209063\n",
      "After 4810 epochs, Loss = 0.3166773943280601\n",
      "After 4820 epochs, Loss = 0.31654352762074095\n",
      "After 4830 epochs, Loss = 0.3164100583708225\n",
      "After 4840 epochs, Loss = 0.31627699245493784\n",
      "After 4850 epochs, Loss = 0.31614432400710846\n",
      "After 4860 epochs, Loss = 0.3160120475320684\n",
      "After 4870 epochs, Loss = 0.3158801664376655\n",
      "After 4880 epochs, Loss = 0.31574868278701423\n",
      "After 4890 epochs, Loss = 0.3156175871200051\n",
      "After 4900 epochs, Loss = 0.3154868852294371\n",
      "After 4910 epochs, Loss = 0.31535656635427656\n",
      "After 4920 epochs, Loss = 0.31522662873959384\n",
      "After 4930 epochs, Loss = 0.3150970706416013\n",
      "After 4940 epochs, Loss = 0.31496789032755007\n",
      "After 4950 epochs, Loss = 0.31483908607564415\n",
      "After 4960 epochs, Loss = 0.3147106561749493\n",
      "After 4970 epochs, Loss = 0.3145825989253054\n",
      "After 4980 epochs, Loss = 0.314454912637241\n",
      "After 4990 epochs, Loss = 0.31432760134389337\n",
      "After 5000 epochs, Loss = 0.31420066008545333\n",
      "After 5010 epochs, Loss = 0.31407408478592874\n",
      "After 5020 epochs, Loss = 0.31394787379777145\n",
      "After 5030 epochs, Loss = 0.31382202548369764\n",
      "After 5040 epochs, Loss = 0.31369653821660654\n",
      "After 5050 epochs, Loss = 0.31357141037949965\n",
      "After 5060 epochs, Loss = 0.31344664036539965\n",
      "After 5070 epochs, Loss = 0.3133222345479623\n",
      "After 5080 epochs, Loss = 0.31319818485920503\n",
      "After 5090 epochs, Loss = 0.31307450093739403\n",
      "After 5100 epochs, Loss = 0.312951170397298\n",
      "After 5110 epochs, Loss = 0.3128281897967112\n",
      "After 5120 epochs, Loss = 0.31270555758699875\n",
      "After 5130 epochs, Loss = 0.31258327222900983\n",
      "After 5140 epochs, Loss = 0.31246133219300193\n",
      "After 5150 epochs, Loss = 0.312339735958568\n",
      "After 5160 epochs, Loss = 0.31221848201456315\n",
      "After 5170 epochs, Loss = 0.3120975745157399\n",
      "After 5180 epochs, Loss = 0.3119770162516974\n",
      "After 5190 epochs, Loss = 0.31185679585654624\n",
      "After 5200 epochs, Loss = 0.3117369118520806\n",
      "After 5210 epochs, Loss = 0.31161736276921054\n",
      "After 5220 epochs, Loss = 0.31149814997223146\n",
      "After 5230 epochs, Loss = 0.31137927678113075\n",
      "After 5240 epochs, Loss = 0.3112607341438398\n",
      "After 5250 epochs, Loss = 0.31114252062627\n",
      "After 5260 epochs, Loss = 0.31102463480389797\n",
      "After 5270 epochs, Loss = 0.3109070774908605\n",
      "After 5280 epochs, Loss = 0.3107898494593042\n",
      "After 5290 epochs, Loss = 0.3106729448996487\n",
      "After 5300 epochs, Loss = 0.31055636242147927\n",
      "After 5310 epochs, Loss = 0.31044010135478095\n",
      "After 5320 epochs, Loss = 0.3103241669901797\n",
      "After 5330 epochs, Loss = 0.3102085505883005\n",
      "After 5340 epochs, Loss = 0.3100932507916513\n",
      "After 5350 epochs, Loss = 0.30997826625082425\n",
      "After 5360 epochs, Loss = 0.30986359562442495\n",
      "After 5370 epochs, Loss = 0.30974923757900824\n",
      "After 5380 epochs, Loss = 0.30963519078901436\n",
      "After 5390 epochs, Loss = 0.30952145393670527\n",
      "After 5400 epochs, Loss = 0.3094080257121031\n",
      "After 5410 epochs, Loss = 0.30929490481292843\n",
      "After 5420 epochs, Loss = 0.30918208994453894\n",
      "After 5430 epochs, Loss = 0.30906957981987\n",
      "After 5440 epochs, Loss = 0.3089573731593751\n",
      "After 5450 epochs, Loss = 0.3088454686909679\n",
      "After 5460 epochs, Loss = 0.30873386514996237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5470 epochs, Loss = 0.3086225612790181\n",
      "After 5480 epochs, Loss = 0.3085115597892816\n",
      "After 5490 epochs, Loss = 0.30840085894156044\n",
      "After 5500 epochs, Loss = 0.30829045403315697\n",
      "After 5510 epochs, Loss = 0.3081803438355347\n",
      "After 5520 epochs, Loss = 0.30807052712722677\n",
      "After 5530 epochs, Loss = 0.3079610026937796\n",
      "After 5540 epochs, Loss = 0.3078517693277006\n",
      "After 5550 epochs, Loss = 0.3077428258284053\n",
      "After 5560 epochs, Loss = 0.3076341710021663\n",
      "After 5570 epochs, Loss = 0.3075258036620615\n",
      "After 5580 epochs, Loss = 0.30741772262792333\n",
      "After 5590 epochs, Loss = 0.3073099267262891\n",
      "After 5600 epochs, Loss = 0.3072024147903499\n",
      "After 5610 epochs, Loss = 0.3070951856599022\n",
      "After 5620 epochs, Loss = 0.30698823818129933\n",
      "After 5630 epochs, Loss = 0.3068815712074036\n",
      "After 5640 epochs, Loss = 0.3067751835975363\n",
      "After 5650 epochs, Loss = 0.30666907421743317\n",
      "After 5660 epochs, Loss = 0.30656324193919476\n",
      "After 5670 epochs, Loss = 0.3064576856412428\n",
      "After 5680 epochs, Loss = 0.3063524042082722\n",
      "After 5690 epochs, Loss = 0.30624739653120514\n",
      "After 5700 epochs, Loss = 0.30614266150714897\n",
      "After 5710 epochs, Loss = 0.306038198039347\n",
      "After 5720 epochs, Loss = 0.3059340050371384\n",
      "After 5730 epochs, Loss = 0.30583008141591106\n",
      "After 5740 epochs, Loss = 0.3057264260970609\n",
      "After 5750 epochs, Loss = 0.3056230398927743\n",
      "After 5760 epochs, Loss = 0.3055199260399035\n",
      "After 5770 epochs, Loss = 0.3054170772774517\n",
      "After 5780 epochs, Loss = 0.30531449255058096\n",
      "After 5790 epochs, Loss = 0.3052121708102542\n",
      "After 5800 epochs, Loss = 0.3051101110131803\n",
      "After 5810 epochs, Loss = 0.3050083121217765\n",
      "After 5820 epochs, Loss = 0.30490677310412745\n",
      "After 5830 epochs, Loss = 0.3048054929339447\n",
      "After 5840 epochs, Loss = 0.30470447059052863\n",
      "After 5850 epochs, Loss = 0.30460370822345534\n",
      "After 5860 epochs, Loss = 0.3045032119383361\n",
      "After 5870 epochs, Loss = 0.3044029704409028\n",
      "After 5880 epochs, Loss = 0.30430298273246026\n",
      "After 5890 epochs, Loss = 0.30420324781971075\n",
      "After 5900 epochs, Loss = 0.3041037647147119\n",
      "After 5910 epochs, Loss = 0.30400453243483877\n",
      "After 5920 epochs, Loss = 0.3039055500027478\n",
      "After 5930 epochs, Loss = 0.30380681644634216\n",
      "After 5940 epochs, Loss = 0.30370833079873105\n",
      "After 5950 epochs, Loss = 0.30361009209819917\n",
      "After 5960 epochs, Loss = 0.3035120993881673\n",
      "After 5970 epochs, Loss = 0.3034143519950359\n",
      "After 5980 epochs, Loss = 0.3033168538333637\n",
      "After 5990 epochs, Loss = 0.3032195988230568\n",
      "After 6000 epochs, Loss = 0.3031225860277063\n",
      "After 6010 epochs, Loss = 0.3030258145158786\n",
      "After 6020 epochs, Loss = 0.3029292833610737\n",
      "After 6030 epochs, Loss = 0.30283299164168737\n",
      "After 6040 epochs, Loss = 0.30273693844098015\n",
      "After 6050 epochs, Loss = 0.3026411228470447\n",
      "After 6060 epochs, Loss = 0.3025455439527715\n",
      "After 6070 epochs, Loss = 0.3024502008558173\n",
      "After 6080 epochs, Loss = 0.30235509265857363\n",
      "After 6090 epochs, Loss = 0.302260218468134\n",
      "After 6100 epochs, Loss = 0.30216557739626143\n",
      "After 6110 epochs, Loss = 0.30207116855936034\n",
      "After 6120 epochs, Loss = 0.30197699107844095\n",
      "After 6130 epochs, Loss = 0.30188304407909217\n",
      "After 6140 epochs, Loss = 0.3017893266914482\n",
      "After 6150 epochs, Loss = 0.30169583805016015\n",
      "After 6160 epochs, Loss = 0.30160257729436635\n",
      "After 6170 epochs, Loss = 0.3015095435676598\n",
      "After 6180 epochs, Loss = 0.3014167360180599\n",
      "After 6190 epochs, Loss = 0.30132415379798466\n",
      "After 6200 epochs, Loss = 0.3012317960642192\n",
      "After 6210 epochs, Loss = 0.30113966197788755\n",
      "After 6220 epochs, Loss = 0.3010477507044249\n",
      "After 6230 epochs, Loss = 0.3009560614135476\n",
      "After 6240 epochs, Loss = 0.3008645959652352\n",
      "After 6250 epochs, Loss = 0.30077335117607323\n",
      "After 6260 epochs, Loss = 0.300682337672175\n",
      "After 6270 epochs, Loss = 0.30059154290808665\n",
      "After 6280 epochs, Loss = 0.3005009660311695\n",
      "After 6290 epochs, Loss = 0.3004106062362821\n",
      "After 6300 epochs, Loss = 0.30032046272236923\n",
      "After 6310 epochs, Loss = 0.30023053469243405\n",
      "After 6320 epochs, Loss = 0.30014082135351255\n",
      "After 6330 epochs, Loss = 0.30005132383034455\n",
      "After 6340 epochs, Loss = 0.2999620447463584\n",
      "After 6350 epochs, Loss = 0.29987297800542545\n",
      "After 6360 epochs, Loss = 0.29978412283043165\n",
      "After 6370 epochs, Loss = 0.2996954784481689\n",
      "After 6380 epochs, Loss = 0.2996070440893081\n",
      "After 6390 epochs, Loss = 0.2995188189883753\n",
      "After 6400 epochs, Loss = 0.29943080238372755\n",
      "After 6410 epochs, Loss = 0.2993429950998367\n",
      "After 6420 epochs, Loss = 0.2992553980573424\n",
      "After 6430 epochs, Loss = 0.2991680072489843\n",
      "After 6440 epochs, Loss = 0.2990808219282007\n",
      "After 6450 epochs, Loss = 0.29899384135213997\n",
      "After 6460 epochs, Loss = 0.2989070647816352\n",
      "After 6470 epochs, Loss = 0.29882049148118295\n",
      "After 6480 epochs, Loss = 0.2987341207189177\n",
      "After 6490 epochs, Loss = 0.29864795176659\n",
      "After 6500 epochs, Loss = 0.29856198389954275\n",
      "After 6510 epochs, Loss = 0.29847621806480734\n",
      "After 6520 epochs, Loss = 0.2983906519347165\n",
      "After 6530 epochs, Loss = 0.29830528473831536\n",
      "After 6540 epochs, Loss = 0.2982201157650451\n",
      "After 6550 epochs, Loss = 0.29813514430784815\n",
      "After 6560 epochs, Loss = 0.2980503696631402\n",
      "After 6570 epochs, Loss = 0.2979657911307734\n",
      "After 6580 epochs, Loss = 0.29788140801401675\n",
      "After 6590 epochs, Loss = 0.2977972211619468\n",
      "After 6600 epochs, Loss = 0.2977132350502938\n",
      "After 6610 epochs, Loss = 0.29762944227459465\n",
      "After 6620 epochs, Loss = 0.29754584215171387\n",
      "After 6630 epochs, Loss = 0.2974624340018267\n",
      "After 6640 epochs, Loss = 0.29737921714839194\n",
      "After 6650 epochs, Loss = 0.29729619315633854\n",
      "After 6660 epochs, Loss = 0.29721336182772223\n",
      "After 6670 epochs, Loss = 0.2971307197796485\n",
      "After 6680 epochs, Loss = 0.2970482663483937\n",
      "After 6690 epochs, Loss = 0.2969660008735812\n",
      "After 6700 epochs, Loss = 0.29688392269801117\n",
      "After 6710 epochs, Loss = 0.2968020311676274\n",
      "After 6720 epochs, Loss = 0.2967203256314967\n",
      "After 6730 epochs, Loss = 0.2966388054417931\n",
      "After 6740 epochs, Loss = 0.29655746995377563\n",
      "After 6750 epochs, Loss = 0.296476318525771\n",
      "After 6760 epochs, Loss = 0.2963953505191553\n",
      "After 6770 epochs, Loss = 0.29631456529833444\n",
      "After 6780 epochs, Loss = 0.29623396223072646\n",
      "After 6790 epochs, Loss = 0.2961535406867431\n",
      "After 6800 epochs, Loss = 0.29607330003977134\n",
      "After 6810 epochs, Loss = 0.295993239666156\n",
      "After 6820 epochs, Loss = 0.29591335894518084\n",
      "After 6830 epochs, Loss = 0.2958336572590511\n",
      "After 6840 epochs, Loss = 0.2957541339928777\n",
      "After 6850 epochs, Loss = 0.2956747885346574\n",
      "After 6860 epochs, Loss = 0.2955956203853163\n",
      "After 6870 epochs, Loss = 0.2955166323889245\n",
      "After 6880 epochs, Loss = 0.29543782038268535\n",
      "After 6890 epochs, Loss = 0.295359183765978\n",
      "After 6900 epochs, Loss = 0.2952807219409853\n",
      "After 6910 epochs, Loss = 0.2952024343126741\n",
      "After 6920 epochs, Loss = 0.2951243202887776\n",
      "After 6930 epochs, Loss = 0.2950463792797802\n",
      "After 6940 epochs, Loss = 0.29496861069890196\n",
      "After 6950 epochs, Loss = 0.294891013962079\n",
      "After 6960 epochs, Loss = 0.29481358848795075\n",
      "After 6970 epochs, Loss = 0.2947363336978421\n",
      "After 6980 epochs, Loss = 0.29465924901574775\n",
      "After 6990 epochs, Loss = 0.29458233386831706\n",
      "After 7000 epochs, Loss = 0.2945055876848372\n",
      "After 7010 epochs, Loss = 0.2944290098972198\n",
      "After 7020 epochs, Loss = 0.294352599939983\n",
      "After 7030 epochs, Loss = 0.29427635725023726\n",
      "After 7040 epochs, Loss = 0.2942002812676702\n",
      "After 7050 epochs, Loss = 0.2941243714345308\n",
      "After 7060 epochs, Loss = 0.29404862719561575\n",
      "After 7070 epochs, Loss = 0.2939730479982522\n",
      "After 7080 epochs, Loss = 0.29389764319951506\n",
      "After 7090 epochs, Loss = 0.29382240540190435\n",
      "After 7100 epochs, Loss = 0.29374734451442536\n",
      "After 7110 epochs, Loss = 0.29367244647741636\n",
      "After 7120 epochs, Loss = 0.2935977107506108\n",
      "After 7130 epochs, Loss = 0.2935231367961963\n",
      "After 7140 epochs, Loss = 0.293448724078791\n",
      "After 7150 epochs, Loss = 0.29337447206542905\n",
      "After 7160 epochs, Loss = 0.2933003802255464\n",
      "After 7170 epochs, Loss = 0.2932264480309662\n",
      "After 7180 epochs, Loss = 0.29315267495588637\n",
      "After 7190 epochs, Loss = 0.29307906047686494\n",
      "After 7200 epochs, Loss = 0.29300560407280746\n",
      "After 7210 epochs, Loss = 0.2929323060941092\n",
      "After 7220 epochs, Loss = 0.2928591684348061\n",
      "After 7230 epochs, Loss = 0.29278618730017686\n",
      "After 7240 epochs, Loss = 0.2927133621783706\n",
      "After 7250 epochs, Loss = 0.29264069255981723\n",
      "After 7260 epochs, Loss = 0.2925681779372142\n",
      "After 7270 epochs, Loss = 0.2924958178055133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 7280 epochs, Loss = 0.292423611661909\n",
      "After 7290 epochs, Loss = 0.2923515590058234\n",
      "After 7300 epochs, Loss = 0.2922796593388962\n",
      "After 7310 epochs, Loss = 0.29220791216496994\n",
      "After 7320 epochs, Loss = 0.29213631699007864\n",
      "After 7330 epochs, Loss = 0.29206487332243536\n",
      "After 7340 epochs, Loss = 0.2919935806724193\n",
      "After 7350 epochs, Loss = 0.29192243855256406\n",
      "After 7360 epochs, Loss = 0.2918514464775456\n",
      "After 7370 epochs, Loss = 0.291780603964169\n",
      "After 7380 epochs, Loss = 0.29170991053135864\n",
      "After 7390 epochs, Loss = 0.2916393657001438\n",
      "After 7400 epochs, Loss = 0.291568968993649\n",
      "After 7410 epochs, Loss = 0.29149871993708076\n",
      "After 7420 epochs, Loss = 0.2914286180577166\n",
      "After 7430 epochs, Loss = 0.29135866288489337\n",
      "After 7440 epochs, Loss = 0.29128885665447535\n",
      "After 7450 epochs, Loss = 0.2912191997425166\n",
      "After 7460 epochs, Loss = 0.29114968813742403\n",
      "After 7470 epochs, Loss = 0.2910803213766496\n",
      "After 7480 epochs, Loss = 0.29101109899964905\n",
      "After 7490 epochs, Loss = 0.2909420205478692\n",
      "After 7500 epochs, Loss = 0.29087308884081514\n",
      "After 7510 epochs, Loss = 0.2908043013149667\n",
      "After 7520 epochs, Loss = 0.2907356563516668\n",
      "After 7530 epochs, Loss = 0.2906671568749044\n",
      "After 7540 epochs, Loss = 0.2905988001246338\n",
      "After 7550 epochs, Loss = 0.29053058458852704\n",
      "After 7560 epochs, Loss = 0.29046250982166455\n",
      "After 7570 epochs, Loss = 0.29039457538103053\n",
      "After 7580 epochs, Loss = 0.2903267808255027\n",
      "After 7590 epochs, Loss = 0.290259127146469\n",
      "After 7600 epochs, Loss = 0.29019161372311597\n",
      "After 7610 epochs, Loss = 0.29012423887259203\n",
      "After 7620 epochs, Loss = 0.29005700216125346\n",
      "After 7630 epochs, Loss = 0.28998990315730144\n",
      "After 7640 epochs, Loss = 0.28992294143076996\n",
      "After 7650 epochs, Loss = 0.2898561165535143\n",
      "After 7660 epochs, Loss = 0.2897894280992046\n",
      "After 7670 epochs, Loss = 0.28972287564331245\n",
      "After 7680 epochs, Loss = 0.28965645876310386\n",
      "After 7690 epochs, Loss = 0.28959017703762674\n",
      "After 7700 epochs, Loss = 0.2895240300477047\n",
      "After 7710 epochs, Loss = 0.28945801737592447\n",
      "After 7720 epochs, Loss = 0.28939213860662744\n",
      "After 7730 epochs, Loss = 0.28932639332590177\n",
      "After 7740 epochs, Loss = 0.2892607811215689\n",
      "After 7750 epochs, Loss = 0.2891953015831799\n",
      "After 7760 epochs, Loss = 0.28912995430200117\n",
      "After 7770 epochs, Loss = 0.28906473887100703\n",
      "After 7780 epochs, Loss = 0.28899965488487184\n",
      "After 7790 epochs, Loss = 0.2889347019399596\n",
      "After 7800 epochs, Loss = 0.288869879634315\n",
      "After 7810 epochs, Loss = 0.2888051875676538\n",
      "After 7820 epochs, Loss = 0.2887406253413563\n",
      "After 7830 epochs, Loss = 0.288676192558455\n",
      "After 7840 epochs, Loss = 0.28861188882362937\n",
      "After 7850 epochs, Loss = 0.288547713743195\n",
      "After 7860 epochs, Loss = 0.28848366692509236\n",
      "After 7870 epochs, Loss = 0.288419755939827\n",
      "After 7880 epochs, Loss = 0.2883559734968931\n",
      "After 7890 epochs, Loss = 0.2882923181428803\n",
      "After 7900 epochs, Loss = 0.28822878949224207\n",
      "After 7910 epochs, Loss = 0.28816538716101414\n",
      "After 7920 epochs, Loss = 0.28810211076680775\n",
      "After 7930 epochs, Loss = 0.2880389599287988\n",
      "After 7940 epochs, Loss = 0.2879759342677215\n",
      "After 7950 epochs, Loss = 0.2879130334058596\n",
      "After 7960 epochs, Loss = 0.28785025696703903\n",
      "After 7970 epochs, Loss = 0.2877876045766181\n",
      "After 7980 epochs, Loss = 0.28772507586148155\n",
      "After 7990 epochs, Loss = 0.28766267683428554\n",
      "After 8000 epochs, Loss = 0.2876004037818893\n",
      "After 8010 epochs, Loss = 0.287538255574504\n",
      "After 8020 epochs, Loss = 0.2874762371421018\n",
      "After 8030 epochs, Loss = 0.2874143405405517\n",
      "After 8040 epochs, Loss = 0.28735256540573667\n",
      "After 8050 epochs, Loss = 0.287290916755884\n",
      "After 8060 epochs, Loss = 0.28722939086146076\n",
      "After 8070 epochs, Loss = 0.28716798534734744\n",
      "After 8080 epochs, Loss = 0.28710669985614073\n",
      "After 8090 epochs, Loss = 0.28704553590282583\n",
      "After 8100 epochs, Loss = 0.2869844934305987\n",
      "After 8110 epochs, Loss = 0.28692356992070356\n",
      "After 8120 epochs, Loss = 0.2868627650205321\n",
      "After 8130 epochs, Loss = 0.2868020783788881\n",
      "After 8140 epochs, Loss = 0.28674150964598266\n",
      "After 8150 epochs, Loss = 0.2866810584734245\n",
      "After 8160 epochs, Loss = 0.2866207245142132\n",
      "After 8170 epochs, Loss = 0.2865605074227337\n",
      "After 8180 epochs, Loss = 0.28650040685474754\n",
      "After 8190 epochs, Loss = 0.286440422467385\n",
      "After 8200 epochs, Loss = 0.28638055391914097\n",
      "After 8210 epochs, Loss = 0.286320802743019\n",
      "After 8220 epochs, Loss = 0.28626117249241956\n",
      "After 8230 epochs, Loss = 0.2862016570570605\n",
      "After 8240 epochs, Loss = 0.28614225610089716\n",
      "After 8250 epochs, Loss = 0.2860829692892393\n",
      "After 8260 epochs, Loss = 0.2860237962887159\n",
      "After 8270 epochs, Loss = 0.285964736767268\n",
      "After 8280 epochs, Loss = 0.28590579039414304\n",
      "After 8290 epochs, Loss = 0.2858469568398868\n",
      "After 8300 epochs, Loss = 0.28578823577633833\n",
      "After 8310 epochs, Loss = 0.2857296268766229\n",
      "After 8320 epochs, Loss = 0.28567112981514475\n",
      "After 8330 epochs, Loss = 0.2856127442675821\n",
      "After 8340 epochs, Loss = 0.2855544699108806\n",
      "After 8350 epochs, Loss = 0.2854963064232467\n",
      "After 8360 epochs, Loss = 0.2854382534841401\n",
      "After 8370 epochs, Loss = 0.2853803107742707\n",
      "After 8380 epochs, Loss = 0.28532247797558885\n",
      "After 8390 epochs, Loss = 0.2852647547712823\n",
      "After 8400 epochs, Loss = 0.2852071469294774\n",
      "After 8410 epochs, Loss = 0.2851496529679742\n",
      "After 8420 epochs, Loss = 0.2850922676530944\n",
      "After 8430 epochs, Loss = 0.2850349906730892\n",
      "After 8440 epochs, Loss = 0.2849778217174103\n",
      "After 8450 epochs, Loss = 0.28492076047670595\n",
      "After 8460 epochs, Loss = 0.2848638066428129\n",
      "After 8470 epochs, Loss = 0.2848069599087518\n",
      "After 8480 epochs, Loss = 0.2847502199687216\n",
      "After 8490 epochs, Loss = 0.2846935865180938\n",
      "After 8500 epochs, Loss = 0.2846370592534063\n",
      "After 8510 epochs, Loss = 0.28458063787235854\n",
      "After 8520 epochs, Loss = 0.28452432207380235\n",
      "After 8530 epochs, Loss = 0.2844681141122686\n",
      "After 8540 epochs, Loss = 0.28441201169782226\n",
      "After 8550 epochs, Loss = 0.28435601397113375\n",
      "After 8560 epochs, Loss = 0.2843001251002631\n",
      "After 8570 epochs, Loss = 0.28424434245899644\n",
      "After 8580 epochs, Loss = 0.2841886636142341\n",
      "After 8590 epochs, Loss = 0.2841330882727488\n",
      "After 8600 epochs, Loss = 0.2840776161424304\n",
      "After 8610 epochs, Loss = 0.28402224693227746\n",
      "After 8620 epochs, Loss = 0.28396698035239026\n",
      "After 8630 epochs, Loss = 0.2839118161139675\n",
      "After 8640 epochs, Loss = 0.2838567539292997\n",
      "After 8650 epochs, Loss = 0.2838017935117648\n",
      "After 8660 epochs, Loss = 0.28374693457582195\n",
      "After 8670 epochs, Loss = 0.28369217683700776\n",
      "After 8680 epochs, Loss = 0.2836375200119291\n",
      "After 8690 epochs, Loss = 0.2835829638182607\n",
      "After 8700 epochs, Loss = 0.2835285079747371\n",
      "After 8710 epochs, Loss = 0.28347415220115046\n",
      "After 8720 epochs, Loss = 0.28341989621834274\n",
      "After 8730 epochs, Loss = 0.2833657397482031\n",
      "After 8740 epochs, Loss = 0.28331168670898826\n",
      "After 8750 epochs, Loss = 0.2832577415942654\n",
      "After 8760 epochs, Loss = 0.2832038951546385\n",
      "After 8770 epochs, Loss = 0.283150147116273\n",
      "After 8780 epochs, Loss = 0.2830965030113355\n",
      "After 8790 epochs, Loss = 0.2830429617524907\n",
      "After 8800 epochs, Loss = 0.28298951808188944\n",
      "After 8810 epochs, Loss = 0.28293617172970736\n",
      "After 8820 epochs, Loss = 0.2828829224271203\n",
      "After 8830 epochs, Loss = 0.28282976990629727\n",
      "After 8840 epochs, Loss = 0.282776713900397\n",
      "After 8850 epochs, Loss = 0.2827237541435631\n",
      "After 8860 epochs, Loss = 0.2826708903709188\n",
      "After 8870 epochs, Loss = 0.2826181223185633\n",
      "After 8880 epochs, Loss = 0.2825654541428759\n",
      "After 8890 epochs, Loss = 0.2825128813612909\n",
      "After 8900 epochs, Loss = 0.2824604035128235\n",
      "After 8910 epochs, Loss = 0.2824080203374308\n",
      "After 8920 epochs, Loss = 0.28235573157602284\n",
      "After 8930 epochs, Loss = 0.28230353697045923\n",
      "After 8940 epochs, Loss = 0.2822514362635439\n",
      "After 8950 epochs, Loss = 0.2821994291990221\n",
      "After 8960 epochs, Loss = 0.28214751552157546\n",
      "After 8970 epochs, Loss = 0.2820956949768171\n",
      "After 8980 epochs, Loss = 0.2820439673112879\n",
      "After 8990 epochs, Loss = 0.2819923322724527\n",
      "After 9000 epochs, Loss = 0.28194078960869506\n",
      "After 9010 epochs, Loss = 0.2818893390693153\n",
      "After 9020 epochs, Loss = 0.2818379804045232\n",
      "After 9030 epochs, Loss = 0.2817867133654359\n",
      "After 9040 epochs, Loss = 0.2817355377040736\n",
      "After 9050 epochs, Loss = 0.2816844531733554\n",
      "After 9060 epochs, Loss = 0.28163345952709407\n",
      "After 9070 epochs, Loss = 0.28158255651999425\n",
      "After 9080 epochs, Loss = 0.28153174390764657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 9090 epochs, Loss = 0.28148102144652426\n",
      "After 9100 epochs, Loss = 0.2814303888939788\n",
      "After 9110 epochs, Loss = 0.28137984600823707\n",
      "After 9120 epochs, Loss = 0.2813293925483958\n",
      "After 9130 epochs, Loss = 0.28127902827441886\n",
      "After 9140 epochs, Loss = 0.2812287529471329\n",
      "After 9150 epochs, Loss = 0.2811785663282239\n",
      "After 9160 epochs, Loss = 0.2811284681802325\n",
      "After 9170 epochs, Loss = 0.2810784582665501\n",
      "After 9180 epochs, Loss = 0.2810285363514161\n",
      "After 9190 epochs, Loss = 0.2809787021999141\n",
      "After 9200 epochs, Loss = 0.2809289555779667\n",
      "After 9210 epochs, Loss = 0.2808792962523325\n",
      "After 9220 epochs, Loss = 0.28082972399060274\n",
      "After 9230 epochs, Loss = 0.28078023856119677\n",
      "After 9240 epochs, Loss = 0.28073083973335994\n",
      "After 9250 epochs, Loss = 0.28068152727715767\n",
      "After 9260 epochs, Loss = 0.2806323009634732\n",
      "After 9270 epochs, Loss = 0.28058316056400345\n",
      "After 9280 epochs, Loss = 0.28053410585125627\n",
      "After 9290 epochs, Loss = 0.2804851365985451\n",
      "After 9300 epochs, Loss = 0.2804362525799875\n",
      "After 9310 epochs, Loss = 0.2803874535704997\n",
      "After 9320 epochs, Loss = 0.2803387393457937\n",
      "After 9330 epochs, Loss = 0.28029010968237433\n",
      "After 9340 epochs, Loss = 0.2802415643575356\n",
      "After 9350 epochs, Loss = 0.2801931031493556\n",
      "After 9360 epochs, Loss = 0.28014472614385205\n",
      "After 9370 epochs, Loss = 0.2800964367010803\n",
      "After 9380 epochs, Loss = 0.2800482318997468\n",
      "After 9390 epochs, Loss = 0.2800001103352626\n",
      "After 9400 epochs, Loss = 0.27995207283637075\n",
      "After 9410 epochs, Loss = 0.2799041235578991\n",
      "After 9420 epochs, Loss = 0.2798562568619033\n",
      "After 9430 epochs, Loss = 0.27980847253262087\n",
      "After 9440 epochs, Loss = 0.27976077035504526\n",
      "After 9450 epochs, Loss = 0.27971315011492\n",
      "After 9460 epochs, Loss = 0.279665611598738\n",
      "After 9470 epochs, Loss = 0.27961815459373573\n",
      "After 9480 epochs, Loss = 0.2795707788878913\n",
      "After 9490 epochs, Loss = 0.2795234842699207\n",
      "After 9500 epochs, Loss = 0.2794762705292754\n",
      "After 9510 epochs, Loss = 0.27942913745613823\n",
      "After 9520 epochs, Loss = 0.2793820848414196\n",
      "After 9530 epochs, Loss = 0.2793351124767564\n",
      "After 9540 epochs, Loss = 0.27928822015450716\n",
      "After 9550 epochs, Loss = 0.27924140766774896\n",
      "After 9560 epochs, Loss = 0.2791946748102752\n",
      "After 9570 epochs, Loss = 0.2791480213765919\n",
      "After 9580 epochs, Loss = 0.27910144716191443\n",
      "After 9590 epochs, Loss = 0.2790549519621645\n",
      "After 9600 epochs, Loss = 0.27900853557396715\n",
      "After 9610 epochs, Loss = 0.27896219779464865\n",
      "After 9620 epochs, Loss = 0.2789159384222317\n",
      "After 9630 epochs, Loss = 0.2788697572554343\n",
      "After 9640 epochs, Loss = 0.2788236540936645\n",
      "After 9650 epochs, Loss = 0.2787776321778749\n",
      "After 9660 epochs, Loss = 0.2787316883772176\n",
      "After 9670 epochs, Loss = 0.2786858219810025\n",
      "After 9680 epochs, Loss = 0.27864003279137367\n",
      "After 9690 epochs, Loss = 0.27859432061115225\n",
      "After 9700 epochs, Loss = 0.2785486852438337\n",
      "After 9710 epochs, Loss = 0.27850312667567484\n",
      "After 9720 epochs, Loss = 0.27845764538815554\n",
      "After 9730 epochs, Loss = 0.27841224032776307\n",
      "After 9740 epochs, Loss = 0.2783669113006565\n",
      "After 9750 epochs, Loss = 0.27832165811365517\n",
      "After 9760 epochs, Loss = 0.27827648057423404\n",
      "After 9770 epochs, Loss = 0.27823138238685197\n",
      "After 9780 epochs, Loss = 0.27818635980621675\n",
      "After 9790 epochs, Loss = 0.27814141229908973\n",
      "After 9800 epochs, Loss = 0.2780965396755447\n",
      "After 9810 epochs, Loss = 0.27805174174629804\n",
      "After 9820 epochs, Loss = 0.2780070183227062\n",
      "After 9830 epochs, Loss = 0.2779623692167618\n",
      "After 9840 epochs, Loss = 0.2779177942410924\n",
      "After 9850 epochs, Loss = 0.2778732932089576\n",
      "After 9860 epochs, Loss = 0.27782886593424444\n",
      "After 9870 epochs, Loss = 0.27778451223146716\n",
      "After 9880 epochs, Loss = 0.2777402327651966\n",
      "After 9890 epochs, Loss = 0.27769603513767244\n",
      "After 9900 epochs, Loss = 0.27765191053208904\n",
      "After 9910 epochs, Loss = 0.2776078587654221\n",
      "After 9920 epochs, Loss = 0.2775638796552661\n",
      "After 9930 epochs, Loss = 0.27751997301982584\n",
      "After 9940 epochs, Loss = 0.2774761386779142\n",
      "After 9950 epochs, Loss = 0.27743237644894897\n",
      "After 9960 epochs, Loss = 0.2773886861529506\n",
      "After 9970 epochs, Loss = 0.2773450676105398\n",
      "After 9980 epochs, Loss = 0.27730152064293523\n",
      "After 9990 epochs, Loss = 0.2772580450719506\n",
      "After 10000 epochs, Loss = 0.27721464071999224\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "final_theta, cost_epochs = logistic_regression(x_train, y_train, 10000, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc21dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the given model on test data for logistic regression is  0.975\n"
     ]
    }
   ],
   "source": [
    "preds_prob = calc_h(x_test, final_theta)\n",
    "y_pred = preds_prob.round()\n",
    "print(\"The accuracy of the given model on test data for logistic regression is \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b69ea7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of the given model on test data for logistic regression is  [[64  1]\n",
      " [ 2 53]]\n"
     ]
    }
   ],
   "source": [
    "C_m_log = confusion_matrix(y_test,y_pred)\n",
    "print(\"The confusion matrix of the given model on test data for logistic regression is \",C_m_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27860f07",
   "metadata": {},
   "source": [
    "<h5> predicting via the neural network </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06d9f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of neural networks with 2layers on test data is  0.975\n"
     ]
    }
   ],
   "source": [
    "X_vector_test= vectorizer.transform(X_test_df)\n",
    "x_test_df_new = [np.reshape(x, (X_vector_test.shape[1], 1)) for x in X_vector_test.toarray()]\n",
    "y_test = y_test_df.array\n",
    "test_data = zip(x_test_df_new, y_test)\n",
    "\n",
    "pred = net.predict(test_data)\n",
    "#pred= pred.round()\n",
    "print(\"The accuracy score of neural networks with 2layers on test data is \",accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e444b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65,  0],\n",
       "       [ 3, 52]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213acace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
